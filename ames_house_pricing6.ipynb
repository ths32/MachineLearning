{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c648adfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import math\n",
    "from scipy.stats import skew\n",
    "from sklearn.metrics import r2_score\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6087265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66acaf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f163b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "924cb973",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Activation, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07af984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb4dd1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_house = pd.read_csv('C:/Users/thsong/df_house_wool_tmp2.csv')\n",
    "df_house_extracted_wool = pd.read_csv('C:/Users/thsong/df_house_extracted_wool_tmp.csv')\n",
    "df_house_test = pd.read_csv('C:/Users/thsong/Downloads/house-prices-advanced-regression-techniques/test.csv')\n",
    "df_house_test_extracted = pd.read_csv('C:/Users/thsong/df_house_test_extracted_tmp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc8474b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_house4 = pd.read_csv('C:/Users/thsong/df_house4_organized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "14568207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>...</th>\n",
       "      <th>BsmtQual_</th>\n",
       "      <th>BsmtCond_</th>\n",
       "      <th>BsmtExposure_</th>\n",
       "      <th>BsmtFinType1_</th>\n",
       "      <th>HeatingQC_</th>\n",
       "      <th>KitchenQual_</th>\n",
       "      <th>FireplaceQu_</th>\n",
       "      <th>GarageType_</th>\n",
       "      <th>GarageFinish_</th>\n",
       "      <th>SaleCondition_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>953</td>\n",
       "      <td>418</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>870</td>\n",
       "      <td>422</td>\n",
       "      <td>1184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>134</td>\n",
       "      <td>220</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>870</td>\n",
       "      <td>422</td>\n",
       "      <td>1184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>114</td>\n",
       "      <td>418</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>870</td>\n",
       "      <td>422</td>\n",
       "      <td>1184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>953</td>\n",
       "      <td>220</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>387</td>\n",
       "      <td>605</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>221</td>\n",
       "      <td>418</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>870</td>\n",
       "      <td>422</td>\n",
       "      <td>1184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley  \\\n",
       "0           0   1          60       RL         65.0     8450   Pave   NaN   \n",
       "1           1   2          20       RL         80.0     9600   Pave   NaN   \n",
       "2           2   3          60       RL         68.0    11250   Pave   NaN   \n",
       "3           3   4          70       RL         60.0     9550   Pave   NaN   \n",
       "4           4   5          60       RL         84.0    14260   Pave   NaN   \n",
       "\n",
       "  LotShape LandContour  ... BsmtQual_ BsmtCond_ BsmtExposure_ BsmtFinType1_  \\\n",
       "0      Reg         Lvl  ...         4         3           953           418   \n",
       "1      Reg         Lvl  ...         4         3           134           220   \n",
       "2      IR1         Lvl  ...         4         3           114           418   \n",
       "3      IR1         Lvl  ...         3         4           953           220   \n",
       "4      IR1         Lvl  ...         4         3           221           418   \n",
       "\n",
       "  HeatingQC_ KitchenQual_ FireplaceQu_ GarageType_  GarageFinish_  \\\n",
       "0          5            4            0         870            422   \n",
       "1          5            3            3         870            422   \n",
       "2          5            4            3         870            422   \n",
       "3          4            4            4         387            605   \n",
       "4          5            4            3         870            422   \n",
       "\n",
       "   SaleCondition_  \n",
       "0            1184  \n",
       "1            1184  \n",
       "2            1184  \n",
       "3              98  \n",
       "4            1184  \n",
       "\n",
       "[5 rows x 107 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_house4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a3b53beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_house4_2 = pd.read_csv('C:/Users/thsong/df_house4_organized.csv',header=None) # ,delim_whitespace=True,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8a605028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Id</td>\n",
       "      <td>MSSubClass</td>\n",
       "      <td>MSZoning</td>\n",
       "      <td>LotFrontage</td>\n",
       "      <td>LotArea</td>\n",
       "      <td>Street</td>\n",
       "      <td>Alley</td>\n",
       "      <td>LotShape</td>\n",
       "      <td>LandContour</td>\n",
       "      <td>...</td>\n",
       "      <td>BsmtQual_</td>\n",
       "      <td>BsmtCond_</td>\n",
       "      <td>BsmtExposure_</td>\n",
       "      <td>BsmtFinType1_</td>\n",
       "      <td>HeatingQC_</td>\n",
       "      <td>KitchenQual_</td>\n",
       "      <td>FireplaceQu_</td>\n",
       "      <td>GarageType_</td>\n",
       "      <td>GarageFinish_</td>\n",
       "      <td>SaleCondition_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>953</td>\n",
       "      <td>418</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>870</td>\n",
       "      <td>422</td>\n",
       "      <td>1184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>134</td>\n",
       "      <td>220</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>870</td>\n",
       "      <td>422</td>\n",
       "      <td>1184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>114</td>\n",
       "      <td>418</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>870</td>\n",
       "      <td>422</td>\n",
       "      <td>1184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>953</td>\n",
       "      <td>220</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>387</td>\n",
       "      <td>605</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1           2         3            4        5       6      7    \\\n",
       "0  NaN  Id  MSSubClass  MSZoning  LotFrontage  LotArea  Street  Alley   \n",
       "1  0.0   1          60        RL         65.0     8450    Pave    NaN   \n",
       "2  1.0   2          20        RL         80.0     9600    Pave    NaN   \n",
       "3  2.0   3          60        RL         68.0    11250    Pave    NaN   \n",
       "4  3.0   4          70        RL         60.0     9550    Pave    NaN   \n",
       "\n",
       "        8            9    ...        97         98             99   \\\n",
       "0  LotShape  LandContour  ...  BsmtQual_  BsmtCond_  BsmtExposure_   \n",
       "1       Reg          Lvl  ...          4          3            953   \n",
       "2       Reg          Lvl  ...          4          3            134   \n",
       "3       IR1          Lvl  ...          4          3            114   \n",
       "4       IR1          Lvl  ...          3          4            953   \n",
       "\n",
       "             100         101           102           103          104  \\\n",
       "0  BsmtFinType1_  HeatingQC_  KitchenQual_  FireplaceQu_  GarageType_   \n",
       "1            418           5             4             0          870   \n",
       "2            220           5             3             3          870   \n",
       "3            418           5             4             3          870   \n",
       "4            220           4             4             4          387   \n",
       "\n",
       "             105             106  \n",
       "0  GarageFinish_  SaleCondition_  \n",
       "1            422            1184  \n",
       "2            422            1184  \n",
       "3            422            1184  \n",
       "4            605              98  \n",
       "\n",
       "[5 rows x 107 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_house4_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8f093bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tst = df_house4_2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "560fd45a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_house4_2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "dc9bcfc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan, 'Id', 'MSSubClass', ..., 'GarageType_', 'GarageFinish_',\n",
       "        'SaleCondition_'],\n",
       "       [0.0, '1', '60', ..., '870', '422', '1184'],\n",
       "       [1.0, '2', '20', ..., '870', '422', '1184'],\n",
       "       ...,\n",
       "       [1437.0, '1457', '20', ..., '870', '605', '1184'],\n",
       "       [1438.0, '1458', '70', ..., '870', '422', '1184'],\n",
       "       [1439.0, '1459', '20', ..., '870', '605', '1184']], dtype=object)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "bedce770",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tst = dataset_tst[:,0:43]\n",
    "y_tst = dataset_tst[:,43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b869aa22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan, 'Id', 'MSSubClass', ..., 'Heating', 'HeatingQC',\n",
       "        'CentralAir'],\n",
       "       [0.0, '1', '60', ..., 'GasA', 'Ex', 'Y'],\n",
       "       [1.0, '2', '20', ..., 'GasA', 'Ex', 'Y'],\n",
       "       ...,\n",
       "       [1437.0, '1457', '20', ..., 'GasA', 'TA', 'Y'],\n",
       "       [1438.0, '1458', '70', ..., 'GasA', 'Ex', 'Y'],\n",
       "       [1439.0, '1459', '20', ..., 'GasA', 'Gd', 'Y']], dtype=object)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c88e1eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Electrical', 'SBrkr', 'SBrkr', ..., 'SBrkr', 'SBrkr', 'FuseA'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "751086b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',\n",
       "       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',\n",
       "       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',\n",
       "       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n",
       "       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',\n",
       "       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',\n",
       "       'SaleCondition', 'SalePrice', 'MSZoning_', 'LotShape_', 'LotConfig_',\n",
       "       'Neighborhood_', 'BldgType_', 'HouseStyle_', 'House_Age', 'Garage_Age',\n",
       "       'RoofStyle_', 'Exterior1st_', 'Exterior2nd_', 'Exterior_Avg',\n",
       "       'MasVnrType_', 'ExterQual_', 'Foundation_', 'BsmtQual_', 'BsmtCond_',\n",
       "       'BsmtExposure_', 'BsmtFinType1_', 'HeatingQC_', 'KitchenQual_',\n",
       "       'FireplaceQu_', 'GarageType_', 'GarageFinish_'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_house.columns[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acc35b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',\n",
       "       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',\n",
       "       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',\n",
       "       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n",
       "       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',\n",
       "       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',\n",
       "       'SaleCondition', 'SalePrice', 'MSZoning_', 'LotShape_', 'LotConfig_',\n",
       "       'Neighborhood_', 'BldgType_', 'HouseStyle_', 'House_Age', 'Garage_Age',\n",
       "       'RoofStyle_', 'Exterior1st_', 'Exterior2nd_', 'Exterior_Avg',\n",
       "       'MasVnrType_', 'ExterQual_', 'Foundation_', 'BsmtQual_', 'BsmtCond_',\n",
       "       'BsmtExposure_', 'BsmtFinType1_', 'HeatingQC_', 'KitchenQual_',\n",
       "       'FireplaceQu_', 'GarageType_', 'GarageFinish_', 'SaleCondition_'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_house4.columns[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "3920376a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1432, 1440)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_house4), len(df_house_extracted_wool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "508649ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MSSubClass', 'MSZoning_', 'LotFrontage', 'LotArea', 'LotShape_',\n",
       "       'LotConfig_', 'Neighborhood_', 'HouseStyle_', 'OverallQual',\n",
       "       'OverallCond', 'RoofStyle_', 'MasVnrType_', 'MasVnrArea', 'ExterQual_',\n",
       "       'Foundation_', 'BsmtQual_', 'BsmtExposure_', 'BsmtFinType1_',\n",
       "       'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF', 'HeatingQC_', '1stFlrSF',\n",
       "       '2ndFlrSF', 'GrLivArea', 'BsmtFullBath', 'FullBath', 'HalfBath',\n",
       "       'BedroomAbvGr', 'KitchenQual_', 'TotRmsAbvGrd', 'Fireplaces',\n",
       "       'FireplaceQu_', 'GarageType_', 'House_Age', 'Garage_Age',\n",
       "       'GarageFinish_', 'GarageCars', 'GarageArea', 'WoodDeckSF',\n",
       "       'OpenPorchSF', 'Exterior_Avg', 'SalePrice', 'MSSubClass(scaled)',\n",
       "       'MSZoning_(scaled)', 'LotFrontage(scaled)', 'LotArea(scaled)',\n",
       "       'LotShape_(scaled)', 'LotConfig_(scaled)', 'Neighborhood_(scaled)',\n",
       "       'HouseStyle_(scaled)', 'OverallQual(scaled)', 'OverallCond(scaled)',\n",
       "       'RoofStyle_(scaled)', 'MasVnrType_(scaled)', 'MasVnrArea(scaled)',\n",
       "       'ExterQual_(scaled)', 'Foundation_(scaled)', 'BsmtQual_(scaled)',\n",
       "       'BsmtExposure_(scaled)', 'BsmtFinType1_(scaled)', 'BsmtFinSF1(scaled)',\n",
       "       'BsmtUnfSF(scaled)', 'TotalBsmtSF(scaled)', 'HeatingQC_(scaled)',\n",
       "       '1stFlrSF(scaled)', '2ndFlrSF(scaled)', 'GrLivArea(scaled)',\n",
       "       'BsmtFullBath(scaled)', 'FullBath(scaled)', 'HalfBath(scaled)',\n",
       "       'BedroomAbvGr(scaled)', 'KitchenQual_(scaled)', 'TotRmsAbvGrd(scaled)',\n",
       "       'Fireplaces(scaled)', 'FireplaceQu_(scaled)', 'GarageType_(scaled)',\n",
       "       'House_Age(scaled)', 'Garage_Age(scaled)', 'GarageFinish_(scaled)',\n",
       "       'GarageCars(scaled)', 'GarageArea(scaled)', 'WoodDeckSF(scaled)',\n",
       "       'OpenPorchSF(scaled)', 'Exterior_Avg(scaled)', 'SalePrice(log)',\n",
       "       'SalePrice(log_stdz)', 'SalePrice(log_mm)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_house_extracted_wool.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "48e1c986",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_lst = [\n",
    "        'MSSubClass', 'MSZoning_', 'LotFrontage', 'LotArea', 'LotShape_',\n",
    "       'LotConfig_', 'Neighborhood_', 'HouseStyle_', 'OverallQual',\n",
    "       'OverallCond', 'RoofStyle_', 'MasVnrType_', 'MasVnrArea', 'ExterQual_',\n",
    "       'Foundation_', 'BsmtQual_', 'BsmtExposure_', 'BsmtFinType1_',\n",
    "       'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF', 'HeatingQC_', '1stFlrSF',\n",
    "       '2ndFlrSF', 'GrLivArea', 'BsmtFullBath', 'FullBath', 'HalfBath',\n",
    "       'BedroomAbvGr', 'KitchenQual_', 'TotRmsAbvGrd', 'Fireplaces',\n",
    "       'FireplaceQu_', 'GarageType_', 'House_Age', 'Garage_Age',\n",
    "       'GarageFinish_', 'GarageCars', 'GarageArea', 'WoodDeckSF',\n",
    "       'OpenPorchSF', 'Exterior_Avg','SaleCondition_'\n",
    "] \n",
    "#--------------------------------------------------------------\n",
    "#------------\n",
    "# create column and contain standardized data from the columns above, in sequence\n",
    "for i in range(len(col_lst)):\n",
    "    # standardize the data, using the library\n",
    "    array_tst = df_house4[col_lst[i]].values.reshape(-1,1)\n",
    "    scaled_data_tst = scaler.fit_transform(array_tst) \n",
    "    # making new column's name\n",
    "    new_col = col_lst[i] + '(scaled)'\n",
    "    # create column for containing the standardized data\n",
    "    df_house4[new_col] = scaled_data_tst\n",
    "###====================================================================\n",
    "###===================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1095fdd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n",
       "       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n",
       "       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
       "       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n",
       "       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n",
       "       'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n",
       "       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',\n",
       "       'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',\n",
       "       'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF',\n",
       "       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
       "       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',\n",
       "       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',\n",
       "       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',\n",
       "       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n",
       "       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',\n",
       "       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',\n",
       "       'SaleCondition'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_house_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3740d1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MSSubClass', 'MSZoning_', 'LotFrontage', 'LotArea', 'LotShape_',\n",
       "       'LotConfig_', 'Neighborhood_', 'HouseStyle_', 'OverallQual',\n",
       "       'OverallCond', 'RoofStyle_', 'MasVnrType_', 'MasVnrArea', 'ExterQual_',\n",
       "       'Foundation_', 'BsmtQual_', 'BsmtExposure_', 'BsmtFinType1_',\n",
       "       'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF', 'HeatingQC_', '1stFlrSF',\n",
       "       '2ndFlrSF', 'GrLivArea', 'BsmtFullBath', 'FullBath', 'HalfBath',\n",
       "       'BedroomAbvGr', 'KitchenQual_', 'TotRmsAbvGrd', 'Fireplaces',\n",
       "       'FireplaceQu_', 'GarageType_', 'House_Age', 'Garage_Age',\n",
       "       'GarageFinish_', 'GarageCars', 'GarageArea', 'WoodDeckSF',\n",
       "       'OpenPorchSF', 'Exterior_Avg'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_house_test_extracted.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edf6c5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "df_house_test['SaleCondition_'] = 0\n",
    "for i in range(len(df_house_test)):\n",
    "    if df_house_test.SaleCondition.iloc[i] == 'Normal':\n",
    "        df_house_test.SaleCondition_.iloc[i] = 1204\n",
    "    elif df_house_test.SaleCondition.iloc[i] == 'Partial':\n",
    "        df_house_test.SaleCondition_.iloc[i] = 120\n",
    "    elif df_house_test.SaleCondition.iloc[i] == 'Abnorml':\n",
    "        df_house_test.SaleCondition_.iloc[i] = 89\n",
    "    elif df_house_test.SaleCondition.iloc[i] == 'Family':\n",
    "        df_house_test.SaleCondition_.iloc[i] = 26\n",
    "    elif df_house_test.SaleCondition.iloc[i] == 'Alloca':\n",
    "        df_house_test.SaleCondition_.iloc[i] = 12\n",
    "    elif df_house_test.SaleCondition.iloc[i] == 'AdjLand':\n",
    "        df_house_test.SaleCondition_.iloc[i] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f161073f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_house_test_extracted_ = pd.concat([df_house_test_extracted,df_house_test[['SaleCondition_']]],axis=1)\n",
    "#--------------------------------------------------------------------------\n",
    "col_lst = [\n",
    "        'MSSubClass', 'MSZoning_', 'LotFrontage', 'LotArea', 'LotShape_',\n",
    "       'LotConfig_', 'Neighborhood_', 'HouseStyle_', 'OverallQual',\n",
    "       'OverallCond', 'RoofStyle_', 'MasVnrType_', 'MasVnrArea', 'ExterQual_',\n",
    "       'Foundation_', 'BsmtQual_', 'BsmtExposure_', 'BsmtFinType1_',\n",
    "       'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF', 'HeatingQC_', '1stFlrSF',\n",
    "       '2ndFlrSF', 'GrLivArea', 'BsmtFullBath', 'FullBath', 'HalfBath',\n",
    "       'BedroomAbvGr', 'KitchenQual_', 'TotRmsAbvGrd', 'Fireplaces',\n",
    "       'FireplaceQu_', 'GarageType_', 'House_Age', 'Garage_Age',\n",
    "       'GarageFinish_', 'GarageCars', 'GarageArea', 'WoodDeckSF',\n",
    "       'OpenPorchSF', 'Exterior_Avg','SaleCondition_'\n",
    "] \n",
    "#--------------------------------------------------------------\n",
    "#------------\n",
    "# create column and contain standardized data from the columns above, in sequence\n",
    "for i in range(len(col_lst)):\n",
    "    # standardize the data, using the library\n",
    "    array_tst = df_house_test_extracted_[col_lst[i]].values.reshape(-1,1)\n",
    "    scaled_data_tst = scaler.fit_transform(array_tst) \n",
    "    # making new column's name\n",
    "    new_col = col_lst[i] + '(scaled)'\n",
    "    # create column for containing the standardized data\n",
    "    df_house_test_extracted_[new_col] = scaled_data_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55dd0229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MSSubClass', 'MSZoning_', 'LotFrontage', 'LotArea', 'LotShape_',\n",
       "       'LotConfig_', 'Neighborhood_', 'HouseStyle_', 'OverallQual',\n",
       "       'OverallCond', 'RoofStyle_', 'MasVnrType_', 'MasVnrArea', 'ExterQual_',\n",
       "       'Foundation_', 'BsmtQual_', 'BsmtExposure_', 'BsmtFinType1_',\n",
       "       'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF', 'HeatingQC_', '1stFlrSF',\n",
       "       '2ndFlrSF', 'GrLivArea', 'BsmtFullBath', 'FullBath', 'HalfBath',\n",
       "       'BedroomAbvGr', 'KitchenQual_', 'TotRmsAbvGrd', 'Fireplaces',\n",
       "       'FireplaceQu_', 'GarageType_', 'House_Age', 'Garage_Age',\n",
       "       'GarageFinish_', 'GarageCars', 'GarageArea', 'WoodDeckSF',\n",
       "       'OpenPorchSF', 'Exterior_Avg', 'SalePrice', 'MSSubClass(scaled)',\n",
       "       'MSZoning_(scaled)', 'LotFrontage(scaled)', 'LotArea(scaled)',\n",
       "       'LotShape_(scaled)', 'LotConfig_(scaled)', 'Neighborhood_(scaled)',\n",
       "       'HouseStyle_(scaled)', 'OverallQual(scaled)', 'OverallCond(scaled)',\n",
       "       'RoofStyle_(scaled)', 'MasVnrType_(scaled)', 'MasVnrArea(scaled)',\n",
       "       'ExterQual_(scaled)', 'Foundation_(scaled)', 'BsmtQual_(scaled)',\n",
       "       'BsmtExposure_(scaled)', 'BsmtFinType1_(scaled)', 'BsmtFinSF1(scaled)',\n",
       "       'BsmtUnfSF(scaled)', 'TotalBsmtSF(scaled)', 'HeatingQC_(scaled)',\n",
       "       '1stFlrSF(scaled)', '2ndFlrSF(scaled)', 'GrLivArea(scaled)',\n",
       "       'BsmtFullBath(scaled)', 'FullBath(scaled)', 'HalfBath(scaled)',\n",
       "       'BedroomAbvGr(scaled)', 'KitchenQual_(scaled)', 'TotRmsAbvGrd(scaled)',\n",
       "       'Fireplaces(scaled)', 'FireplaceQu_(scaled)', 'GarageType_(scaled)',\n",
       "       'House_Age(scaled)', 'Garage_Age(scaled)', 'GarageFinish_(scaled)',\n",
       "       'GarageCars(scaled)', 'GarageArea(scaled)', 'WoodDeckSF(scaled)',\n",
       "       'OpenPorchSF(scaled)', 'Exterior_Avg(scaled)', 'SalePrice(log)',\n",
       "       'SalePrice(log_stdz)', 'SalePrice(log_mm)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_house_extracted_wool.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d22af150",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_house_extracted_wool_ = pd.concat([df_house_extracted_wool,df_house4[['SaleCondition_']]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cff6bbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_lst = [\n",
    "  'SaleCondition_'\n",
    "]\n",
    "#--------------------------------------------------------------\n",
    "# create column and contain standardized data from the columns above, in sequence\n",
    "for i in range(len(col_lst)):\n",
    "    # standardize the data, using the library\n",
    "    array_tst = df_house_extracted_wool_[col_lst[i]].values.reshape(-1,1)\n",
    "    scaled_data_tst = scaler.fit_transform(array_tst) \n",
    "    # making new column's name\n",
    "    new_col = col_lst[i] + '(scaled)'\n",
    "    # create column for containing the standardized data\n",
    "    df_house_extracted_wool_[new_col] = scaled_data_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a6bae9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea',\n",
       "       'Street', 'Alley', 'LotShape', 'LandContour',\n",
       "       ...\n",
       "       'GarageType_(scaled)', 'House_Age(scaled)', 'Garage_Age(scaled)',\n",
       "       'GarageFinish_(scaled)', 'GarageCars(scaled)', 'GarageArea(scaled)',\n",
       "       'WoodDeckSF(scaled)', 'OpenPorchSF(scaled)', 'Exterior_Avg(scaled)',\n",
       "       'SaleCondition_(scaled)'],\n",
       "      dtype='object', length=150)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_house4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a6a5d25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_house4.drop(['Unnamed: 0'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "a34b9cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n",
       "       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n",
       "       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
       "       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n",
       "       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n",
       "       'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n",
       "       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',\n",
       "       'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',\n",
       "       'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF',\n",
       "       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath',\n",
       "       'FullBath'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_house4.columns[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "75f2ee78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',\n",
       "       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',\n",
       "       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',\n",
       "       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n",
       "       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',\n",
       "       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',\n",
       "       'SaleCondition', 'SalePrice', 'MSZoning_', 'LotShape_', 'LotConfig_',\n",
       "       'Neighborhood_', 'BldgType_', 'HouseStyle_', 'House_Age', 'Garage_Age',\n",
       "       'RoofStyle_', 'Exterior1st_', 'Exterior2nd_', 'Exterior_Avg',\n",
       "       'MasVnrType_', 'ExterQual_', 'Foundation_', 'BsmtQual_', 'BsmtCond_',\n",
       "       'BsmtExposure_', 'BsmtFinType1_', 'HeatingQC_', 'KitchenQual_',\n",
       "       'FireplaceQu_', 'GarageType_', 'GarageFinish_', 'SaleCondition_',\n",
       "       'MSSubClass(scaled)', 'MSZoning_(scaled)', 'LotFrontage(scaled)',\n",
       "       'LotArea(scaled)', 'LotShape_(scaled)', 'LotConfig_(scaled)',\n",
       "       'Neighborhood_(scaled)', 'HouseStyle_(scaled)', 'OverallQual(scaled)',\n",
       "       'OverallCond(scaled)', 'RoofStyle_(scaled)', 'MasVnrType_(scaled)',\n",
       "       'MasVnrArea(scaled)', 'ExterQual_(scaled)', 'Foundation_(scaled)',\n",
       "       'BsmtQual_(scaled)', 'BsmtExposure_(scaled)', 'BsmtFinType1_(scaled)',\n",
       "       'BsmtFinSF1(scaled)', 'BsmtUnfSF(scaled)', 'TotalBsmtSF(scaled)',\n",
       "       'HeatingQC_(scaled)', '1stFlrSF(scaled)', '2ndFlrSF(scaled)',\n",
       "       'GrLivArea(scaled)', 'BsmtFullBath(scaled)', 'FullBath(scaled)',\n",
       "       'HalfBath(scaled)', 'BedroomAbvGr(scaled)', 'KitchenQual_(scaled)',\n",
       "       'TotRmsAbvGrd(scaled)', 'Fireplaces(scaled)', 'FireplaceQu_(scaled)',\n",
       "       'GarageType_(scaled)', 'House_Age(scaled)', 'Garage_Age(scaled)',\n",
       "       'GarageFinish_(scaled)', 'GarageCars(scaled)', 'GarageArea(scaled)',\n",
       "       'WoodDeckSF(scaled)', 'OpenPorchSF(scaled)', 'Exterior_Avg(scaled)',\n",
       "       'SaleCondition_(scaled)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_house4.columns[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "bbff3e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_SalePrice = np.log1p(df_house4['SalePrice'])\n",
    "df_house4['SalePrice(log)'] = log_SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "0b419edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SaleCondition_', 'MSSubClass(scaled)', 'MSZoning_(scaled)',\n",
       "       'LotFrontage(scaled)', 'LotArea(scaled)', 'LotShape_(scaled)',\n",
       "       'LotConfig_(scaled)', 'Neighborhood_(scaled)', 'HouseStyle_(scaled)',\n",
       "       'OverallQual(scaled)', 'OverallCond(scaled)', 'RoofStyle_(scaled)',\n",
       "       'MasVnrType_(scaled)', 'MasVnrArea(scaled)', 'ExterQual_(scaled)',\n",
       "       'Foundation_(scaled)', 'BsmtQual_(scaled)', 'BsmtExposure_(scaled)',\n",
       "       'BsmtFinType1_(scaled)', 'BsmtFinSF1(scaled)', 'BsmtUnfSF(scaled)',\n",
       "       'TotalBsmtSF(scaled)', 'HeatingQC_(scaled)', '1stFlrSF(scaled)',\n",
       "       '2ndFlrSF(scaled)', 'GrLivArea(scaled)', 'BsmtFullBath(scaled)',\n",
       "       'FullBath(scaled)', 'HalfBath(scaled)', 'BedroomAbvGr(scaled)',\n",
       "       'KitchenQual_(scaled)', 'TotRmsAbvGrd(scaled)', 'Fireplaces(scaled)',\n",
       "       'FireplaceQu_(scaled)', 'GarageType_(scaled)', 'House_Age(scaled)',\n",
       "       'Garage_Age(scaled)', 'GarageFinish_(scaled)', 'GarageCars(scaled)',\n",
       "       'GarageArea(scaled)', 'WoodDeckSF(scaled)', 'OpenPorchSF(scaled)',\n",
       "       'Exterior_Avg(scaled)', 'SaleCondition_(scaled)', 'SalePrice(log)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_house4.columns[105:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "903e1f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "3013057f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "29c12644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HeatingQC_', 'KitchenQual_', 'FireplaceQu_', 'GarageType_',\n",
       "       'GarageFinish_', 'SaleCondition_', 'MSSubClass(scaled)',\n",
       "       'MSZoning_(scaled)', 'LotFrontage(scaled)', 'LotArea(scaled)',\n",
       "       'LotShape_(scaled)', 'LotConfig_(scaled)', 'Neighborhood_(scaled)',\n",
       "       'HouseStyle_(scaled)', 'OverallQual(scaled)', 'OverallCond(scaled)',\n",
       "       'RoofStyle_(scaled)', 'MasVnrType_(scaled)', 'MasVnrArea(scaled)',\n",
       "       'ExterQual_(scaled)', 'Foundation_(scaled)', 'BsmtQual_(scaled)',\n",
       "       'BsmtExposure_(scaled)', 'BsmtFinType1_(scaled)', 'BsmtFinSF1(scaled)',\n",
       "       'BsmtUnfSF(scaled)', 'TotalBsmtSF(scaled)', 'HeatingQC_(scaled)',\n",
       "       '1stFlrSF(scaled)', '2ndFlrSF(scaled)', 'GrLivArea(scaled)',\n",
       "       'BsmtFullBath(scaled)', 'FullBath(scaled)', 'HalfBath(scaled)',\n",
       "       'BedroomAbvGr(scaled)', 'KitchenQual_(scaled)', 'TotRmsAbvGrd(scaled)',\n",
       "       'Fireplaces(scaled)', 'FireplaceQu_(scaled)', 'GarageType_(scaled)',\n",
       "       'House_Age(scaled)', 'Garage_Age(scaled)', 'GarageFinish_(scaled)',\n",
       "       'GarageCars(scaled)', 'GarageArea(scaled)', 'WoodDeckSF(scaled)',\n",
       "       'OpenPorchSF(scaled)', 'Exterior_Avg(scaled)', 'SaleCondition_(scaled)',\n",
       "       'SalePrice(log)', 'MSSubClass(mm)', 'MSZoning_(mm)', 'LotFrontage(mm)',\n",
       "       'LotArea(mm)', 'LotShape_(mm)', 'LotConfig_(mm)', 'Neighborhood_(mm)',\n",
       "       'HouseStyle_(mm)', 'OverallQual(mm)', 'OverallCond(mm)',\n",
       "       'RoofStyle_(mm)', 'MasVnrType_(mm)', 'MasVnrArea(mm)', 'ExterQual_(mm)',\n",
       "       'Foundation_(mm)', 'BsmtQual_(mm)', 'BsmtExposure_(mm)',\n",
       "       'BsmtFinType1_(mm)', 'BsmtFinSF1(mm)', 'BsmtUnfSF(mm)',\n",
       "       'TotalBsmtSF(mm)', 'HeatingQC_(mm)', '1stFlrSF(mm)', '2ndFlrSF(mm)',\n",
       "       'GrLivArea(mm)', 'BsmtFullBath(mm)', 'FullBath(mm)', 'HalfBath(mm)',\n",
       "       'BedroomAbvGr(mm)', 'KitchenQual_(mm)', 'TotRmsAbvGrd(mm)',\n",
       "       'Fireplaces(mm)', 'FireplaceQu_(mm)', 'GarageType_(mm)',\n",
       "       'House_Age(mm)', 'Garage_Age(mm)', 'GarageFinish_(mm)',\n",
       "       'GarageCars(mm)', 'GarageArea(mm)', 'WoodDeckSF(mm)', 'OpenPorchSF(mm)',\n",
       "       'Exterior_Avg(mm)', 'SaleCondition_(mm)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_house4.columns[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "9c5c53d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_lst = [\n",
    "  'MSSubClass', 'MSZoning_',\n",
    "       'LotFrontage', 'LotArea', 'LotShape_',\n",
    "       'LotConfig_', 'Neighborhood_', 'HouseStyle_',\n",
    "       'OverallQual', 'OverallCond', 'RoofStyle_',\n",
    "       'MasVnrType_', 'MasVnrArea', 'ExterQual_',\n",
    "       'Foundation_', 'BsmtQual_', 'BsmtExposure_',\n",
    "       'BsmtFinType1_', 'BsmtFinSF1', 'BsmtUnfSF',\n",
    "       'TotalBsmtSF', 'HeatingQC_', '1stFlrSF',\n",
    "       '2ndFlrSF', 'GrLivArea', 'BsmtFullBath',\n",
    "       'FullBath', 'HalfBath', 'BedroomAbvGr',\n",
    "       'KitchenQual_', 'TotRmsAbvGrd', 'Fireplaces',\n",
    "       'FireplaceQu_', 'GarageType_', 'House_Age',\n",
    "       'Garage_Age', 'GarageFinish_', 'GarageCars',\n",
    "       'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n",
    "       'Exterior_Avg', 'SaleCondition_'\n",
    "]\n",
    "#--------------------------------------------------------------\n",
    "# create column and contain standardized data from the columns above, in sequence\n",
    "for i in range(len(col_lst)):\n",
    "    # standardize the data, using the library\n",
    "    array_tst = df_house4[col_lst[i]].values.reshape(-1,1)\n",
    "    scaled_data_tst = mm_scaler.fit_transform(array_tst) \n",
    "    # making new column's name\n",
    "    new_col = col_lst[i] + '(mm)'\n",
    "    # create column for containing the standardized data\n",
    "    df_house4[new_col] = scaled_data_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "d1110a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_house4[[\n",
    "        'MSSubClass(mm)', 'MSZoning_(mm)',\n",
    "       'LotFrontage(mm)', 'LotArea(mm)', 'LotShape_(mm)',\n",
    "       'LotConfig_(mm)', 'Neighborhood_(mm)', 'HouseStyle_(mm)',\n",
    "       'OverallQual(mm)', 'OverallCond(mm)', 'RoofStyle_(mm)',\n",
    "       'MasVnrType_(mm)', 'MasVnrArea(mm)', 'ExterQual_(mm)',\n",
    "       'Foundation_(mm)', 'BsmtQual_(mm)', 'BsmtExposure_(mm)',\n",
    "       'BsmtFinType1_(mm)', 'BsmtFinSF1(mm)', 'BsmtUnfSF(mm)',\n",
    "       'TotalBsmtSF(mm)', 'HeatingQC_(mm)', '1stFlrSF(mm)',\n",
    "       '2ndFlrSF(mm)', 'GrLivArea(mm)', 'BsmtFullBath(mm)',\n",
    "       'FullBath(mm)', 'HalfBath(mm)', 'BedroomAbvGr(mm)',\n",
    "       'KitchenQual_(mm)', 'TotRmsAbvGrd(mm)', 'Fireplaces(mm)',\n",
    "       'FireplaceQu_(mm)', 'GarageType_(mm)', 'House_Age(mm)',\n",
    "       'Garage_Age(mm)', 'GarageFinish_(mm)', 'GarageCars(mm)',\n",
    "       'GarageArea(mm)', 'WoodDeckSF(mm)', 'OpenPorchSF(mm)',\n",
    "       'Exterior_Avg(mm)', 'SaleCondition_(mm)'\n",
    "]]\n",
    "y = df_house4['SalePrice(log)']\n",
    "### df_house_extracted_wool_\n",
    "# split X and y \n",
    "# make sure that 'test' here is different from 'test' in df_house_test\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "488466bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tst = df_house_extracted_wool_[[\n",
    "        'MSSubClass(scaled)',\n",
    "        'MasVnrArea(scaled)','BsmtFinSF1(scaled)','BsmtUnfSF(scaled)', '2ndFlrSF(scaled)', 'BsmtFullBath(scaled)',\n",
    "        'FullBath(scaled)', 'HalfBath(scaled)','Fireplaces(scaled)','WoodDeckSF(scaled)','OpenPorchSF(scaled)', \n",
    "        \n",
    "        'House_Age(scaled)','Garage_Age(scaled)','Exterior_Avg(scaled)',\n",
    "    \n",
    "    \n",
    "       'MSZoning_(scaled)', 'LotFrontage(scaled)', 'LotArea(scaled)',\n",
    "       'LotShape_(scaled)', 'LotConfig_(scaled)', 'Neighborhood_(scaled)',\n",
    "       'HouseStyle_(scaled)', 'OverallQual(scaled)', 'OverallCond(scaled)',\n",
    "       'RoofStyle_(scaled)', 'MasVnrType_(scaled)', \n",
    "       'ExterQual_(scaled)', 'Foundation_(scaled)', 'BsmtQual_(scaled)',\n",
    "       'BsmtExposure_(scaled)', 'BsmtFinType1_(scaled)', \n",
    "       'TotalBsmtSF(scaled)', 'HeatingQC_(scaled)',\n",
    "       '1stFlrSF(scaled)', 'GrLivArea(scaled)',\n",
    "        \n",
    "       'BedroomAbvGr(scaled)', 'KitchenQual_(scaled)', 'TotRmsAbvGrd(scaled)',\n",
    "        'FireplaceQu_(scaled)', 'GarageType_(scaled)',\n",
    "       'GarageFinish_(scaled)',\n",
    "       'GarageCars(scaled)', 'GarageArea(scaled)', \n",
    "        'SaleCondition_(scaled)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "6bb71626",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_house_extracted_wool['SaleCondition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "cad2a753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass(scaled)</th>\n",
       "      <th>MasVnrArea(scaled)</th>\n",
       "      <th>BsmtFinSF1(scaled)</th>\n",
       "      <th>BsmtUnfSF(scaled)</th>\n",
       "      <th>2ndFlrSF(scaled)</th>\n",
       "      <th>BsmtFullBath(scaled)</th>\n",
       "      <th>FullBath(scaled)</th>\n",
       "      <th>HalfBath(scaled)</th>\n",
       "      <th>Fireplaces(scaled)</th>\n",
       "      <th>WoodDeckSF(scaled)</th>\n",
       "      <th>...</th>\n",
       "      <th>GrLivArea(scaled)</th>\n",
       "      <th>BedroomAbvGr(scaled)</th>\n",
       "      <th>KitchenQual_(scaled)</th>\n",
       "      <th>TotRmsAbvGrd(scaled)</th>\n",
       "      <th>FireplaceQu_(scaled)</th>\n",
       "      <th>GarageType_(scaled)</th>\n",
       "      <th>GarageFinish_(scaled)</th>\n",
       "      <th>GarageCars(scaled)</th>\n",
       "      <th>GarageArea(scaled)</th>\n",
       "      <th>SaleCondition_(scaled)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.074321</td>\n",
       "      <td>0.566802</td>\n",
       "      <td>0.635842</td>\n",
       "      <td>-0.943828</td>\n",
       "      <td>1.202667</td>\n",
       "      <td>1.128963</td>\n",
       "      <td>0.809818</td>\n",
       "      <td>1.238364</td>\n",
       "      <td>-0.945886</td>\n",
       "      <td>-0.769255</td>\n",
       "      <td>...</td>\n",
       "      <td>0.429431</td>\n",
       "      <td>0.171519</td>\n",
       "      <td>0.747701</td>\n",
       "      <td>0.939501</td>\n",
       "      <td>-0.997971</td>\n",
       "      <td>0.776405</td>\n",
       "      <td>-0.236464</td>\n",
       "      <td>0.319312</td>\n",
       "      <td>0.365850</td>\n",
       "      <td>0.466272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.871732</td>\n",
       "      <td>-0.589223</td>\n",
       "      <td>1.271728</td>\n",
       "      <td>-0.640608</td>\n",
       "      <td>-0.795163</td>\n",
       "      <td>-0.820325</td>\n",
       "      <td>0.809818</td>\n",
       "      <td>-0.754088</td>\n",
       "      <td>0.623326</td>\n",
       "      <td>1.745354</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.484757</td>\n",
       "      <td>0.171519</td>\n",
       "      <td>-0.766630</td>\n",
       "      <td>-0.308545</td>\n",
       "      <td>0.660196</td>\n",
       "      <td>0.776405</td>\n",
       "      <td>-0.236464</td>\n",
       "      <td>0.319312</td>\n",
       "      <td>-0.050089</td>\n",
       "      <td>0.466272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.074321</td>\n",
       "      <td>0.366267</td>\n",
       "      <td>0.121523</td>\n",
       "      <td>-0.301182</td>\n",
       "      <td>1.230739</td>\n",
       "      <td>1.128963</td>\n",
       "      <td>0.809818</td>\n",
       "      <td>1.238364</td>\n",
       "      <td>0.623326</td>\n",
       "      <td>-0.769255</td>\n",
       "      <td>...</td>\n",
       "      <td>0.584517</td>\n",
       "      <td>0.171519</td>\n",
       "      <td>0.747701</td>\n",
       "      <td>-0.308545</td>\n",
       "      <td>0.660196</td>\n",
       "      <td>0.776405</td>\n",
       "      <td>-0.236464</td>\n",
       "      <td>0.319312</td>\n",
       "      <td>0.649445</td>\n",
       "      <td>0.466272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.310834</td>\n",
       "      <td>-0.589223</td>\n",
       "      <td>-0.509688</td>\n",
       "      <td>-0.061321</td>\n",
       "      <td>0.973407</td>\n",
       "      <td>1.128963</td>\n",
       "      <td>-1.026620</td>\n",
       "      <td>-0.754088</td>\n",
       "      <td>0.623326</td>\n",
       "      <td>-0.769255</td>\n",
       "      <td>...</td>\n",
       "      <td>0.443715</td>\n",
       "      <td>0.171519</td>\n",
       "      <td>0.747701</td>\n",
       "      <td>0.315478</td>\n",
       "      <td>1.212918</td>\n",
       "      <td>-0.763634</td>\n",
       "      <td>0.952194</td>\n",
       "      <td>1.659865</td>\n",
       "      <td>0.810149</td>\n",
       "      <td>-2.141564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.074321</td>\n",
       "      <td>1.475108</td>\n",
       "      <td>0.516614</td>\n",
       "      <td>-0.174463</td>\n",
       "      <td>1.668203</td>\n",
       "      <td>1.128963</td>\n",
       "      <td>0.809818</td>\n",
       "      <td>1.238364</td>\n",
       "      <td>0.623326</td>\n",
       "      <td>0.850895</td>\n",
       "      <td>...</td>\n",
       "      <td>1.425243</td>\n",
       "      <td>1.406454</td>\n",
       "      <td>0.747701</td>\n",
       "      <td>1.563524</td>\n",
       "      <td>0.660196</td>\n",
       "      <td>0.776405</td>\n",
       "      <td>-0.236464</td>\n",
       "      <td>1.659865</td>\n",
       "      <td>1.727105</td>\n",
       "      <td>0.466272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>-0.871732</td>\n",
       "      <td>-0.589223</td>\n",
       "      <td>-0.056151</td>\n",
       "      <td>0.551908</td>\n",
       "      <td>-0.795163</td>\n",
       "      <td>1.128963</td>\n",
       "      <td>0.809818</td>\n",
       "      <td>-0.754088</td>\n",
       "      <td>-0.945886</td>\n",
       "      <td>-0.769255</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.568421</td>\n",
       "      <td>-1.063416</td>\n",
       "      <td>0.747701</td>\n",
       "      <td>-0.308545</td>\n",
       "      <td>-0.997971</td>\n",
       "      <td>0.776405</td>\n",
       "      <td>-0.236464</td>\n",
       "      <td>0.319312</td>\n",
       "      <td>-0.333683</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>0.074321</td>\n",
       "      <td>-0.589223</td>\n",
       "      <td>-1.014656</td>\n",
       "      <td>0.873230</td>\n",
       "      <td>0.828366</td>\n",
       "      <td>-0.820325</td>\n",
       "      <td>0.809818</td>\n",
       "      <td>1.238364</td>\n",
       "      <td>0.623326</td>\n",
       "      <td>-0.769255</td>\n",
       "      <td>...</td>\n",
       "      <td>0.300873</td>\n",
       "      <td>0.171519</td>\n",
       "      <td>-0.766630</td>\n",
       "      <td>0.315478</td>\n",
       "      <td>0.660196</td>\n",
       "      <td>0.776405</td>\n",
       "      <td>-0.236464</td>\n",
       "      <td>0.319312</td>\n",
       "      <td>-0.050089</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>-0.871732</td>\n",
       "      <td>0.112649</td>\n",
       "      <td>0.832219</td>\n",
       "      <td>0.049558</td>\n",
       "      <td>-0.795163</td>\n",
       "      <td>1.128963</td>\n",
       "      <td>0.809818</td>\n",
       "      <td>-0.754088</td>\n",
       "      <td>2.192538</td>\n",
       "      <td>2.175706</td>\n",
       "      <td>...</td>\n",
       "      <td>1.170168</td>\n",
       "      <td>0.171519</td>\n",
       "      <td>-0.766630</td>\n",
       "      <td>0.315478</td>\n",
       "      <td>0.660196</td>\n",
       "      <td>0.776405</td>\n",
       "      <td>0.952194</td>\n",
       "      <td>0.319312</td>\n",
       "      <td>0.138975</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>0.310834</td>\n",
       "      <td>-0.589223</td>\n",
       "      <td>-0.371757</td>\n",
       "      <td>0.701255</td>\n",
       "      <td>1.899802</td>\n",
       "      <td>-0.820325</td>\n",
       "      <td>0.809818</td>\n",
       "      <td>-0.754088</td>\n",
       "      <td>2.192538</td>\n",
       "      <td>-0.769255</td>\n",
       "      <td>...</td>\n",
       "      <td>1.715008</td>\n",
       "      <td>1.406454</td>\n",
       "      <td>0.747701</td>\n",
       "      <td>1.563524</td>\n",
       "      <td>1.212918</td>\n",
       "      <td>0.776405</td>\n",
       "      <td>-0.236464</td>\n",
       "      <td>-1.021240</td>\n",
       "      <td>-1.033217</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>-0.871732</td>\n",
       "      <td>-0.589223</td>\n",
       "      <td>-0.900103</td>\n",
       "      <td>-1.283253</td>\n",
       "      <td>-0.795163</td>\n",
       "      <td>1.128963</td>\n",
       "      <td>-1.026620</td>\n",
       "      <td>-0.754088</td>\n",
       "      <td>-0.945886</td>\n",
       "      <td>2.319157</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.860227</td>\n",
       "      <td>-1.063416</td>\n",
       "      <td>0.747701</td>\n",
       "      <td>-0.932567</td>\n",
       "      <td>-0.997971</td>\n",
       "      <td>0.776405</td>\n",
       "      <td>0.952194</td>\n",
       "      <td>-1.021240</td>\n",
       "      <td>-1.089936</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1440 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSSubClass(scaled)  MasVnrArea(scaled)  BsmtFinSF1(scaled)  \\\n",
       "0               0.074321            0.566802            0.635842   \n",
       "1              -0.871732           -0.589223            1.271728   \n",
       "2               0.074321            0.366267            0.121523   \n",
       "3               0.310834           -0.589223           -0.509688   \n",
       "4               0.074321            1.475108            0.516614   \n",
       "...                  ...                 ...                 ...   \n",
       "1435           -0.871732           -0.589223           -0.056151   \n",
       "1436            0.074321           -0.589223           -1.014656   \n",
       "1437           -0.871732            0.112649            0.832219   \n",
       "1438            0.310834           -0.589223           -0.371757   \n",
       "1439           -0.871732           -0.589223           -0.900103   \n",
       "\n",
       "      BsmtUnfSF(scaled)  2ndFlrSF(scaled)  BsmtFullBath(scaled)  \\\n",
       "0             -0.943828          1.202667              1.128963   \n",
       "1             -0.640608         -0.795163             -0.820325   \n",
       "2             -0.301182          1.230739              1.128963   \n",
       "3             -0.061321          0.973407              1.128963   \n",
       "4             -0.174463          1.668203              1.128963   \n",
       "...                 ...               ...                   ...   \n",
       "1435           0.551908         -0.795163              1.128963   \n",
       "1436           0.873230          0.828366             -0.820325   \n",
       "1437           0.049558         -0.795163              1.128963   \n",
       "1438           0.701255          1.899802             -0.820325   \n",
       "1439          -1.283253         -0.795163              1.128963   \n",
       "\n",
       "      FullBath(scaled)  HalfBath(scaled)  Fireplaces(scaled)  \\\n",
       "0             0.809818          1.238364           -0.945886   \n",
       "1             0.809818         -0.754088            0.623326   \n",
       "2             0.809818          1.238364            0.623326   \n",
       "3            -1.026620         -0.754088            0.623326   \n",
       "4             0.809818          1.238364            0.623326   \n",
       "...                ...               ...                 ...   \n",
       "1435          0.809818         -0.754088           -0.945886   \n",
       "1436          0.809818          1.238364            0.623326   \n",
       "1437          0.809818         -0.754088            2.192538   \n",
       "1438          0.809818         -0.754088            2.192538   \n",
       "1439         -1.026620         -0.754088           -0.945886   \n",
       "\n",
       "      WoodDeckSF(scaled)  ...  GrLivArea(scaled)  BedroomAbvGr(scaled)  \\\n",
       "0              -0.769255  ...           0.429431              0.171519   \n",
       "1               1.745354  ...          -0.484757              0.171519   \n",
       "2              -0.769255  ...           0.584517              0.171519   \n",
       "3              -0.769255  ...           0.443715              0.171519   \n",
       "4               0.850895  ...           1.425243              1.406454   \n",
       "...                  ...  ...                ...                   ...   \n",
       "1435           -0.769255  ...          -0.568421             -1.063416   \n",
       "1436           -0.769255  ...           0.300873              0.171519   \n",
       "1437            2.175706  ...           1.170168              0.171519   \n",
       "1438           -0.769255  ...           1.715008              1.406454   \n",
       "1439            2.319157  ...          -0.860227             -1.063416   \n",
       "\n",
       "      KitchenQual_(scaled)  TotRmsAbvGrd(scaled)  FireplaceQu_(scaled)  \\\n",
       "0                 0.747701              0.939501             -0.997971   \n",
       "1                -0.766630             -0.308545              0.660196   \n",
       "2                 0.747701             -0.308545              0.660196   \n",
       "3                 0.747701              0.315478              1.212918   \n",
       "4                 0.747701              1.563524              0.660196   \n",
       "...                    ...                   ...                   ...   \n",
       "1435              0.747701             -0.308545             -0.997971   \n",
       "1436             -0.766630              0.315478              0.660196   \n",
       "1437             -0.766630              0.315478              0.660196   \n",
       "1438              0.747701              1.563524              1.212918   \n",
       "1439              0.747701             -0.932567             -0.997971   \n",
       "\n",
       "      GarageType_(scaled)  GarageFinish_(scaled)  GarageCars(scaled)  \\\n",
       "0                0.776405              -0.236464            0.319312   \n",
       "1                0.776405              -0.236464            0.319312   \n",
       "2                0.776405              -0.236464            0.319312   \n",
       "3               -0.763634               0.952194            1.659865   \n",
       "4                0.776405              -0.236464            1.659865   \n",
       "...                   ...                    ...                 ...   \n",
       "1435             0.776405              -0.236464            0.319312   \n",
       "1436             0.776405              -0.236464            0.319312   \n",
       "1437             0.776405               0.952194            0.319312   \n",
       "1438             0.776405              -0.236464           -1.021240   \n",
       "1439             0.776405               0.952194           -1.021240   \n",
       "\n",
       "      GarageArea(scaled)  SaleCondition_(scaled)  \n",
       "0               0.365850                0.466272  \n",
       "1              -0.050089                0.466272  \n",
       "2               0.649445                0.466272  \n",
       "3               0.810149               -2.141564  \n",
       "4               1.727105                0.466272  \n",
       "...                  ...                     ...  \n",
       "1435           -0.333683                     NaN  \n",
       "1436           -0.050089                     NaN  \n",
       "1437            0.138975                     NaN  \n",
       "1438           -1.033217                     NaN  \n",
       "1439           -1.089936                     NaN  \n",
       "\n",
       "[1440 rows x 43 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "415c9078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07432099,  0.56680218,  0.63584226, ...,  0.31931218,\n",
       "         0.36585029,  0.46627249],\n",
       "       [-0.87173183, -0.58922311,  1.27172835, ...,  0.31931218,\n",
       "        -0.0500886 ,  0.46627249],\n",
       "       [ 0.07432099,  0.36626718,  0.12152262, ...,  0.31931218,\n",
       "         0.64944498,  0.46627249],\n",
       "       ...,\n",
       "       [-0.87173183,  0.11264939,  0.83221885, ...,  0.31931218,\n",
       "         0.13897453,         nan],\n",
       "       [ 0.3108342 , -0.58922311, -0.37175666, ..., -1.02124043,\n",
       "        -1.03321688,         nan],\n",
       "       [-0.87173183, -0.58922311, -0.90010319, ..., -1.02124043,\n",
       "        -1.08993582,         nan]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tst.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ba22a219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_tst.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "ba4e5b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = df_tst.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "577a8bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1440, 43)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "35a03703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07432099,  0.56680218,  0.63584226, ...,  0.31931218,\n",
       "         0.36585029,  0.46627249],\n",
       "       [-0.87173183, -0.58922311,  1.27172835, ...,  0.31931218,\n",
       "        -0.0500886 ,  0.46627249],\n",
       "       [ 0.07432099,  0.36626718,  0.12152262, ...,  0.31931218,\n",
       "         0.64944498,  0.46627249],\n",
       "       ...,\n",
       "       [-0.87173183,  0.11264939,  0.83221885, ...,  0.31931218,\n",
       "         0.13897453,         nan],\n",
       "       [ 0.3108342 , -0.58922311, -0.37175666, ..., -1.02124043,\n",
       "        -1.03321688,         nan],\n",
       "       [-0.87173183, -0.58922311, -0.90010319, ..., -1.02124043,\n",
       "        -1.08993582,         nan]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:,0:43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f0ca95bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07432099,  0.56680218,  0.63584226, ..., -0.23646405,\n",
       "         0.31931218,  0.36585029],\n",
       "       [-0.87173183, -0.58922311,  1.27172835, ..., -0.23646405,\n",
       "         0.31931218, -0.0500886 ],\n",
       "       [ 0.07432099,  0.36626718,  0.12152262, ..., -0.23646405,\n",
       "         0.31931218,  0.64944498],\n",
       "       ...,\n",
       "       [-0.87173183,  0.11264939,  0.83221885, ...,  0.95219371,\n",
       "         0.31931218,  0.13897453],\n",
       "       [ 0.3108342 , -0.58922311, -0.37175666, ..., -0.23646405,\n",
       "        -1.02124043, -1.03321688],\n",
       "       [-0.87173183, -0.58922311, -0.90010319, ...,  0.95219371,\n",
       "        -1.02124043, -1.08993582]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:,0:42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8c4ca50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07432099,  0.56680218,  0.63584226, ..., -0.99797069,\n",
       "         0.77640533, -0.23646405],\n",
       "       [-0.87173183, -0.58922311,  1.27172835, ...,  0.660196  ,\n",
       "         0.77640533, -0.23646405],\n",
       "       [ 0.07432099,  0.36626718,  0.12152262, ...,  0.660196  ,\n",
       "         0.77640533, -0.23646405],\n",
       "       ...,\n",
       "       [-0.87173183,  0.11264939,  0.83221885, ...,  0.660196  ,\n",
       "         0.77640533,  0.95219371],\n",
       "       [ 0.3108342 , -0.58922311, -0.37175666, ...,  1.21291822,\n",
       "         0.77640533, -0.23646405],\n",
       "       [-0.87173183, -0.58922311, -0.90010319, ..., -0.99797069,\n",
       "         0.77640533,  0.95219371]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:,0:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f9f14d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07432099,  0.56680218,  0.63584226, ...,  0.31931218,\n",
       "         0.36585029,  0.46627249],\n",
       "       [-0.87173183, -0.58922311,  1.27172835, ...,  0.31931218,\n",
       "        -0.0500886 ,  0.46627249],\n",
       "       [ 0.07432099,  0.36626718,  0.12152262, ...,  0.31931218,\n",
       "         0.64944498,  0.46627249],\n",
       "       ...,\n",
       "       [-0.87173183,  0.11264939,  0.83221885, ...,  0.31931218,\n",
       "         0.13897453,         nan],\n",
       "       [ 0.3108342 , -0.58922311, -0.37175666, ..., -1.02124043,\n",
       "        -1.03321688,         nan],\n",
       "       [-0.87173183, -0.58922311, -0.90010319, ..., -1.02124043,\n",
       "        -1.08993582,         nan]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:,0:44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "6dca0543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.665604  , -0.58922311,  0.73169274, -1.2832534 , -0.79516305,\n",
       "        1.12896252, -1.02661954, -0.75408784, -0.94588627,  1.2559331 ,\n",
       "       -0.73937084,  0.04936306, -0.14867797, -0.42341203,  0.51422431,\n",
       "        0.01953061, -0.1512151 , -1.090879  , -1.86820115, -0.64621774,\n",
       "       -1.99533651, -0.79336167,  1.27930664,  0.52286499, -0.73492143,\n",
       "       -0.68345558,  0.33381094,  0.59259013, -1.20778994, -0.64539436,\n",
       "       -0.50335086, -0.14814154, -0.73305427, -1.25610288, -1.06341634,\n",
       "        0.74770104, -0.93256729, -0.99797069,  0.77640533, -0.23646405,\n",
       "        0.31931218,  0.15788084,  0.46627249])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ba5d2c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.87173183, -0.58922311, -0.36006758, -0.90535959, -0.79516305,\n",
       "        1.12896252, -1.02661954, -0.75408784, -0.94588627,  0.45429612,\n",
       "       -0.73937084,  0.24304921,  0.13160517,  1.29832374,  0.51422431,\n",
       "        0.01953061, -0.14742192, -1.090879  , -1.86820115,  0.77631238,\n",
       "        0.8645287 , -0.79336167,  0.38023316, -1.81079354, -0.73492143,\n",
       "       -0.68345558,  0.33381094,  0.59259013, -1.20778994, -1.68799912,\n",
       "       -0.26468919, -1.18874549, -0.58629507, -1.14591058,  0.17151877,\n",
       "       -0.76663018, -0.93256729, -0.99797069, -0.76363367,  0.95219371,\n",
       "       -1.02124043, -0.7685285 ,  0.46627249])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b9573760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.87173183, -0.58922311, -0.59618705, -0.23103411, -0.79516305,\n",
       "        1.12896252, -1.02661954, -0.75408784, -0.94588627, -0.76925507,\n",
       "       -0.73937084,  1.16305837,  0.77225236, -0.95452375,  0.51422431,\n",
       "        0.01712946, -0.38544351,  0.71632475,  0.61433096,  1.94872731,\n",
       "        0.8645287 , -0.79336167,  0.38023316,  0.52286499, -0.73492143,\n",
       "       -0.68345558,  0.33381094, -0.54975229,  0.7200621 , -0.64539436,\n",
       "        0.25159933,  0.89246242, -0.01012933, -0.7133038 ,  0.17151877,\n",
       "       -0.76663018, -0.30854457, -0.99797069,  0.77640533, -0.23646405,\n",
       "       -1.02124043, -0.80634112,  0.46627249])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a5bf59cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.46627249, 0.46627249, 0.46627249, ...,        nan,        nan,\n",
       "              nan])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:,42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47abbbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "45f70dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass(scaled)</th>\n",
       "      <th>MSZoning_(scaled)</th>\n",
       "      <th>LotFrontage(scaled)</th>\n",
       "      <th>LotArea(scaled)</th>\n",
       "      <th>LotShape_(scaled)</th>\n",
       "      <th>LotConfig_(scaled)</th>\n",
       "      <th>Neighborhood_(scaled)</th>\n",
       "      <th>HouseStyle_(scaled)</th>\n",
       "      <th>OverallQual(scaled)</th>\n",
       "      <th>OverallCond(scaled)</th>\n",
       "      <th>...</th>\n",
       "      <th>GarageType_(scaled)</th>\n",
       "      <th>House_Age(scaled)</th>\n",
       "      <th>Garage_Age(scaled)</th>\n",
       "      <th>GarageFinish_(scaled)</th>\n",
       "      <th>GarageCars(scaled)</th>\n",
       "      <th>GarageArea(scaled)</th>\n",
       "      <th>WoodDeckSF(scaled)</th>\n",
       "      <th>OpenPorchSF(scaled)</th>\n",
       "      <th>Exterior_Avg(scaled)</th>\n",
       "      <th>SaleCondition_(scaled)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.073130</td>\n",
       "      <td>0.514966</td>\n",
       "      <td>-0.228033</td>\n",
       "      <td>-0.305269</td>\n",
       "      <td>0.713371</td>\n",
       "      <td>0.612423</td>\n",
       "      <td>0.773780</td>\n",
       "      <td>-0.299030</td>\n",
       "      <td>0.677196</td>\n",
       "      <td>-0.519022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.776614</td>\n",
       "      <td>-0.871228</td>\n",
       "      <td>-0.908183</td>\n",
       "      <td>-0.235940</td>\n",
       "      <td>0.321525</td>\n",
       "      <td>0.367774</td>\n",
       "      <td>-0.768275</td>\n",
       "      <td>0.271719</td>\n",
       "      <td>1.293156</td>\n",
       "      <td>0.466272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.873506</td>\n",
       "      <td>0.514966</td>\n",
       "      <td>0.528953</td>\n",
       "      <td>-0.045540</td>\n",
       "      <td>0.713371</td>\n",
       "      <td>-1.996090</td>\n",
       "      <td>-1.401297</td>\n",
       "      <td>0.865712</td>\n",
       "      <td>-0.057459</td>\n",
       "      <td>2.173875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.776614</td>\n",
       "      <td>0.386920</td>\n",
       "      <td>0.131432</td>\n",
       "      <td>-0.235940</td>\n",
       "      <td>0.321525</td>\n",
       "      <td>-0.047560</td>\n",
       "      <td>1.756236</td>\n",
       "      <td>-0.738333</td>\n",
       "      <td>-0.413219</td>\n",
       "      <td>0.466272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.073130</td>\n",
       "      <td>0.514966</td>\n",
       "      <td>-0.076636</td>\n",
       "      <td>0.327115</td>\n",
       "      <td>-1.108342</td>\n",
       "      <td>0.612423</td>\n",
       "      <td>0.773780</td>\n",
       "      <td>-0.299030</td>\n",
       "      <td>0.677196</td>\n",
       "      <td>-0.519022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.776614</td>\n",
       "      <td>-0.822838</td>\n",
       "      <td>-0.828213</td>\n",
       "      <td>-0.235940</td>\n",
       "      <td>0.321525</td>\n",
       "      <td>0.650956</td>\n",
       "      <td>-0.768275</td>\n",
       "      <td>-0.042887</td>\n",
       "      <td>1.293156</td>\n",
       "      <td>0.466272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.309789</td>\n",
       "      <td>0.514966</td>\n",
       "      <td>-0.480361</td>\n",
       "      <td>-0.056832</td>\n",
       "      <td>-1.108342</td>\n",
       "      <td>-1.435454</td>\n",
       "      <td>-0.775375</td>\n",
       "      <td>-0.299030</td>\n",
       "      <td>0.677196</td>\n",
       "      <td>-0.519022</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.762956</td>\n",
       "      <td>0.628871</td>\n",
       "      <td>-0.788228</td>\n",
       "      <td>0.951171</td>\n",
       "      <td>1.659967</td>\n",
       "      <td>0.811426</td>\n",
       "      <td>-0.768275</td>\n",
       "      <td>-0.158795</td>\n",
       "      <td>-0.967426</td>\n",
       "      <td>-2.141564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.073130</td>\n",
       "      <td>0.514966</td>\n",
       "      <td>0.730816</td>\n",
       "      <td>1.006927</td>\n",
       "      <td>-1.108342</td>\n",
       "      <td>-1.996090</td>\n",
       "      <td>-0.931856</td>\n",
       "      <td>-0.299030</td>\n",
       "      <td>1.411850</td>\n",
       "      <td>-0.519022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.776614</td>\n",
       "      <td>-0.726057</td>\n",
       "      <td>-0.788228</td>\n",
       "      <td>-0.235940</td>\n",
       "      <td>1.659967</td>\n",
       "      <td>1.727048</td>\n",
       "      <td>0.858255</td>\n",
       "      <td>0.652559</td>\n",
       "      <td>1.293156</td>\n",
       "      <td>0.466272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>-0.873506</td>\n",
       "      <td>-2.187358</td>\n",
       "      <td>-0.379430</td>\n",
       "      <td>-0.519827</td>\n",
       "      <td>0.713371</td>\n",
       "      <td>0.612423</td>\n",
       "      <td>-0.227694</td>\n",
       "      <td>0.865712</td>\n",
       "      <td>0.677196</td>\n",
       "      <td>-0.519022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.776614</td>\n",
       "      <td>-0.919619</td>\n",
       "      <td>-0.908183</td>\n",
       "      <td>-0.235940</td>\n",
       "      <td>0.321525</td>\n",
       "      <td>-0.330742</td>\n",
       "      <td>-0.768275</td>\n",
       "      <td>1.132748</td>\n",
       "      <td>1.293156</td>\n",
       "      <td>0.466272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428</th>\n",
       "      <td>0.073130</td>\n",
       "      <td>0.514966</td>\n",
       "      <td>-0.379430</td>\n",
       "      <td>-0.425648</td>\n",
       "      <td>0.713371</td>\n",
       "      <td>0.612423</td>\n",
       "      <td>-0.337230</td>\n",
       "      <td>-0.299030</td>\n",
       "      <td>-0.057459</td>\n",
       "      <td>-0.519022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.776614</td>\n",
       "      <td>-0.774448</td>\n",
       "      <td>-0.788228</td>\n",
       "      <td>-0.235940</td>\n",
       "      <td>0.321525</td>\n",
       "      <td>-0.047560</td>\n",
       "      <td>-0.768275</td>\n",
       "      <td>-0.076004</td>\n",
       "      <td>1.293156</td>\n",
       "      <td>0.466272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>-0.873506</td>\n",
       "      <td>0.514966</td>\n",
       "      <td>0.781282</td>\n",
       "      <td>0.761879</td>\n",
       "      <td>0.713371</td>\n",
       "      <td>0.612423</td>\n",
       "      <td>-0.431119</td>\n",
       "      <td>0.865712</td>\n",
       "      <td>-0.057459</td>\n",
       "      <td>0.378610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.776614</td>\n",
       "      <td>-0.048593</td>\n",
       "      <td>0.171417</td>\n",
       "      <td>0.951171</td>\n",
       "      <td>0.321525</td>\n",
       "      <td>0.141228</td>\n",
       "      <td>2.188283</td>\n",
       "      <td>-0.738333</td>\n",
       "      <td>-0.949925</td>\n",
       "      <td>0.466272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>0.309789</td>\n",
       "      <td>0.514966</td>\n",
       "      <td>-0.177567</td>\n",
       "      <td>-0.171565</td>\n",
       "      <td>0.713371</td>\n",
       "      <td>0.612423</td>\n",
       "      <td>-0.775375</td>\n",
       "      <td>-0.299030</td>\n",
       "      <td>0.677196</td>\n",
       "      <td>3.071507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.776614</td>\n",
       "      <td>-0.919619</td>\n",
       "      <td>1.650869</td>\n",
       "      <td>-0.235940</td>\n",
       "      <td>-1.016917</td>\n",
       "      <td>-1.029257</td>\n",
       "      <td>-0.768275</td>\n",
       "      <td>0.255161</td>\n",
       "      <td>-1.326203</td>\n",
       "      <td>0.466272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>-0.873506</td>\n",
       "      <td>0.514966</td>\n",
       "      <td>-0.076636</td>\n",
       "      <td>-0.019115</td>\n",
       "      <td>0.713371</td>\n",
       "      <td>0.612423</td>\n",
       "      <td>1.947383</td>\n",
       "      <td>0.865712</td>\n",
       "      <td>-0.792114</td>\n",
       "      <td>0.378610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.776614</td>\n",
       "      <td>-0.435716</td>\n",
       "      <td>1.291002</td>\n",
       "      <td>0.951171</td>\n",
       "      <td>-1.016917</td>\n",
       "      <td>-1.085893</td>\n",
       "      <td>2.332299</td>\n",
       "      <td>-0.738333</td>\n",
       "      <td>-0.413219</td>\n",
       "      <td>0.466272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1432 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSSubClass(scaled)  MSZoning_(scaled)  LotFrontage(scaled)  \\\n",
       "0               0.073130           0.514966            -0.228033   \n",
       "1              -0.873506           0.514966             0.528953   \n",
       "2               0.073130           0.514966            -0.076636   \n",
       "3               0.309789           0.514966            -0.480361   \n",
       "4               0.073130           0.514966             0.730816   \n",
       "...                  ...                ...                  ...   \n",
       "1427           -0.873506          -2.187358            -0.379430   \n",
       "1428            0.073130           0.514966            -0.379430   \n",
       "1429           -0.873506           0.514966             0.781282   \n",
       "1430            0.309789           0.514966            -0.177567   \n",
       "1431           -0.873506           0.514966            -0.076636   \n",
       "\n",
       "      LotArea(scaled)  LotShape_(scaled)  LotConfig_(scaled)  \\\n",
       "0           -0.305269           0.713371            0.612423   \n",
       "1           -0.045540           0.713371           -1.996090   \n",
       "2            0.327115          -1.108342            0.612423   \n",
       "3           -0.056832          -1.108342           -1.435454   \n",
       "4            1.006927          -1.108342           -1.996090   \n",
       "...               ...                ...                 ...   \n",
       "1427        -0.519827           0.713371            0.612423   \n",
       "1428        -0.425648           0.713371            0.612423   \n",
       "1429         0.761879           0.713371            0.612423   \n",
       "1430        -0.171565           0.713371            0.612423   \n",
       "1431        -0.019115           0.713371            0.612423   \n",
       "\n",
       "      Neighborhood_(scaled)  HouseStyle_(scaled)  OverallQual(scaled)  \\\n",
       "0                  0.773780            -0.299030             0.677196   \n",
       "1                 -1.401297             0.865712            -0.057459   \n",
       "2                  0.773780            -0.299030             0.677196   \n",
       "3                 -0.775375            -0.299030             0.677196   \n",
       "4                 -0.931856            -0.299030             1.411850   \n",
       "...                     ...                  ...                  ...   \n",
       "1427              -0.227694             0.865712             0.677196   \n",
       "1428              -0.337230            -0.299030            -0.057459   \n",
       "1429              -0.431119             0.865712            -0.057459   \n",
       "1430              -0.775375            -0.299030             0.677196   \n",
       "1431               1.947383             0.865712            -0.792114   \n",
       "\n",
       "      OverallCond(scaled)  ...  GarageType_(scaled)  House_Age(scaled)  \\\n",
       "0               -0.519022  ...             0.776614          -0.871228   \n",
       "1                2.173875  ...             0.776614           0.386920   \n",
       "2               -0.519022  ...             0.776614          -0.822838   \n",
       "3               -0.519022  ...            -0.762956           0.628871   \n",
       "4               -0.519022  ...             0.776614          -0.726057   \n",
       "...                   ...  ...                  ...                ...   \n",
       "1427            -0.519022  ...             0.776614          -0.919619   \n",
       "1428            -0.519022  ...             0.776614          -0.774448   \n",
       "1429             0.378610  ...             0.776614          -0.048593   \n",
       "1430             3.071507  ...             0.776614          -0.919619   \n",
       "1431             0.378610  ...             0.776614          -0.435716   \n",
       "\n",
       "      Garage_Age(scaled)  GarageFinish_(scaled)  GarageCars(scaled)  \\\n",
       "0              -0.908183              -0.235940            0.321525   \n",
       "1               0.131432              -0.235940            0.321525   \n",
       "2              -0.828213              -0.235940            0.321525   \n",
       "3              -0.788228               0.951171            1.659967   \n",
       "4              -0.788228              -0.235940            1.659967   \n",
       "...                  ...                    ...                 ...   \n",
       "1427           -0.908183              -0.235940            0.321525   \n",
       "1428           -0.788228              -0.235940            0.321525   \n",
       "1429            0.171417               0.951171            0.321525   \n",
       "1430            1.650869              -0.235940           -1.016917   \n",
       "1431            1.291002               0.951171           -1.016917   \n",
       "\n",
       "      GarageArea(scaled)  WoodDeckSF(scaled)  OpenPorchSF(scaled)  \\\n",
       "0               0.367774           -0.768275             0.271719   \n",
       "1              -0.047560            1.756236            -0.738333   \n",
       "2               0.650956           -0.768275            -0.042887   \n",
       "3               0.811426           -0.768275            -0.158795   \n",
       "4               1.727048            0.858255             0.652559   \n",
       "...                  ...                 ...                  ...   \n",
       "1427           -0.330742           -0.768275             1.132748   \n",
       "1428           -0.047560           -0.768275            -0.076004   \n",
       "1429            0.141228            2.188283            -0.738333   \n",
       "1430           -1.029257           -0.768275             0.255161   \n",
       "1431           -1.085893            2.332299            -0.738333   \n",
       "\n",
       "      Exterior_Avg(scaled)  SaleCondition_(scaled)  \n",
       "0                 1.293156                0.466272  \n",
       "1                -0.413219                0.466272  \n",
       "2                 1.293156                0.466272  \n",
       "3                -0.967426               -2.141564  \n",
       "4                 1.293156                0.466272  \n",
       "...                    ...                     ...  \n",
       "1427              1.293156                0.466272  \n",
       "1428              1.293156                0.466272  \n",
       "1429             -0.949925                0.466272  \n",
       "1430             -1.326203                0.466272  \n",
       "1431             -0.413219                0.466272  \n",
       "\n",
       "[1432 rows x 43 columns]"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "9bb9fb69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       12.247699\n",
       "1       12.109016\n",
       "2       12.317171\n",
       "3       11.849405\n",
       "4       12.429220\n",
       "          ...    \n",
       "1427    12.128117\n",
       "1428    12.072547\n",
       "1429    12.254868\n",
       "1430    12.493133\n",
       "1431    11.864469\n",
       "Name: SalePrice(log), Length: 1432, dtype: float64"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "5811607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b41f92bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07312961,  0.57873318,  0.64213459, ...,  0.3215251 ,\n",
       "         0.36777403,  0.46627249],\n",
       "       [-0.87350636, -0.59367906,  1.27919727, ...,  0.3215251 ,\n",
       "        -0.04755954,  0.46627249],\n",
       "       [ 0.07312961,  0.37535555,  0.12686331, ...,  0.3215251 ,\n",
       "         0.65095601,  0.46627249],\n",
       "       ...,\n",
       "       [-0.87350636,  0.11814266,  0.83887454, ...,  0.3215251 ,\n",
       "         0.14122844,  0.46627249],\n",
       "       [ 0.30978861, -0.59367906, -0.36732869, ..., -1.01691658,\n",
       "        -1.02925707,  0.46627249],\n",
       "       [-0.87350636, -0.59367906, -0.89665283, ..., -1.01691658,\n",
       "        -1.08589347,  0.46627249]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "e755effa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1432, 43)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "5389e363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07312961,  0.57873318,  0.64213459, ...,  0.3215251 ,\n",
       "         0.36777403,  0.46627249],\n",
       "       [-0.87350636, -0.59367906,  1.27919727, ...,  0.3215251 ,\n",
       "        -0.04755954,  0.46627249],\n",
       "       [ 0.07312961,  0.37535555,  0.12686331, ...,  0.3215251 ,\n",
       "         0.65095601,  0.46627249],\n",
       "       ...,\n",
       "       [-0.87350636,  0.11814266,  0.83887454, ...,  0.3215251 ,\n",
       "         0.14122844,  0.46627249],\n",
       "       [ 0.30978861, -0.59367906, -0.36732869, ..., -1.01691658,\n",
       "        -1.02925707,  0.46627249],\n",
       "       [-0.87350636, -0.59367906, -0.89665283, ..., -1.01691658,\n",
       "        -1.08589347,  0.46627249]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:,0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "382247da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07312961,  0.57873318,  0.64213459, ...,  0.3215251 ,\n",
       "         0.36777403,  0.46627249],\n",
       "       [-0.87350636, -0.59367906,  1.27919727, ...,  0.3215251 ,\n",
       "        -0.04755954,  0.46627249],\n",
       "       [ 0.07312961,  0.37535555,  0.12686331, ...,  0.3215251 ,\n",
       "         0.65095601,  0.46627249],\n",
       "       ...,\n",
       "       [-0.87350636,  0.11814266,  0.83887454, ...,  0.3215251 ,\n",
       "         0.14122844,  0.46627249],\n",
       "       [ 0.30978861, -0.59367906, -0.36732869, ..., -1.01691658,\n",
       "        -1.02925707,  0.46627249],\n",
       "       [-0.87350636, -0.59367906, -0.89665283, ..., -1.01691658,\n",
       "        -1.08589347,  0.46627249]])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:,0:43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "9513cf4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.87350636, -0.59367906, -0.35561798, -0.90744288, -0.79608549,\n",
       "        1.13455418, -1.02595927, -0.7539801 , -0.9422651 ,  0.46009426,\n",
       "       -0.73833325,  0.24174878,  0.13143177,  1.29315561,  0.51496564,\n",
       "        0.02681699, -0.1358803 , -1.10834183, -1.87409998,  0.77378001,\n",
       "        0.86571231, -0.79211366,  0.37861025, -1.82356674, -0.73583701,\n",
       "       -0.68347799,  0.33508438,  0.59401909, -1.21428601, -1.68712781,\n",
       "       -0.26211733, -1.18690963, -0.58581186, -1.14707286,  0.17256056,\n",
       "       -0.76380438, -0.93279853, -0.99309724, -0.76295621,  0.9511707 ,\n",
       "       -1.01691658, -0.76495389,  0.46627249])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b3bb5471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.6647771 , -0.59367906,  0.73816242, -1.28615152, -0.79608549,\n",
       "        1.13455418, -1.02595927, -0.7539801 , -0.9422651 ,  1.26488799,\n",
       "       -0.73833325,  0.04818755, -0.14846457, -0.42780365,  0.51496564,\n",
       "        0.02681699, -0.14039733, -1.10834183, -1.87409998, -0.65019113,\n",
       "       -1.99018543, -0.79211366,  1.27624248,  0.52054891, -0.73583701,\n",
       "       -0.68347799,  0.33508438,  0.59401909, -1.21428601, -0.64529212,\n",
       "       -0.50312803, -0.14672934, -0.73459241, -1.25804949, -1.06297307,\n",
       "        0.75110957, -0.93279853, -0.99309724,  0.77661446, -0.23593991,\n",
       "        0.3215251 ,  0.16010724,  0.46627249])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "b2793354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07312961,  0.57873318,  0.64213459, ...,  0.3215251 ,\n",
       "         0.36777403,  0.46627249],\n",
       "       [-0.87350636, -0.59367906,  1.27919727, ...,  0.3215251 ,\n",
       "        -0.04755954,  0.46627249],\n",
       "       [ 0.07312961,  0.37535555,  0.12686331, ...,  0.3215251 ,\n",
       "         0.65095601,  0.46627249],\n",
       "       ...,\n",
       "       [-0.87350636,  0.11814266,  0.83887454, ...,  0.3215251 ,\n",
       "         0.14122844,  0.46627249],\n",
       "       [ 0.30978861, -0.59367906, -0.36732869, ..., -1.01691658,\n",
       "        -1.02925707,  0.46627249],\n",
       "       [-0.87350636, -0.59367906, -0.89665283, ..., -1.01691658,\n",
       "        -1.08589347,  0.46627249]])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "cf32810d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.07312961, -0.87350636,  0.07312961,  0.30978861,  0.07312961])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0:5,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "f9a1a726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07312961,  0.57873318,  0.64213459, -0.94599406,  1.20600643],\n",
       "       [-0.87350636, -0.59367906,  1.27919727, -0.64212006, -0.79608549],\n",
       "       [ 0.07312961,  0.37535555,  0.12686331, -0.3019626 ,  1.23413887],\n",
       "       [ 0.30978861, -0.59367906, -0.50551508, -0.06158465,  0.97625818],\n",
       "       [ 0.07312961,  1.49991423,  0.52268534, -0.17497048,  1.67253605]])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0:5,0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "6888e629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.07312961, -0.87350636,  0.07312961, ..., -0.87350636,\n",
       "        0.30978861, -0.87350636])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:,0]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "3cb0c73f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.07312961,  0.57873318,  0.64213459, -0.94599406,  1.20600643,\n",
       "        1.13455418,  0.8174205 ,  1.23808541, -0.9422651 , -0.76827512,\n",
       "        0.27171931, -0.87122827, -0.90818321,  1.29315561,  0.51496564,\n",
       "       -0.22803278, -0.30526877,  0.71337087,  0.61242316,  0.77378001,\n",
       "       -0.29902973,  0.67719561, -0.51902199,  0.52054891,  1.4805257 ,\n",
       "        1.07684662,  0.41085087,  0.59401909,  0.71642985,  0.76760835,\n",
       "       -0.46377934,  0.89345095, -0.81173788,  0.439482  ,  0.17256056,\n",
       "        0.75110957,  0.94944782, -0.99309724,  0.77661446, -0.23593991,\n",
       "        0.3215251 ,  0.36777403,  0.46627249])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "d3af6f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.46627249, 0.46627249, 0.46627249, ..., 0.46627249, 0.46627249,\n",
       "       0.46627249])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:,42]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "edb9ff4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.46627249,  0.46627249,  0.46627249, -2.14156407,  0.46627249])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0:5,42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "6dcacb7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07312961,  0.57873318,  0.64213459, ...,  0.3215251 ,\n",
       "         0.36777403,  0.46627249],\n",
       "       [-0.87350636, -0.59367906,  1.27919727, ...,  0.3215251 ,\n",
       "        -0.04755954,  0.46627249],\n",
       "       [ 0.07312961,  0.37535555,  0.12686331, ...,  0.3215251 ,\n",
       "         0.65095601,  0.46627249],\n",
       "       ...,\n",
       "       [-0.87350636,  0.11814266,  0.83887454, ...,  0.3215251 ,\n",
       "         0.14122844,  0.46627249],\n",
       "       [ 0.30978861, -0.59367906, -0.36732869, ..., -1.01691658,\n",
       "        -1.02925707,  0.46627249],\n",
       "       [-0.87350636, -0.59367906, -0.89665283, ..., -1.01691658,\n",
       "        -1.08589347,  0.46627249]])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "82ca3c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.87350636, -0.59367906, -0.89665283, -1.28615152, -0.79608549,\n",
       "        1.13455418, -1.02595927, -0.7539801 , -0.9422651 ,  2.33229862,\n",
       "       -0.73833325, -0.43571551,  1.29100233, -0.41321925,  0.51496564,\n",
       "       -0.07663563, -0.01911519,  0.71337087,  0.61242316,  1.9473826 ,\n",
       "        0.86571231, -0.79211366,  0.37861025, -1.82356674, -0.73583701,\n",
       "       -0.68347799,  0.33508438, -0.54624275, -1.49650814,  0.76760835,\n",
       "        0.08218366, -0.14672934, -0.20008452, -0.85935565, -1.06297307,\n",
       "        0.75110957, -0.93279853, -0.99309724,  0.77661446,  0.9511707 ,\n",
       "       -1.01691658, -1.08589347,  0.46627249])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "7298e105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.87350636, -0.59367906, -0.35561798, -0.90744288, -0.79608549,\n",
       "        1.13455418, -1.02595927, -0.7539801 , -0.9422651 ,  0.46009426,\n",
       "       -0.73833325,  0.24174878,  0.13143177,  1.29315561,  0.51496564,\n",
       "        0.02681699, -0.1358803 , -1.10834183, -1.87409998,  0.77378001,\n",
       "        0.86571231, -0.79211366,  0.37861025, -1.82356674, -0.73583701,\n",
       "       -0.68347799,  0.33508438,  0.59401909, -1.21428601, -1.68712781,\n",
       "       -0.26211733, -1.18690963, -0.58581186, -1.14707286,  0.17256056,\n",
       "       -0.76380438, -0.93279853, -0.99309724, -0.76295621,  0.9511707 ,\n",
       "       -1.01691658, -0.76495389,  0.46627249])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "8fbb54c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1432, 43)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "6735143d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.87350636, -0.59367906, -0.89665283, -1.28615152, -0.79608549,\n",
       "        1.13455418, -1.02595927, -0.7539801 , -0.9422651 ,  2.33229862,\n",
       "       -0.73833325, -0.43571551,  1.29100233, -0.41321925,  0.51496564,\n",
       "       -0.07663563, -0.01911519,  0.71337087,  0.61242316,  1.9473826 ,\n",
       "        0.86571231, -0.79211366,  0.37861025, -1.82356674, -0.73583701,\n",
       "       -0.68347799,  0.33508438, -0.54624275, -1.49650814,  0.76760835,\n",
       "        0.08218366, -0.14672934, -0.20008452, -0.85935565, -1.06297307,\n",
       "        0.75110957, -0.93279853, -0.99309724,  0.77661446,  0.9511707 ,\n",
       "       -1.01691658, -1.08589347,  0.46627249])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1431]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "b3ada6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SaleCondition_</th>\n",
       "      <th>MSSubClass(scaled)</th>\n",
       "      <th>MSZoning_(scaled)</th>\n",
       "      <th>LotFrontage(scaled)</th>\n",
       "      <th>LotArea(scaled)</th>\n",
       "      <th>LotShape_(scaled)</th>\n",
       "      <th>LotConfig_(scaled)</th>\n",
       "      <th>Neighborhood_(scaled)</th>\n",
       "      <th>HouseStyle_(scaled)</th>\n",
       "      <th>OverallQual(scaled)</th>\n",
       "      <th>...</th>\n",
       "      <th>House_Age(scaled)</th>\n",
       "      <th>Garage_Age(scaled)</th>\n",
       "      <th>GarageFinish_(scaled)</th>\n",
       "      <th>GarageCars(scaled)</th>\n",
       "      <th>GarageArea(scaled)</th>\n",
       "      <th>WoodDeckSF(scaled)</th>\n",
       "      <th>OpenPorchSF(scaled)</th>\n",
       "      <th>Exterior_Avg(scaled)</th>\n",
       "      <th>SaleCondition_(scaled)</th>\n",
       "      <th>SalePrice(log)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1184</td>\n",
       "      <td>0.073130</td>\n",
       "      <td>0.514966</td>\n",
       "      <td>-0.228033</td>\n",
       "      <td>-0.305269</td>\n",
       "      <td>0.713371</td>\n",
       "      <td>0.612423</td>\n",
       "      <td>0.773780</td>\n",
       "      <td>-0.299030</td>\n",
       "      <td>0.677196</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.871228</td>\n",
       "      <td>-0.908183</td>\n",
       "      <td>-0.235940</td>\n",
       "      <td>0.321525</td>\n",
       "      <td>0.367774</td>\n",
       "      <td>-0.768275</td>\n",
       "      <td>0.271719</td>\n",
       "      <td>1.293156</td>\n",
       "      <td>0.466272</td>\n",
       "      <td>12.247699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1184</td>\n",
       "      <td>-0.873506</td>\n",
       "      <td>0.514966</td>\n",
       "      <td>0.528953</td>\n",
       "      <td>-0.045540</td>\n",
       "      <td>0.713371</td>\n",
       "      <td>-1.996090</td>\n",
       "      <td>-1.401297</td>\n",
       "      <td>0.865712</td>\n",
       "      <td>-0.057459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.386920</td>\n",
       "      <td>0.131432</td>\n",
       "      <td>-0.235940</td>\n",
       "      <td>0.321525</td>\n",
       "      <td>-0.047560</td>\n",
       "      <td>1.756236</td>\n",
       "      <td>-0.738333</td>\n",
       "      <td>-0.413219</td>\n",
       "      <td>0.466272</td>\n",
       "      <td>12.109016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1184</td>\n",
       "      <td>0.073130</td>\n",
       "      <td>0.514966</td>\n",
       "      <td>-0.076636</td>\n",
       "      <td>0.327115</td>\n",
       "      <td>-1.108342</td>\n",
       "      <td>0.612423</td>\n",
       "      <td>0.773780</td>\n",
       "      <td>-0.299030</td>\n",
       "      <td>0.677196</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.822838</td>\n",
       "      <td>-0.828213</td>\n",
       "      <td>-0.235940</td>\n",
       "      <td>0.321525</td>\n",
       "      <td>0.650956</td>\n",
       "      <td>-0.768275</td>\n",
       "      <td>-0.042887</td>\n",
       "      <td>1.293156</td>\n",
       "      <td>0.466272</td>\n",
       "      <td>12.317171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>98</td>\n",
       "      <td>0.309789</td>\n",
       "      <td>0.514966</td>\n",
       "      <td>-0.480361</td>\n",
       "      <td>-0.056832</td>\n",
       "      <td>-1.108342</td>\n",
       "      <td>-1.435454</td>\n",
       "      <td>-0.775375</td>\n",
       "      <td>-0.299030</td>\n",
       "      <td>0.677196</td>\n",
       "      <td>...</td>\n",
       "      <td>0.628871</td>\n",
       "      <td>-0.788228</td>\n",
       "      <td>0.951171</td>\n",
       "      <td>1.659967</td>\n",
       "      <td>0.811426</td>\n",
       "      <td>-0.768275</td>\n",
       "      <td>-0.158795</td>\n",
       "      <td>-0.967426</td>\n",
       "      <td>-2.141564</td>\n",
       "      <td>11.849405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1184</td>\n",
       "      <td>0.073130</td>\n",
       "      <td>0.514966</td>\n",
       "      <td>0.730816</td>\n",
       "      <td>1.006927</td>\n",
       "      <td>-1.108342</td>\n",
       "      <td>-1.996090</td>\n",
       "      <td>-0.931856</td>\n",
       "      <td>-0.299030</td>\n",
       "      <td>1.411850</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.726057</td>\n",
       "      <td>-0.788228</td>\n",
       "      <td>-0.235940</td>\n",
       "      <td>1.659967</td>\n",
       "      <td>1.727048</td>\n",
       "      <td>0.858255</td>\n",
       "      <td>0.652559</td>\n",
       "      <td>1.293156</td>\n",
       "      <td>0.466272</td>\n",
       "      <td>12.429220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>1184</td>\n",
       "      <td>-0.873506</td>\n",
       "      <td>-2.187358</td>\n",
       "      <td>-0.379430</td>\n",
       "      <td>-0.519827</td>\n",
       "      <td>0.713371</td>\n",
       "      <td>0.612423</td>\n",
       "      <td>-0.227694</td>\n",
       "      <td>0.865712</td>\n",
       "      <td>0.677196</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.919619</td>\n",
       "      <td>-0.908183</td>\n",
       "      <td>-0.235940</td>\n",
       "      <td>0.321525</td>\n",
       "      <td>-0.330742</td>\n",
       "      <td>-0.768275</td>\n",
       "      <td>1.132748</td>\n",
       "      <td>1.293156</td>\n",
       "      <td>0.466272</td>\n",
       "      <td>12.128117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428</th>\n",
       "      <td>1184</td>\n",
       "      <td>0.073130</td>\n",
       "      <td>0.514966</td>\n",
       "      <td>-0.379430</td>\n",
       "      <td>-0.425648</td>\n",
       "      <td>0.713371</td>\n",
       "      <td>0.612423</td>\n",
       "      <td>-0.337230</td>\n",
       "      <td>-0.299030</td>\n",
       "      <td>-0.057459</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.774448</td>\n",
       "      <td>-0.788228</td>\n",
       "      <td>-0.235940</td>\n",
       "      <td>0.321525</td>\n",
       "      <td>-0.047560</td>\n",
       "      <td>-0.768275</td>\n",
       "      <td>-0.076004</td>\n",
       "      <td>1.293156</td>\n",
       "      <td>0.466272</td>\n",
       "      <td>12.072547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>1184</td>\n",
       "      <td>-0.873506</td>\n",
       "      <td>0.514966</td>\n",
       "      <td>0.781282</td>\n",
       "      <td>0.761879</td>\n",
       "      <td>0.713371</td>\n",
       "      <td>0.612423</td>\n",
       "      <td>-0.431119</td>\n",
       "      <td>0.865712</td>\n",
       "      <td>-0.057459</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.048593</td>\n",
       "      <td>0.171417</td>\n",
       "      <td>0.951171</td>\n",
       "      <td>0.321525</td>\n",
       "      <td>0.141228</td>\n",
       "      <td>2.188283</td>\n",
       "      <td>-0.738333</td>\n",
       "      <td>-0.949925</td>\n",
       "      <td>0.466272</td>\n",
       "      <td>12.254868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>1184</td>\n",
       "      <td>0.309789</td>\n",
       "      <td>0.514966</td>\n",
       "      <td>-0.177567</td>\n",
       "      <td>-0.171565</td>\n",
       "      <td>0.713371</td>\n",
       "      <td>0.612423</td>\n",
       "      <td>-0.775375</td>\n",
       "      <td>-0.299030</td>\n",
       "      <td>0.677196</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.919619</td>\n",
       "      <td>1.650869</td>\n",
       "      <td>-0.235940</td>\n",
       "      <td>-1.016917</td>\n",
       "      <td>-1.029257</td>\n",
       "      <td>-0.768275</td>\n",
       "      <td>0.255161</td>\n",
       "      <td>-1.326203</td>\n",
       "      <td>0.466272</td>\n",
       "      <td>12.493133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>1184</td>\n",
       "      <td>-0.873506</td>\n",
       "      <td>0.514966</td>\n",
       "      <td>-0.076636</td>\n",
       "      <td>-0.019115</td>\n",
       "      <td>0.713371</td>\n",
       "      <td>0.612423</td>\n",
       "      <td>1.947383</td>\n",
       "      <td>0.865712</td>\n",
       "      <td>-0.792114</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.435716</td>\n",
       "      <td>1.291002</td>\n",
       "      <td>0.951171</td>\n",
       "      <td>-1.016917</td>\n",
       "      <td>-1.085893</td>\n",
       "      <td>2.332299</td>\n",
       "      <td>-0.738333</td>\n",
       "      <td>-0.413219</td>\n",
       "      <td>0.466272</td>\n",
       "      <td>11.864469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1432 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SaleCondition_  MSSubClass(scaled)  MSZoning_(scaled)  \\\n",
       "0               1184            0.073130           0.514966   \n",
       "1               1184           -0.873506           0.514966   \n",
       "2               1184            0.073130           0.514966   \n",
       "3                 98            0.309789           0.514966   \n",
       "4               1184            0.073130           0.514966   \n",
       "...              ...                 ...                ...   \n",
       "1427            1184           -0.873506          -2.187358   \n",
       "1428            1184            0.073130           0.514966   \n",
       "1429            1184           -0.873506           0.514966   \n",
       "1430            1184            0.309789           0.514966   \n",
       "1431            1184           -0.873506           0.514966   \n",
       "\n",
       "      LotFrontage(scaled)  LotArea(scaled)  LotShape_(scaled)  \\\n",
       "0               -0.228033        -0.305269           0.713371   \n",
       "1                0.528953        -0.045540           0.713371   \n",
       "2               -0.076636         0.327115          -1.108342   \n",
       "3               -0.480361        -0.056832          -1.108342   \n",
       "4                0.730816         1.006927          -1.108342   \n",
       "...                   ...              ...                ...   \n",
       "1427            -0.379430        -0.519827           0.713371   \n",
       "1428            -0.379430        -0.425648           0.713371   \n",
       "1429             0.781282         0.761879           0.713371   \n",
       "1430            -0.177567        -0.171565           0.713371   \n",
       "1431            -0.076636        -0.019115           0.713371   \n",
       "\n",
       "      LotConfig_(scaled)  Neighborhood_(scaled)  HouseStyle_(scaled)  \\\n",
       "0               0.612423               0.773780            -0.299030   \n",
       "1              -1.996090              -1.401297             0.865712   \n",
       "2               0.612423               0.773780            -0.299030   \n",
       "3              -1.435454              -0.775375            -0.299030   \n",
       "4              -1.996090              -0.931856            -0.299030   \n",
       "...                  ...                    ...                  ...   \n",
       "1427            0.612423              -0.227694             0.865712   \n",
       "1428            0.612423              -0.337230            -0.299030   \n",
       "1429            0.612423              -0.431119             0.865712   \n",
       "1430            0.612423              -0.775375            -0.299030   \n",
       "1431            0.612423               1.947383             0.865712   \n",
       "\n",
       "      OverallQual(scaled)  ...  House_Age(scaled)  Garage_Age(scaled)  \\\n",
       "0                0.677196  ...          -0.871228           -0.908183   \n",
       "1               -0.057459  ...           0.386920            0.131432   \n",
       "2                0.677196  ...          -0.822838           -0.828213   \n",
       "3                0.677196  ...           0.628871           -0.788228   \n",
       "4                1.411850  ...          -0.726057           -0.788228   \n",
       "...                   ...  ...                ...                 ...   \n",
       "1427             0.677196  ...          -0.919619           -0.908183   \n",
       "1428            -0.057459  ...          -0.774448           -0.788228   \n",
       "1429            -0.057459  ...          -0.048593            0.171417   \n",
       "1430             0.677196  ...          -0.919619            1.650869   \n",
       "1431            -0.792114  ...          -0.435716            1.291002   \n",
       "\n",
       "      GarageFinish_(scaled)  GarageCars(scaled)  GarageArea(scaled)  \\\n",
       "0                 -0.235940            0.321525            0.367774   \n",
       "1                 -0.235940            0.321525           -0.047560   \n",
       "2                 -0.235940            0.321525            0.650956   \n",
       "3                  0.951171            1.659967            0.811426   \n",
       "4                 -0.235940            1.659967            1.727048   \n",
       "...                     ...                 ...                 ...   \n",
       "1427              -0.235940            0.321525           -0.330742   \n",
       "1428              -0.235940            0.321525           -0.047560   \n",
       "1429               0.951171            0.321525            0.141228   \n",
       "1430              -0.235940           -1.016917           -1.029257   \n",
       "1431               0.951171           -1.016917           -1.085893   \n",
       "\n",
       "      WoodDeckSF(scaled)  OpenPorchSF(scaled)  Exterior_Avg(scaled)  \\\n",
       "0              -0.768275             0.271719              1.293156   \n",
       "1               1.756236            -0.738333             -0.413219   \n",
       "2              -0.768275            -0.042887              1.293156   \n",
       "3              -0.768275            -0.158795             -0.967426   \n",
       "4               0.858255             0.652559              1.293156   \n",
       "...                  ...                  ...                   ...   \n",
       "1427           -0.768275             1.132748              1.293156   \n",
       "1428           -0.768275            -0.076004              1.293156   \n",
       "1429            2.188283            -0.738333             -0.949925   \n",
       "1430           -0.768275             0.255161             -1.326203   \n",
       "1431            2.332299            -0.738333             -0.413219   \n",
       "\n",
       "      SaleCondition_(scaled)  SalePrice(log)  \n",
       "0                   0.466272       12.247699  \n",
       "1                   0.466272       12.109016  \n",
       "2                   0.466272       12.317171  \n",
       "3                  -2.141564       11.849405  \n",
       "4                   0.466272       12.429220  \n",
       "...                      ...             ...  \n",
       "1427                0.466272       12.128117  \n",
       "1428                0.466272       12.072547  \n",
       "1429                0.466272       12.254868  \n",
       "1430                0.466272       12.493133  \n",
       "1431                0.466272       11.864469  \n",
       "\n",
       "[1432 rows x 45 columns]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_house4[df_house4.columns[105:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "da020120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.46627249, 0.46627249, 0.46627249, ..., 0.46627249, 0.46627249,\n",
       "       0.46627249])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "bcf76ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07312961,  0.51496564, -0.22803278, ...,  0.27171931,\n",
       "         1.29315561,  0.46627249],\n",
       "       [-0.87350636,  0.51496564,  0.52895299, ..., -0.73833325,\n",
       "        -0.41321925,  0.46627249],\n",
       "       [ 0.07312961,  0.51496564, -0.07663563, ..., -0.04288723,\n",
       "         1.29315561,  0.46627249],\n",
       "       ...,\n",
       "       [-0.87350636,  0.51496564,  0.78128158, ..., -0.73833325,\n",
       "        -0.94992519,  0.46627249],\n",
       "       [ 0.30978861,  0.51496564, -0.17756706, ...,  0.25516107,\n",
       "        -1.32620272,  0.46627249],\n",
       "       [-0.87350636,  0.51496564, -0.07663563, ..., -0.73833325,\n",
       "        -0.41321925,  0.46627249]])"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "e3d2da71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.07312961,  0.57873318,  0.64213459, -0.94599406,  1.20600643,\n",
       "        1.13455418,  0.8174205 ,  1.23808541, -0.9422651 , -0.76827512,\n",
       "        0.27171931, -0.87122827, -0.90818321,  1.29315561,  0.51496564,\n",
       "       -0.22803278, -0.30526877,  0.71337087,  0.61242316,  0.77378001,\n",
       "       -0.29902973,  0.67719561, -0.51902199,  0.52054891,  1.4805257 ,\n",
       "        1.07684662,  0.41085087,  0.59401909,  0.71642985,  0.76760835,\n",
       "       -0.46377934,  0.89345095, -0.81173788,  0.439482  ,  0.17256056,\n",
       "        0.75110957,  0.94944782, -0.99309724,  0.77661446, -0.23593991,\n",
       "        0.3215251 ,  0.36777403,  0.46627249])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "42c5081d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.46627249, 0.46627249, 0.46627249, ..., 0.46627249, 0.46627249,\n",
       "       0.46627249])"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:,42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "eb214536",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[:,0:42]\n",
    "y = dataset[:,42]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "8d61a1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07312961,  0.51496564, -0.22803278, ..., -0.76827512,\n",
       "         0.27171931,  1.29315561],\n",
       "       [-0.87350636,  0.51496564,  0.52895299, ...,  1.75623574,\n",
       "        -0.73833325, -0.41321925],\n",
       "       [ 0.07312961,  0.51496564, -0.07663563, ..., -0.76827512,\n",
       "        -0.04288723,  1.29315561],\n",
       "       ...,\n",
       "       [-0.87350636,  0.51496564,  0.78128158, ...,  2.1882829 ,\n",
       "        -0.73833325, -0.94992519],\n",
       "       [ 0.30978861,  0.51496564, -0.17756706, ..., -0.76827512,\n",
       "         0.25516107, -1.32620272],\n",
       "       [-0.87350636,  0.51496564, -0.07663563, ...,  2.33229862,\n",
       "        -0.73833325, -0.41321925]])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "9d8f49cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.46627249, 0.46627249, 0.46627249, ..., 0.46627249, 0.46627249,\n",
       "       0.46627249])"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ae27e1e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MSSubClass', 'MSZoning_', 'LotFrontage', 'LotArea', 'LotShape_',\n",
       "       'LotConfig_', 'Neighborhood_', 'HouseStyle_', 'OverallQual',\n",
       "       'OverallCond', 'RoofStyle_', 'MasVnrType_', 'MasVnrArea', 'ExterQual_',\n",
       "       'Foundation_', 'BsmtQual_', 'BsmtExposure_', 'BsmtFinType1_',\n",
       "       'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF', 'HeatingQC_', '1stFlrSF',\n",
       "       '2ndFlrSF', 'GrLivArea', 'BsmtFullBath', 'FullBath', 'HalfBath',\n",
       "       'BedroomAbvGr', 'KitchenQual_', 'TotRmsAbvGrd', 'Fireplaces',\n",
       "       'FireplaceQu_', 'GarageType_', 'House_Age', 'Garage_Age',\n",
       "       'GarageFinish_', 'GarageCars', 'GarageArea', 'WoodDeckSF',\n",
       "       'OpenPorchSF', 'Exterior_Avg', 'SaleCondition_', 'MSSubClass(scaled)',\n",
       "       'MSZoning_(scaled)', 'LotFrontage(scaled)', 'LotArea(scaled)',\n",
       "       'LotShape_(scaled)', 'LotConfig_(scaled)', 'Neighborhood_(scaled)',\n",
       "       'HouseStyle_(scaled)', 'OverallQual(scaled)', 'OverallCond(scaled)',\n",
       "       'RoofStyle_(scaled)', 'MasVnrType_(scaled)', 'MasVnrArea(scaled)',\n",
       "       'ExterQual_(scaled)', 'Foundation_(scaled)', 'BsmtQual_(scaled)',\n",
       "       'BsmtExposure_(scaled)', 'BsmtFinType1_(scaled)', 'BsmtFinSF1(scaled)',\n",
       "       'BsmtUnfSF(scaled)', 'TotalBsmtSF(scaled)', 'HeatingQC_(scaled)',\n",
       "       '1stFlrSF(scaled)', '2ndFlrSF(scaled)', 'GrLivArea(scaled)',\n",
       "       'BsmtFullBath(scaled)', 'FullBath(scaled)', 'HalfBath(scaled)',\n",
       "       'BedroomAbvGr(scaled)', 'KitchenQual_(scaled)', 'TotRmsAbvGrd(scaled)',\n",
       "       'Fireplaces(scaled)', 'FireplaceQu_(scaled)', 'GarageType_(scaled)',\n",
       "       'House_Age(scaled)', 'Garage_Age(scaled)', 'GarageFinish_(scaled)',\n",
       "       'GarageCars(scaled)', 'GarageArea(scaled)', 'WoodDeckSF(scaled)',\n",
       "       'OpenPorchSF(scaled)', 'Exterior_Avg(scaled)',\n",
       "       'SaleCondition_(scaled)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_house_test_extracted_.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7bbf9764",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_house_test_extracted2b_predicted = df_house_test_extracted_[[\n",
    "    \n",
    "        'MSSubClass(scaled)',\n",
    "        'MasVnrArea(scaled)', 'BsmtFinSF1(scaled)',\n",
    "        'BsmtUnfSF(scaled)', '2ndFlrSF(scaled)', 'BsmtFullBath(scaled)', 'FullBath(scaled)', \n",
    "        'HalfBath(scaled)','Fireplaces(scaled)', 'WoodDeckSF(scaled)','OpenPorchSF(scaled)', \n",
    "   \n",
    "        'House_Age(scaled)','Garage_Age(scaled)','Exterior_Avg(scaled)',\n",
    "\n",
    "       'MSZoning_(scaled)', 'LotFrontage(scaled)', 'LotArea(scaled)',\n",
    "       'LotShape_(scaled)', 'LotConfig_(scaled)', 'Neighborhood_(scaled)',\n",
    "       'HouseStyle_(scaled)', 'OverallQual(scaled)', 'OverallCond(scaled)',\n",
    "       'RoofStyle_(scaled)', 'MasVnrType_(scaled)', \n",
    "       'ExterQual_(scaled)', 'Foundation_(scaled)', 'BsmtQual_(scaled)',\n",
    "       'BsmtExposure_(scaled)', 'BsmtFinType1_(scaled)',\n",
    "       'TotalBsmtSF(scaled)', 'HeatingQC_(scaled)',\n",
    "       '1stFlrSF(scaled)', 'GrLivArea(scaled)',\n",
    "       \n",
    "       'BedroomAbvGr(scaled)', 'KitchenQual_(scaled)', 'TotRmsAbvGrd(scaled)',\n",
    "       'FireplaceQu_(scaled)', 'GarageType_(scaled)',\n",
    "       'GarageFinish_(scaled)',\n",
    "       'GarageCars(scaled)', 'GarageArea(scaled)', \n",
    "       'SaleCondition_(scaled)' \n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bfd8ec9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_house_test_extracted2b_predicted.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "729b05f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7987bece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass(scaled)</th>\n",
       "      <th>MasVnrArea(scaled)</th>\n",
       "      <th>BsmtFinSF1(scaled)</th>\n",
       "      <th>BsmtUnfSF(scaled)</th>\n",
       "      <th>2ndFlrSF(scaled)</th>\n",
       "      <th>BsmtFullBath(scaled)</th>\n",
       "      <th>FullBath(scaled)</th>\n",
       "      <th>HalfBath(scaled)</th>\n",
       "      <th>Fireplaces(scaled)</th>\n",
       "      <th>WoodDeckSF(scaled)</th>\n",
       "      <th>...</th>\n",
       "      <th>GrLivArea(scaled)</th>\n",
       "      <th>BedroomAbvGr(scaled)</th>\n",
       "      <th>KitchenQual_(scaled)</th>\n",
       "      <th>TotRmsAbvGrd(scaled)</th>\n",
       "      <th>FireplaceQu_(scaled)</th>\n",
       "      <th>GarageType_(scaled)</th>\n",
       "      <th>GarageFinish_(scaled)</th>\n",
       "      <th>GarageCars(scaled)</th>\n",
       "      <th>GarageArea(scaled)</th>\n",
       "      <th>SaleCondition_(scaled)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>-0.871732</td>\n",
       "      <td>3.262228</td>\n",
       "      <td>-1.014656</td>\n",
       "      <td>2.531890</td>\n",
       "      <td>-0.795163</td>\n",
       "      <td>-0.820325</td>\n",
       "      <td>0.809818</td>\n",
       "      <td>1.238364</td>\n",
       "      <td>0.623326</td>\n",
       "      <td>4.074320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423309</td>\n",
       "      <td>-1.063416</td>\n",
       "      <td>-0.766630</td>\n",
       "      <td>-0.308545</td>\n",
       "      <td>0.660196</td>\n",
       "      <td>0.776405</td>\n",
       "      <td>-0.236464</td>\n",
       "      <td>0.319312</td>\n",
       "      <td>0.190967</td>\n",
       "      <td>0.466272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>-0.871732</td>\n",
       "      <td>-0.589223</td>\n",
       "      <td>-0.533066</td>\n",
       "      <td>0.784980</td>\n",
       "      <td>-0.795163</td>\n",
       "      <td>-0.820325</td>\n",
       "      <td>-1.026620</td>\n",
       "      <td>-0.754088</td>\n",
       "      <td>-0.945886</td>\n",
       "      <td>-0.769255</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.774522</td>\n",
       "      <td>0.171519</td>\n",
       "      <td>-0.766630</td>\n",
       "      <td>-0.308545</td>\n",
       "      <td>-0.997971</td>\n",
       "      <td>-1.997578</td>\n",
       "      <td>-2.977522</td>\n",
       "      <td>-2.361793</td>\n",
       "      <td>-2.224315</td>\n",
       "      <td>-2.083932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>-0.280449</td>\n",
       "      <td>-0.589223</td>\n",
       "      <td>-1.014656</td>\n",
       "      <td>0.963744</td>\n",
       "      <td>-0.795163</td>\n",
       "      <td>-0.820325</td>\n",
       "      <td>-1.026620</td>\n",
       "      <td>-0.754088</td>\n",
       "      <td>0.623326</td>\n",
       "      <td>-0.769255</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.921445</td>\n",
       "      <td>-1.063416</td>\n",
       "      <td>-0.766630</td>\n",
       "      <td>-0.932567</td>\n",
       "      <td>1.212918</td>\n",
       "      <td>-0.763634</td>\n",
       "      <td>0.952194</td>\n",
       "      <td>-1.021240</td>\n",
       "      <td>-0.900873</td>\n",
       "      <td>0.466272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>-0.871732</td>\n",
       "      <td>1.103528</td>\n",
       "      <td>1.117432</td>\n",
       "      <td>1.058783</td>\n",
       "      <td>-0.795163</td>\n",
       "      <td>1.128963</td>\n",
       "      <td>0.809818</td>\n",
       "      <td>-0.754088</td>\n",
       "      <td>0.623326</td>\n",
       "      <td>1.770668</td>\n",
       "      <td>...</td>\n",
       "      <td>1.443608</td>\n",
       "      <td>0.171519</td>\n",
       "      <td>-0.766630</td>\n",
       "      <td>0.315478</td>\n",
       "      <td>1.212918</td>\n",
       "      <td>0.776405</td>\n",
       "      <td>-0.236464</td>\n",
       "      <td>0.319312</td>\n",
       "      <td>0.498194</td>\n",
       "      <td>0.466272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>-0.635219</td>\n",
       "      <td>-0.589223</td>\n",
       "      <td>-1.014656</td>\n",
       "      <td>-0.685864</td>\n",
       "      <td>-0.795163</td>\n",
       "      <td>-0.820325</td>\n",
       "      <td>-1.026620</td>\n",
       "      <td>-0.754088</td>\n",
       "      <td>0.623326</td>\n",
       "      <td>-0.769255</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.427513</td>\n",
       "      <td>-2.298351</td>\n",
       "      <td>-2.280961</td>\n",
       "      <td>-1.556590</td>\n",
       "      <td>-0.445248</td>\n",
       "      <td>-1.997578</td>\n",
       "      <td>-2.977522</td>\n",
       "      <td>-2.361793</td>\n",
       "      <td>-2.224315</td>\n",
       "      <td>0.466272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>0.074321</td>\n",
       "      <td>0.496025</td>\n",
       "      <td>0.832219</td>\n",
       "      <td>-0.982296</td>\n",
       "      <td>2.461253</td>\n",
       "      <td>1.128963</td>\n",
       "      <td>2.646255</td>\n",
       "      <td>1.238364</td>\n",
       "      <td>2.192538</td>\n",
       "      <td>-0.769255</td>\n",
       "      <td>...</td>\n",
       "      <td>2.621033</td>\n",
       "      <td>2.641389</td>\n",
       "      <td>0.747701</td>\n",
       "      <td>3.435592</td>\n",
       "      <td>0.660196</td>\n",
       "      <td>0.776405</td>\n",
       "      <td>-0.236464</td>\n",
       "      <td>0.319312</td>\n",
       "      <td>0.441476</td>\n",
       "      <td>0.466272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>0.429091</td>\n",
       "      <td>-0.589223</td>\n",
       "      <td>-0.130962</td>\n",
       "      <td>-0.509363</td>\n",
       "      <td>0.889190</td>\n",
       "      <td>-0.820325</td>\n",
       "      <td>0.809818</td>\n",
       "      <td>1.238364</td>\n",
       "      <td>0.623326</td>\n",
       "      <td>1.441575</td>\n",
       "      <td>...</td>\n",
       "      <td>1.412999</td>\n",
       "      <td>1.406454</td>\n",
       "      <td>0.747701</td>\n",
       "      <td>0.939501</td>\n",
       "      <td>1.212918</td>\n",
       "      <td>-0.763634</td>\n",
       "      <td>0.952194</td>\n",
       "      <td>-1.021240</td>\n",
       "      <td>-1.089936</td>\n",
       "      <td>0.466272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>-0.871732</td>\n",
       "      <td>0.035974</td>\n",
       "      <td>-1.014656</td>\n",
       "      <td>1.934501</td>\n",
       "      <td>-0.795163</td>\n",
       "      <td>-0.820325</td>\n",
       "      <td>0.809818</td>\n",
       "      <td>-0.754088</td>\n",
       "      <td>-0.945886</td>\n",
       "      <td>0.850895</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158261</td>\n",
       "      <td>0.171519</td>\n",
       "      <td>0.747701</td>\n",
       "      <td>0.315478</td>\n",
       "      <td>-0.997971</td>\n",
       "      <td>0.776405</td>\n",
       "      <td>-0.236464</td>\n",
       "      <td>0.319312</td>\n",
       "      <td>0.734523</td>\n",
       "      <td>0.466272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>-0.871732</td>\n",
       "      <td>-0.589223</td>\n",
       "      <td>-0.755159</td>\n",
       "      <td>-0.102052</td>\n",
       "      <td>-0.795163</td>\n",
       "      <td>-0.820325</td>\n",
       "      <td>-1.026620</td>\n",
       "      <td>-0.754088</td>\n",
       "      <td>-0.945886</td>\n",
       "      <td>-0.667996</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.198966</td>\n",
       "      <td>-1.063416</td>\n",
       "      <td>-0.766630</td>\n",
       "      <td>-0.932567</td>\n",
       "      <td>-0.997971</td>\n",
       "      <td>0.776405</td>\n",
       "      <td>-0.691142</td>\n",
       "      <td>-1.021240</td>\n",
       "      <td>-0.820521</td>\n",
       "      <td>0.466272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>-0.871732</td>\n",
       "      <td>-0.199949</td>\n",
       "      <td>2.368164</td>\n",
       "      <td>-0.077161</td>\n",
       "      <td>-0.795163</td>\n",
       "      <td>1.128963</td>\n",
       "      <td>0.809818</td>\n",
       "      <td>-0.754088</td>\n",
       "      <td>0.623326</td>\n",
       "      <td>0.867772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.980393</td>\n",
       "      <td>0.171519</td>\n",
       "      <td>2.262032</td>\n",
       "      <td>0.939501</td>\n",
       "      <td>1.212918</td>\n",
       "      <td>0.776405</td>\n",
       "      <td>-0.691142</td>\n",
       "      <td>1.659865</td>\n",
       "      <td>1.415151</td>\n",
       "      <td>-2.141564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>288 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSSubClass(scaled)  MasVnrArea(scaled)  BsmtFinSF1(scaled)  \\\n",
       "168            -0.871732            3.262228           -1.014656   \n",
       "605            -0.871732           -0.589223           -0.533066   \n",
       "548            -0.280449           -0.589223           -1.014656   \n",
       "65             -0.871732            1.103528            1.117432   \n",
       "628            -0.635219           -0.589223           -1.014656   \n",
       "...                  ...                 ...                 ...   \n",
       "1367            0.074321            0.496025            0.832219   \n",
       "265             0.429091           -0.589223           -0.130962   \n",
       "1425           -0.871732            0.035974           -1.014656   \n",
       "493            -0.871732           -0.589223           -0.755159   \n",
       "1340           -0.871732           -0.199949            2.368164   \n",
       "\n",
       "      BsmtUnfSF(scaled)  2ndFlrSF(scaled)  BsmtFullBath(scaled)  \\\n",
       "168            2.531890         -0.795163             -0.820325   \n",
       "605            0.784980         -0.795163             -0.820325   \n",
       "548            0.963744         -0.795163             -0.820325   \n",
       "65             1.058783         -0.795163              1.128963   \n",
       "628           -0.685864         -0.795163             -0.820325   \n",
       "...                 ...               ...                   ...   \n",
       "1367          -0.982296          2.461253              1.128963   \n",
       "265           -0.509363          0.889190             -0.820325   \n",
       "1425           1.934501         -0.795163             -0.820325   \n",
       "493           -0.102052         -0.795163             -0.820325   \n",
       "1340          -0.077161         -0.795163              1.128963   \n",
       "\n",
       "      FullBath(scaled)  HalfBath(scaled)  Fireplaces(scaled)  \\\n",
       "168           0.809818          1.238364            0.623326   \n",
       "605          -1.026620         -0.754088           -0.945886   \n",
       "548          -1.026620         -0.754088            0.623326   \n",
       "65            0.809818         -0.754088            0.623326   \n",
       "628          -1.026620         -0.754088            0.623326   \n",
       "...                ...               ...                 ...   \n",
       "1367          2.646255          1.238364            2.192538   \n",
       "265           0.809818          1.238364            0.623326   \n",
       "1425          0.809818         -0.754088           -0.945886   \n",
       "493          -1.026620         -0.754088           -0.945886   \n",
       "1340          0.809818         -0.754088            0.623326   \n",
       "\n",
       "      WoodDeckSF(scaled)  ...  GrLivArea(scaled)  BedroomAbvGr(scaled)  \\\n",
       "168             4.074320  ...           0.423309             -1.063416   \n",
       "605            -0.769255  ...          -0.774522              0.171519   \n",
       "548            -0.769255  ...          -0.921445             -1.063416   \n",
       "65              1.770668  ...           1.443608              0.171519   \n",
       "628            -0.769255  ...          -1.427513             -2.298351   \n",
       "...                  ...  ...                ...                   ...   \n",
       "1367           -0.769255  ...           2.621033              2.641389   \n",
       "265             1.441575  ...           1.412999              1.406454   \n",
       "1425            0.850895  ...          -0.158261              0.171519   \n",
       "493            -0.667996  ...          -1.198966             -1.063416   \n",
       "1340            0.867772  ...           0.980393              0.171519   \n",
       "\n",
       "      KitchenQual_(scaled)  TotRmsAbvGrd(scaled)  FireplaceQu_(scaled)  \\\n",
       "168              -0.766630             -0.308545              0.660196   \n",
       "605              -0.766630             -0.308545             -0.997971   \n",
       "548              -0.766630             -0.932567              1.212918   \n",
       "65               -0.766630              0.315478              1.212918   \n",
       "628              -2.280961             -1.556590             -0.445248   \n",
       "...                    ...                   ...                   ...   \n",
       "1367              0.747701              3.435592              0.660196   \n",
       "265               0.747701              0.939501              1.212918   \n",
       "1425              0.747701              0.315478             -0.997971   \n",
       "493              -0.766630             -0.932567             -0.997971   \n",
       "1340              2.262032              0.939501              1.212918   \n",
       "\n",
       "      GarageType_(scaled)  GarageFinish_(scaled)  GarageCars(scaled)  \\\n",
       "168              0.776405              -0.236464            0.319312   \n",
       "605             -1.997578              -2.977522           -2.361793   \n",
       "548             -0.763634               0.952194           -1.021240   \n",
       "65               0.776405              -0.236464            0.319312   \n",
       "628             -1.997578              -2.977522           -2.361793   \n",
       "...                   ...                    ...                 ...   \n",
       "1367             0.776405              -0.236464            0.319312   \n",
       "265             -0.763634               0.952194           -1.021240   \n",
       "1425             0.776405              -0.236464            0.319312   \n",
       "493              0.776405              -0.691142           -1.021240   \n",
       "1340             0.776405              -0.691142            1.659865   \n",
       "\n",
       "      GarageArea(scaled)  SaleCondition_(scaled)  \n",
       "168             0.190967                0.466272  \n",
       "605            -2.224315               -2.083932  \n",
       "548            -0.900873                0.466272  \n",
       "65              0.498194                0.466272  \n",
       "628            -2.224315                0.466272  \n",
       "...                  ...                     ...  \n",
       "1367            0.441476                0.466272  \n",
       "265            -1.089936                0.466272  \n",
       "1425            0.734523                0.466272  \n",
       "493            -0.820521                0.466272  \n",
       "1340            1.415151               -2.141564  \n",
       "\n",
       "[288 rows x 43 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "484908d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0df7e979",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ea03e78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30, 13, 6\n",
    "model.add(Dense(90, input_dim=43, activation='relu'))\n",
    "model.add(Dense(21, activation='relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e38990aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5bc446da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "116/116 [==============================] - 1s 1ms/step - loss: nan\n",
      "Epoch 2/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 3/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 4/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 5/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 6/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 7/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 8/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 9/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 10/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 11/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 12/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 13/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 14/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 15/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 16/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 17/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 18/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 19/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 20/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 21/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 22/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 23/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 24/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 25/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 26/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 27/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 28/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 29/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 30/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 31/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 32/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 33/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 34/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 35/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 36/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 37/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 38/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 39/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 40/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 41/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 42/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 43/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 44/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 45/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 46/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 47/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 48/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 49/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 50/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 51/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 52/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 53/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 54/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 55/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 56/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 57/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 58/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 59/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 60/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 61/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 62/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 63/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 64/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 65/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 66/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 67/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 68/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 69/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 70/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 71/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 72/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 73/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 74/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 75/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 76/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 77/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 78/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 79/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 80/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 81/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 82/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 83/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 84/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 85/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 86/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 87/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 88/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 89/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 90/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 91/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 92/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 93/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 94/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 95/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 96/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 97/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 98/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 99/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 100/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 101/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 102/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 103/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 104/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 105/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 106/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 107/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 108/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 109/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 110/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 111/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 112/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 113/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 114/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 115/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 116/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 117/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 118/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 119/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 120/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 121/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 122/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 123/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 124/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 125/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 126/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 127/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 128/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 129/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 130/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 131/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 132/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 133/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 134/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 135/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 136/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 137/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 138/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 139/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 140/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 141/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 142/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 143/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 144/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 145/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 146/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 147/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 148/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 149/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 150/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 151/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 152/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 153/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 154/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 155/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 156/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 157/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 158/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 159/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 160/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 161/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 162/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 163/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 164/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 165/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 166/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 167/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 168/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 169/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 170/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 171/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 172/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 173/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 174/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 175/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 176/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 177/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 178/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 179/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 180/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 181/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 182/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 183/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 184/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 185/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 186/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 187/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 188/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 189/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 190/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 191/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 192/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 193/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 194/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 195/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 196/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 197/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 198/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 199/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 200/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x247563a3dc0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train, epochs=200, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e94cca8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fd8a7f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 90)                3960      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 21)                1911      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,893\n",
      "Trainable params: 5,893\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0b19ffa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8b326a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "116/116 [==============================] - 1s 2ms/step - loss: nan\n",
      "Epoch 2/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 3/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 4/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 5/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 6/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 7/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 8/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 9/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 10/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 11/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 12/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 13/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 14/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 15/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 16/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 17/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 18/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 19/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 20/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 21/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 22/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 23/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 24/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 25/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 26/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 27/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 28/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 29/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 30/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 31/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 32/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 33/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 34/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 35/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 36/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 37/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 38/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 39/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 40/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 41/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 42/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 43/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 44/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 45/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 46/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 47/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 48/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 49/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 50/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 51/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 52/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 53/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 54/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 55/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 56/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 57/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 58/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 59/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 60/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 61/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 62/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 63/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 64/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 65/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 66/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 67/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 68/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 69/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 70/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 71/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 72/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 73/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 74/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 75/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 76/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 77/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 78/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 79/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 80/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 81/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 82/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 83/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 84/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 85/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 86/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 87/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 88/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 89/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 90/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 91/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 92/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 93/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 94/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 95/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 96/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 97/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 98/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 99/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 100/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 101/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 102/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 103/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 104/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 105/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 106/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 107/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 108/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 109/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 110/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 111/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 112/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 113/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 114/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 115/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 116/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 117/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 118/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 119/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 120/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 121/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 122/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 123/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 124/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 125/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 126/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 127/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 128/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 129/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 130/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 131/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 132/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 133/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 134/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 135/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 136/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 137/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 138/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 139/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 140/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 141/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 142/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 143/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 144/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 145/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 146/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 147/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 148/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 149/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 150/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 151/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 152/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 153/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 154/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 155/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 156/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 157/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 158/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 159/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 160/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 161/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 162/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 163/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 164/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 165/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 166/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 167/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 168/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 169/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 170/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 171/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 172/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 173/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 174/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 175/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 176/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 177/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 178/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 179/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 180/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 181/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 182/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 183/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 184/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 185/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 186/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 187/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 188/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 189/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 190/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 191/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 192/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 193/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 194/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 195/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 196/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 197/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 198/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 199/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 200/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "9/9 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.add(Dense(60, input_dim=43, activation='relu'))\n",
    "model.add(Dense(21, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error',optimizer='adam')\n",
    "model.fit(X_train,y_train, epochs=200, batch_size=10)\n",
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "575879e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 2/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 3/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 4/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 5/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 6/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 7/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 8/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 9/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 10/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 11/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 12/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 13/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 14/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 15/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 16/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 17/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 18/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 19/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 20/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 21/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 22/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 23/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 24/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 25/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 26/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 27/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 28/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 29/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 30/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 31/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 32/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 33/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 34/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 35/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 36/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 37/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 38/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 39/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 40/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 41/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 42/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 43/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 44/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 45/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 46/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 47/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 48/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 49/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 50/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 51/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 52/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 53/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 54/1000\n",
      "24/24 [==============================] - 0s 3ms/step - loss: nan\n",
      "Epoch 55/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 56/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 57/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 58/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 59/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 60/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 61/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 62/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 63/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 64/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 65/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 66/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 67/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 68/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 69/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 70/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 71/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 72/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 73/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 74/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 75/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 76/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 77/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 78/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 79/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 80/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 81/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 82/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 83/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 84/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 85/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 86/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 87/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 88/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 89/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 90/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 91/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 92/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 93/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 94/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 95/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 96/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 97/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 98/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 99/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 100/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 101/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 102/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 103/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 104/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 105/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 106/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 107/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 108/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 109/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 110/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 111/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 112/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 113/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 114/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 115/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 116/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 117/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 118/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 119/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 120/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 121/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 122/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 123/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 124/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 125/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 126/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 127/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 128/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 129/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 130/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 131/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 132/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 133/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 134/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 135/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 136/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 137/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 138/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 139/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 140/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 141/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 142/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 143/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 144/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 145/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 146/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 147/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 148/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 149/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 150/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 151/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 152/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 153/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 154/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 155/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 156/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 157/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 158/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 159/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 160/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 161/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 162/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 163/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 164/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 165/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 166/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 167/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 168/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 169/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 170/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 171/1000\n",
      "24/24 [==============================] - 0s 3ms/step - loss: nan\n",
      "Epoch 172/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 173/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 174/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 175/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 176/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 177/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 178/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 179/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 180/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 181/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 182/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 183/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 184/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 185/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 186/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 187/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 188/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 189/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 190/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 191/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 192/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 193/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 194/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 195/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 196/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 197/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 198/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 199/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 200/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 201/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 202/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 203/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 204/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 205/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 206/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 207/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 208/1000\n",
      "24/24 [==============================] - 0s 3ms/step - loss: nan\n",
      "Epoch 209/1000\n",
      "24/24 [==============================] - 0s 3ms/step - loss: nan\n",
      "Epoch 210/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 211/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 212/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 213/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 214/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 215/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 216/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 217/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 218/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 219/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 220/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 221/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 222/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 223/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 224/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 225/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 226/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 227/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 228/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 229/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 230/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 231/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 232/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 233/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 234/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 235/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 236/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 237/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 238/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 239/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 240/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 241/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 242/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 243/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 244/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 245/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 246/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 247/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 248/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 249/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 250/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 251/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 252/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 253/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 254/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 255/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 256/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 257/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 258/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 259/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 260/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 261/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 262/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 263/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 264/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 265/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 266/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 267/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 268/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 269/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 270/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 271/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 272/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 273/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 274/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 275/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 276/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 277/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 278/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 279/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 280/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 281/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 282/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 283/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 284/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 285/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 286/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 287/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 288/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 289/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 290/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 291/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 292/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 293/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 294/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 295/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 296/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 297/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 298/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 299/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 300/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 301/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 302/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 303/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 304/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 305/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 306/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 307/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 308/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 309/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 310/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 311/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 312/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 313/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 314/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 315/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 316/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 317/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 318/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 319/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 320/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 321/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 322/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 323/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 324/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 325/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 326/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 327/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 328/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 329/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 330/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 331/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 332/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 333/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 334/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 335/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 336/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 337/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 338/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 339/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 340/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 341/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 342/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 343/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 344/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 345/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 346/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 347/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 348/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 349/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 350/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 351/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 352/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 353/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 354/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 355/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 356/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 357/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 358/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 359/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 360/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 361/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 362/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 363/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 364/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 365/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 366/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 367/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 368/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 369/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 370/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 371/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 372/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 373/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 374/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 375/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 376/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 377/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 378/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 379/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 380/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 381/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 382/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 383/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 384/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 385/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 386/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 387/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 388/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 389/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 390/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 391/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 392/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 393/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 394/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 395/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 396/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 397/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 398/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 399/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 400/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 401/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 402/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 403/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 404/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 405/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 406/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 407/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 408/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 409/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 410/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 411/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 412/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 413/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 414/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 415/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 416/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 417/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 418/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 419/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 420/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 421/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 422/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 423/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 424/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 425/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 426/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 427/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 428/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 429/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 430/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 431/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 432/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 433/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 434/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 435/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 436/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 437/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 438/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 439/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 440/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 441/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 442/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 443/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 444/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 445/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 446/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 447/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 448/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 449/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 450/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 451/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 452/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 453/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 454/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 455/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 456/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 457/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 458/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 459/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 460/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 461/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 462/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 463/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 464/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 465/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 466/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 467/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 468/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 469/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 470/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 471/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 472/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 473/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 474/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 475/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 476/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 477/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 478/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 479/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 480/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 481/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 482/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 483/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 484/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 485/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 486/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 487/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 488/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 489/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 490/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 491/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 492/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 493/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 494/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 495/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 496/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 497/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 498/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 499/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 500/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 501/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 502/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 503/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 504/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 505/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 506/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 507/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 508/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 509/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 510/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 511/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 512/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 513/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 514/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 515/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 516/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 517/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 518/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 519/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 520/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 521/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 522/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 523/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 524/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 525/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 526/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 527/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 528/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 529/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 530/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 531/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 532/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 533/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 534/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 535/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 536/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 537/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 538/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 539/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 540/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 541/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 542/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 543/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 544/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 545/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 546/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 547/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 548/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 549/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 550/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 551/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 552/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 553/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 554/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 555/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 556/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 557/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 558/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 559/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 560/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 561/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 562/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 563/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 564/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 565/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 566/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 567/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 568/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 569/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 570/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 571/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 572/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 573/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 574/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 575/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 576/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 577/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 578/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 579/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 580/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 581/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 582/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 583/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 584/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 585/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 586/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 587/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 588/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 589/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 590/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 591/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 592/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 593/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 594/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 595/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 596/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 597/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 598/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 599/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 600/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 601/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 602/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 603/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 604/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 605/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 606/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 607/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 608/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 609/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 610/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 611/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 612/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 613/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 614/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 615/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 616/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 617/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 618/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 619/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 620/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 621/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 622/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 623/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 624/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 625/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 626/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 627/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 628/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 629/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 630/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 631/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 632/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 633/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 634/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 635/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 636/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 637/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 638/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 639/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 640/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 641/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 642/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 643/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 644/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 645/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 646/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 647/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 648/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 649/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 650/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 651/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 652/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 653/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 654/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 655/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 656/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 657/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 658/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 659/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 660/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 661/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 662/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 663/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 664/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 665/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 666/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 667/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 668/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 669/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 670/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 671/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 672/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 673/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 674/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 675/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 676/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 677/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 678/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 679/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 680/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 681/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 682/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 683/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 684/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 685/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 686/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 687/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 688/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 689/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 690/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 691/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 692/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 693/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 694/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 695/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 696/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 697/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 698/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 699/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 700/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 701/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 702/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 703/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 704/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 705/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 706/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 707/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 708/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 709/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 710/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 711/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 712/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 713/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 714/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 715/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 716/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 717/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 718/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 719/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 720/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 721/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 722/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 723/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 724/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 725/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 726/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 727/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 728/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 729/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 730/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 731/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 732/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 733/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 734/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 735/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 736/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 737/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 738/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 739/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 740/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 741/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 742/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 743/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 744/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 745/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 746/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 747/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 748/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 749/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 750/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 751/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 752/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 753/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 754/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 755/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 756/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 757/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 758/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 759/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 760/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 761/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 762/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 763/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 764/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 765/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 766/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 767/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 768/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 769/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 770/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 771/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 772/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 773/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 774/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 775/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 776/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 777/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 778/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 779/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 780/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 781/1000\n",
      "24/24 [==============================] - 0s 3ms/step - loss: nan\n",
      "Epoch 782/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 783/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 784/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 785/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 786/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 787/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 788/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 789/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 790/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 791/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 792/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 793/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 794/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 795/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 796/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 797/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 798/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 799/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 800/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 801/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 802/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 803/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 804/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 805/1000\n",
      "24/24 [==============================] - 0s 3ms/step - loss: nan\n",
      "Epoch 806/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 807/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 808/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 809/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 810/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 811/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 812/1000\n",
      "24/24 [==============================] - 0s 3ms/step - loss: nan\n",
      "Epoch 813/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 814/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 815/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 816/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 817/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 818/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 819/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 820/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 821/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 822/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 823/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 824/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 825/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 826/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 827/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 828/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 829/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 830/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 831/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 832/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 833/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 834/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 835/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 836/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 837/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 838/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 839/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 840/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 841/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 842/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 843/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 844/1000\n",
      "24/24 [==============================] - 0s 3ms/step - loss: nan\n",
      "Epoch 845/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 846/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 847/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 848/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 849/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 850/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 851/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 852/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 853/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 854/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 855/1000\n",
      "24/24 [==============================] - 0s 3ms/step - loss: nan\n",
      "Epoch 856/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 857/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 858/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 859/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 860/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 861/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 862/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 863/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 864/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 865/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 866/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 867/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 868/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 869/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 870/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 871/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 872/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 873/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 874/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 875/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 876/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 877/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 878/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 879/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 880/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 881/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 882/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 883/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 884/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 885/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 886/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 887/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 888/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 889/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 890/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 891/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 892/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 893/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 894/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 895/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 896/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 897/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 898/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 899/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 900/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 901/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 902/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 903/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 904/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 905/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 906/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 907/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 908/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 909/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 910/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 911/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 912/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 913/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 914/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 915/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 916/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 917/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 918/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 919/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 920/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 921/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 922/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 923/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 924/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 925/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 926/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 927/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 928/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 929/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 930/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 931/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 932/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 933/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 934/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 935/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 936/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 937/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 938/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 939/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 940/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 941/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 942/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 943/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 944/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 945/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 946/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 947/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 948/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 949/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 950/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 951/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 952/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 953/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 954/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 955/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 956/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 957/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 958/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 959/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 960/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 961/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 962/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 963/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 964/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 965/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 966/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 967/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 968/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 969/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 970/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 971/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 972/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 973/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 974/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 975/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 976/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 977/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 978/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 979/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 980/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 981/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 982/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 983/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 984/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 985/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 986/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 987/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 988/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 989/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 990/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 991/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 992/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 993/1000\n",
      "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 994/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 995/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 996/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 997/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 998/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 999/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 1000/1000\n",
      "24/24 [==============================] - 0s 2ms/step - loss: nan\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 43) for input KerasTensor(type_spec=TensorSpec(shape=(None, 43), dtype=tf.float32, name='dense_33_input'), name='dense_33_input', description=\"created by layer 'dense_33_input'\"), but it was called on an input with incompatible shape (32, 1).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 277, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"sequential_6\" \"                 f\"(type Sequential).\n    \n    Input 0 of layer \"dense_33\" is incompatible with the layer: expected axis -1 of input shape to have value 43, but received input with shape (32, 1)\n    \n    Call arguments received by layer \"sequential_6\" \"                 f\"(type Sequential):\n      • inputs=tf.Tensor(shape=(32, 1), dtype=float32)\n      • training=False\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21464/2849250704.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean_squared_error'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                     \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 277, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"sequential_6\" \"                 f\"(type Sequential).\n    \n    Input 0 of layer \"dense_33\" is incompatible with the layer: expected axis -1 of input shape to have value 43, but received input with shape (32, 1)\n    \n    Call arguments received by layer \"sequential_6\" \"                 f\"(type Sequential):\n      • inputs=tf.Tensor(shape=(32, 1), dtype=float32)\n      • training=False\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1000, input_dim=43, activation='relu'))\n",
    "model.add(Dense(21, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error',optimizer='adam')\n",
    "model.fit(X_train,y_train, epochs=1000, batch_size=50)\n",
    "model.predict(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b4dc330d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       ...,\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan]], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(df_house_test_extracted2b_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b53945e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c575f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ed39c109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168     12.337105\n",
       "605     11.898195\n",
       "548     11.635152\n",
       "65      12.100718\n",
       "628     11.002117\n",
       "          ...    \n",
       "1367    12.429220\n",
       "265     12.097936\n",
       "1425    12.098493\n",
       "493     11.695255\n",
       "1340    12.660331\n",
       "Name: SalePrice(log), Length: 288, dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f0bf09ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.337105293891872"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "47081d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.337105293891872"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "95d98b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "29669be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "be02792b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual value : 12.337, predicted value : nan\n",
      "actual value : 11.898, predicted value : nan\n",
      "actual value : 11.635, predicted value : nan\n",
      "actual value : 12.101, predicted value : nan\n",
      "actual value : 11.002, predicted value : nan\n",
      "actual value : 11.356, predicted value : nan\n",
      "actual value : 12.278, predicted value : nan\n",
      "actual value : 11.914, predicted value : nan\n",
      "actual value : 11.736, predicted value : nan\n",
      "actual value : 12.395, predicted value : nan\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    label = y_test.iloc[i]\n",
    "    prediction = y_pred[i]\n",
    "    print('actual value : {:.3f}, predicted value : {:.3f}'.format(label,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "23ba7107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "116/116 [==============================] - 1s 1ms/step - loss: nan\n",
      "Epoch 2/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 3/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 4/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 5/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 6/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 7/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 8/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 9/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 10/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 11/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 12/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 13/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 14/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 15/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 16/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 17/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 18/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 19/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 20/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 21/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 22/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 23/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 24/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 25/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 26/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 27/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 28/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 29/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 30/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 31/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 32/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 33/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 34/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 35/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 36/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 37/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 38/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 39/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 40/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 41/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 42/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 43/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 44/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 45/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 46/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 47/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 48/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 49/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 50/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 51/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 52/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 53/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 54/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 55/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 56/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 57/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 58/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 59/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 60/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 61/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 62/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 63/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 64/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 65/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 66/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 67/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 68/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 69/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 70/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 71/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 72/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 73/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 74/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 75/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 76/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 77/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 78/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 79/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 80/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 81/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 82/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 83/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 84/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 85/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 86/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 87/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 88/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 89/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 90/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 91/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 92/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 93/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 94/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 95/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 96/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 97/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 98/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 99/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 100/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 101/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 102/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 103/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 104/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 105/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 106/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 107/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 108/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 109/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 110/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 111/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 112/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 113/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 114/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 115/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 116/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 117/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 118/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 119/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 120/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 121/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 122/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 123/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 124/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 125/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 126/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 127/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 128/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 129/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 130/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 131/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 132/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 133/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 134/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 135/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 136/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 137/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 138/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 139/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 140/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 141/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 142/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 143/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 144/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 145/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 146/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 147/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 148/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 149/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 150/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 151/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 152/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 153/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 154/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 155/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 156/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 157/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 158/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 159/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 160/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 161/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 162/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 163/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 164/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 165/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 166/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 167/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 168/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 169/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 170/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 171/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 172/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 173/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 174/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 175/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 176/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 177/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 178/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 179/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 180/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 181/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 182/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 183/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 184/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 185/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 186/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 187/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 188/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 189/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 190/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 191/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 192/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 193/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 194/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 195/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 196/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 197/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 198/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 199/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 200/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2475be2d850>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# 30, 13, 6\n",
    "model.add(Dense(90, input_dim=43, activation='relu'))\n",
    "model.add(Dense(21, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error',optimizer='adam')\n",
    "model.fit(X_train,y_train, epochs=200, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "75916946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(df_house_test_extracted2b_predicted).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "77312893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, ..., nan, nan, nan], dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0cdf829d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred(~pd.isna(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ddab6a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ...,  True,  True,  True])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isna(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0e460036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ..., False, False, False])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "~pd.isna(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fd2eb9e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "~pd.isna(y_pred)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b307d7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### All the items are False\n",
    "for i in range(len(y_pred)):\n",
    "    if ~pd.isna(y_pred)[i]==True:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d76a16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "771734f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 2/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 3/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 4/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 5/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 6/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 7/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 8/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 9/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 10/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 11/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 12/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 13/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 14/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 15/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 16/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 17/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 18/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 19/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 20/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 21/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 22/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 23/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 24/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 25/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 26/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 27/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 28/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 29/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 30/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 31/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 32/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 33/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 34/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 35/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 36/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 37/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 38/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 39/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 40/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 41/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 42/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 43/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 44/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 45/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 46/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 47/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 48/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 49/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 50/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 51/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 52/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 53/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 54/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 55/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 56/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 57/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 58/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 59/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 60/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 61/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 62/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 63/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 64/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 65/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 66/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 67/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 68/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 69/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 70/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 71/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 72/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 73/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 74/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 75/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 76/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 77/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 78/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 79/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 81/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 82/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 83/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 84/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 85/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 86/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 87/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 88/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 89/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 90/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 91/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 92/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 93/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 94/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 95/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 96/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 97/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 98/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 99/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 100/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 101/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 102/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 103/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 104/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 105/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 106/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 107/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 108/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 109/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 110/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 111/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 112/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 113/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 114/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 115/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 116/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 117/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 118/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 119/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 120/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 121/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 122/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 123/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 124/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 125/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 126/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 127/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 128/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 129/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 130/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 131/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 132/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 133/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 134/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 135/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 136/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 137/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 138/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 139/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 140/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 141/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 142/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 143/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 144/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 145/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 146/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 147/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 148/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 149/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 150/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 151/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 152/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 153/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 154/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 155/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 156/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 157/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 158/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 160/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 161/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 162/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 163/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 164/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 165/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 166/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 167/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 168/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 169/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 170/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 171/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 172/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 173/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 174/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 175/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 176/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 177/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 178/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 179/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 180/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 181/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 182/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 183/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 184/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 185/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 186/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 187/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 188/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 189/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 190/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 191/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 192/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 193/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 194/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 195/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 196/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 197/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 198/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 199/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 200/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x247614196a0>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# 30, 13, 6\n",
    "model.add(Dense(90, input_dim=43, activation='relu'))\n",
    "model.add(Dense(21, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error',optimizer='adam', metrics=['accuracy']) # \n",
    "model.fit(X_train,y_train, epochs=200, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f0a5a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "03fd3c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 368ms/epoch - 8ms/step\n",
      "Epoch 2/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 59ms/epoch - 1ms/step\n",
      "Epoch 3/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 52ms/epoch - 1ms/step\n",
      "Epoch 4/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 53ms/epoch - 1ms/step\n",
      "Epoch 5/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 57ms/epoch - 1ms/step\n",
      "Epoch 6/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 54ms/epoch - 1ms/step\n",
      "Epoch 7/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 8/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 55ms/epoch - 1ms/step\n",
      "Epoch 9/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 53ms/epoch - 1ms/step\n",
      "Epoch 10/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 11/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 58ms/epoch - 1ms/step\n",
      "Epoch 12/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 54ms/epoch - 1ms/step\n",
      "Epoch 13/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 53ms/epoch - 1ms/step\n",
      "Epoch 14/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 15/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 54ms/epoch - 1ms/step\n",
      "Epoch 16/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 62ms/epoch - 1ms/step\n",
      "Epoch 17/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 53ms/epoch - 1ms/step\n",
      "Epoch 18/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 52ms/epoch - 1ms/step\n",
      "Epoch 19/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 20/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 56ms/epoch - 1ms/step\n",
      "Epoch 21/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 52ms/epoch - 1ms/step\n",
      "Epoch 22/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 53ms/epoch - 1ms/step\n",
      "Epoch 23/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 53ms/epoch - 1ms/step\n",
      "Epoch 24/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 54ms/epoch - 1ms/step\n",
      "Epoch 25/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 54ms/epoch - 1ms/step\n",
      "Epoch 26/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 58ms/epoch - 1ms/step\n",
      "Epoch 27/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 28/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 53ms/epoch - 1ms/step\n",
      "Epoch 29/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 54ms/epoch - 1ms/step\n",
      "Epoch 30/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 53ms/epoch - 1ms/step\n",
      "Epoch 31/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 52ms/epoch - 1ms/step\n",
      "Epoch 32/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 33/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 56ms/epoch - 1ms/step\n",
      "Epoch 34/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 52ms/epoch - 1ms/step\n",
      "Epoch 35/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 52ms/epoch - 1ms/step\n",
      "Epoch 36/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 37/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 56ms/epoch - 1ms/step\n",
      "Epoch 38/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 53ms/epoch - 1ms/step\n",
      "Epoch 39/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 55ms/epoch - 1ms/step\n",
      "Epoch 40/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 52ms/epoch - 1ms/step\n",
      "Epoch 41/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 54ms/epoch - 1ms/step\n",
      "Epoch 42/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 53ms/epoch - 1ms/step\n",
      "Epoch 43/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 53ms/epoch - 1ms/step\n",
      "Epoch 44/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 52ms/epoch - 1ms/step\n",
      "Epoch 45/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 46/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 52ms/epoch - 1ms/step\n",
      "Epoch 47/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 55ms/epoch - 1ms/step\n",
      "Epoch 48/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 52ms/epoch - 1ms/step\n",
      "Epoch 49/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 52ms/epoch - 1ms/step\n",
      "Epoch 50/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 52ms/epoch - 1ms/step\n",
      "Epoch 51/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 56ms/epoch - 1ms/step\n",
      "Epoch 52/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 52ms/epoch - 1ms/step\n",
      "Epoch 53/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 54/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 63ms/epoch - 1ms/step\n",
      "Epoch 55/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 57ms/epoch - 1ms/step\n",
      "Epoch 56/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 63ms/epoch - 1ms/step\n",
      "Epoch 57/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 60ms/epoch - 1ms/step\n",
      "Epoch 58/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 94ms/epoch - 2ms/step\n",
      "Epoch 59/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 59ms/epoch - 1ms/step\n",
      "Epoch 60/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 59ms/epoch - 1ms/step\n",
      "Epoch 61/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 55ms/epoch - 1ms/step\n",
      "Epoch 62/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 55ms/epoch - 1ms/step\n",
      "Epoch 63/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 56ms/epoch - 1ms/step\n",
      "Epoch 64/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 58ms/epoch - 1ms/step\n",
      "Epoch 65/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 57ms/epoch - 1ms/step\n",
      "Epoch 66/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 52ms/epoch - 1ms/step\n",
      "Epoch 67/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 55ms/epoch - 1ms/step\n",
      "Epoch 68/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 56ms/epoch - 1ms/step\n",
      "Epoch 69/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 55ms/epoch - 1ms/step\n",
      "Epoch 70/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 54ms/epoch - 1ms/step\n",
      "Epoch 71/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 53ms/epoch - 1ms/step\n",
      "Epoch 72/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 47ms/epoch - 1ms/step\n",
      "Epoch 73/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 45ms/epoch - 997us/step\n",
      "Epoch 74/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 52ms/epoch - 1ms/step\n",
      "Epoch 75/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 76/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 77/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 78ms/epoch - 2ms/step\n",
      "Epoch 78/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 79/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 85ms/epoch - 2ms/step\n",
      "Epoch 80/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 81/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 61ms/epoch - 1ms/step\n",
      "Epoch 82/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 83/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 44ms/epoch - 975us/step\n",
      "Epoch 84/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 85/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 86/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 47ms/epoch - 1ms/step\n",
      "Epoch 87/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 88/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 89/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 90/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 91/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 92/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 93/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 94/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 95/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 96/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 97/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 52ms/epoch - 1ms/step\n",
      "Epoch 98/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 99/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 100/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 101/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 102/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 103/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 52ms/epoch - 1ms/step\n",
      "Epoch 104/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 57ms/epoch - 1ms/step\n",
      "Epoch 105/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 106/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 54ms/epoch - 1ms/step\n",
      "Epoch 107/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 57ms/epoch - 1ms/step\n",
      "Epoch 108/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 56ms/epoch - 1ms/step\n",
      "Epoch 109/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 56ms/epoch - 1ms/step\n",
      "Epoch 110/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 57ms/epoch - 1ms/step\n",
      "Epoch 111/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 52ms/epoch - 1ms/step\n",
      "Epoch 112/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 53ms/epoch - 1ms/step\n",
      "Epoch 113/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 52ms/epoch - 1ms/step\n",
      "Epoch 114/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 115/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 52ms/epoch - 1ms/step\n",
      "Epoch 116/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 118/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 119/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 52ms/epoch - 1ms/step\n",
      "Epoch 120/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 121/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 122/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 123/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 124/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 125/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 126/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 127/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 128/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 129/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 58ms/epoch - 1ms/step\n",
      "Epoch 130/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 52ms/epoch - 1ms/step\n",
      "Epoch 131/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 47ms/epoch - 1ms/step\n",
      "Epoch 132/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 133/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 134/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 135/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 136/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 137/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 138/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 139/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 140/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 141/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 142/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 143/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 144/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 145/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 146/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 147/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 148/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 47ms/epoch - 1ms/step\n",
      "Epoch 149/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 150/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 151/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 152/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 153/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 154/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 155/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 156/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 157/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 158/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 159/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 160/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 161/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 162/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 163/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 164/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 165/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 166/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 167/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 168/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 169/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 170/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 171/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 172/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 173/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 174/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 175/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 176/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 177/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 178/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 179/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 180/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 181/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 182/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 183/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 184/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 185/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 186/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 187/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 188/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 52ms/epoch - 1ms/step\n",
      "Epoch 189/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 64ms/epoch - 1ms/step\n",
      "Epoch 190/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 46ms/epoch - 1ms/step\n",
      "Epoch 191/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 192/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 47ms/epoch - 1ms/step\n",
      "Epoch 193/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 47ms/epoch - 1ms/step\n",
      "Epoch 194/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 46ms/epoch - 1ms/step\n",
      "Epoch 195/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 196/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 46ms/epoch - 1ms/step\n",
      "Epoch 197/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 47ms/epoch - 1ms/step\n",
      "Epoch 198/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 47ms/epoch - 1ms/step\n",
      "Epoch 199/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 200/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 201/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 46ms/epoch - 1ms/step\n",
      "Epoch 202/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 203/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 204/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 205/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 206/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 207/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 208/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 209/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 47ms/epoch - 1ms/step\n",
      "Epoch 210/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 211/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 212/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 213/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 214/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 215/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 216/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 217/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 218/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 219/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 220/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 221/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 222/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 223/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 224/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 225/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 226/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 227/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 228/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 229/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 230/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 231/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 232/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 233/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 234/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 235/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 236/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 237/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 238/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 239/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 240/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 241/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 242/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 243/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 244/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 245/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 246/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 247/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 248/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 249/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 250/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 251/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 252/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 253/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 254/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 255/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 256/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 257/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 258/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 259/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 260/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 261/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 262/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 263/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 264/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 265/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 266/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 267/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 268/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 269/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 270/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 47ms/epoch - 1ms/step\n",
      "Epoch 271/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 272/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 273/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 274/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 275/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 276/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 277/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 278/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 279/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 280/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 281/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 282/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 283/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 284/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 285/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 286/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 287/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 288/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 289/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 290/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 291/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 292/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 293/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 47ms/epoch - 1ms/step\n",
      "Epoch 294/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 295/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 296/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 297/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 298/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 299/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 300/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 301/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 302/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 303/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 304/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 305/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 306/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 53ms/epoch - 1ms/step\n",
      "Epoch 307/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 54ms/epoch - 1ms/step\n",
      "Epoch 308/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 309/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 310/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 311/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 312/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 313/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 314/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 315/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 316/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 317/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 318/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 319/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 320/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 321/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 322/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 323/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 324/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 325/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 326/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 327/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 328/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 329/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 330/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 331/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 332/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 333/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 334/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 52ms/epoch - 1ms/step\n",
      "Epoch 335/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 336/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 337/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 338/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 339/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 340/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 341/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 342/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 343/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 344/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 54ms/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 345/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 346/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 347/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 348/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 349/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 350/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 351/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 53ms/epoch - 1ms/step\n",
      "Epoch 352/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 353/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 354/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 355/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 356/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 357/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 358/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 359/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 360/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 54ms/epoch - 1ms/step\n",
      "Epoch 361/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 362/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 59ms/epoch - 1ms/step\n",
      "Epoch 363/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 61ms/epoch - 1ms/step\n",
      "Epoch 364/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 54ms/epoch - 1ms/step\n",
      "Epoch 365/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 58ms/epoch - 1ms/step\n",
      "Epoch 366/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 367/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 53ms/epoch - 1ms/step\n",
      "Epoch 368/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 58ms/epoch - 1ms/step\n",
      "Epoch 369/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 55ms/epoch - 1ms/step\n",
      "Epoch 370/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 56ms/epoch - 1ms/step\n",
      "Epoch 371/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 55ms/epoch - 1ms/step\n",
      "Epoch 372/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 61ms/epoch - 1ms/step\n",
      "Epoch 373/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 55ms/epoch - 1ms/step\n",
      "Epoch 374/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 56ms/epoch - 1ms/step\n",
      "Epoch 375/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 54ms/epoch - 1ms/step\n",
      "Epoch 376/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 56ms/epoch - 1ms/step\n",
      "Epoch 377/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 56ms/epoch - 1ms/step\n",
      "Epoch 378/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 55ms/epoch - 1ms/step\n",
      "Epoch 379/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 53ms/epoch - 1ms/step\n",
      "Epoch 380/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 55ms/epoch - 1ms/step\n",
      "Epoch 381/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 382/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 383/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 384/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 385/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 386/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 47ms/epoch - 1ms/step\n",
      "Epoch 387/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 47ms/epoch - 1ms/step\n",
      "Epoch 388/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 47ms/epoch - 1ms/step\n",
      "Epoch 389/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 390/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 391/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 392/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 393/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 394/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 395/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 47ms/epoch - 1ms/step\n",
      "Epoch 396/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 47ms/epoch - 1ms/step\n",
      "Epoch 397/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 398/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 399/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 400/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 401/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 402/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 403/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 52ms/epoch - 1ms/step\n",
      "Epoch 404/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 405/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 406/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 407/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 408/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 409/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 53ms/epoch - 1ms/step\n",
      "Epoch 410/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 411/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 54ms/epoch - 1ms/step\n",
      "Epoch 412/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 54ms/epoch - 1ms/step\n",
      "Epoch 413/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 56ms/epoch - 1ms/step\n",
      "Epoch 414/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 58ms/epoch - 1ms/step\n",
      "Epoch 415/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 54ms/epoch - 1ms/step\n",
      "Epoch 416/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 53ms/epoch - 1ms/step\n",
      "Epoch 417/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 52ms/epoch - 1ms/step\n",
      "Epoch 418/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 54ms/epoch - 1ms/step\n",
      "Epoch 419/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 55ms/epoch - 1ms/step\n",
      "Epoch 420/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 57ms/epoch - 1ms/step\n",
      "Epoch 421/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 55ms/epoch - 1ms/step\n",
      "Epoch 422/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 56ms/epoch - 1ms/step\n",
      "Epoch 423/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 56ms/epoch - 1ms/step\n",
      "Epoch 424/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 60ms/epoch - 1ms/step\n",
      "Epoch 425/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 57ms/epoch - 1ms/step\n",
      "Epoch 426/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 55ms/epoch - 1ms/step\n",
      "Epoch 427/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 55ms/epoch - 1ms/step\n",
      "Epoch 428/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 52ms/epoch - 1ms/step\n",
      "Epoch 429/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 430/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 431/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 432/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 53ms/epoch - 1ms/step\n",
      "Epoch 433/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 434/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 435/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 52ms/epoch - 1ms/step\n",
      "Epoch 436/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 53ms/epoch - 1ms/step\n",
      "Epoch 437/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 53ms/epoch - 1ms/step\n",
      "Epoch 438/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 439/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 440/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 441/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 55ms/epoch - 1ms/step\n",
      "Epoch 442/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 443/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 52ms/epoch - 1ms/step\n",
      "Epoch 444/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 445/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 446/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 52ms/epoch - 1ms/step\n",
      "Epoch 447/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 56ms/epoch - 1ms/step\n",
      "Epoch 448/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 449/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 450/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 53ms/epoch - 1ms/step\n",
      "Epoch 451/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 452/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 57ms/epoch - 1ms/step\n",
      "Epoch 453/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 56ms/epoch - 1ms/step\n",
      "Epoch 454/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 56ms/epoch - 1ms/step\n",
      "Epoch 455/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 55ms/epoch - 1ms/step\n",
      "Epoch 456/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 52ms/epoch - 1ms/step\n",
      "Epoch 457/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 54ms/epoch - 1ms/step\n",
      "Epoch 458/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 52ms/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 459/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 460/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 52ms/epoch - 1ms/step\n",
      "Epoch 461/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 462/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 463/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 53ms/epoch - 1ms/step\n",
      "Epoch 464/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 55ms/epoch - 1ms/step\n",
      "Epoch 465/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 55ms/epoch - 1ms/step\n",
      "Epoch 466/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 56ms/epoch - 1ms/step\n",
      "Epoch 467/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 55ms/epoch - 1ms/step\n",
      "Epoch 468/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 55ms/epoch - 1ms/step\n",
      "Epoch 469/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 470/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 471/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 472/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 473/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 474/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 475/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 476/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 477/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 478/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 479/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 52ms/epoch - 1ms/step\n",
      "Epoch 480/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 53ms/epoch - 1ms/step\n",
      "Epoch 481/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 482/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 483/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 484/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 485/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 486/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 487/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 488/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 489/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 490/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 491/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 55ms/epoch - 1ms/step\n",
      "Epoch 492/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 493/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 494/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 495/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 496/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 53ms/epoch - 1ms/step\n",
      "Epoch 497/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 57ms/epoch - 1ms/step\n",
      "Epoch 498/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 55ms/epoch - 1ms/step\n",
      "Epoch 499/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 59ms/epoch - 1ms/step\n",
      "Epoch 500/500\n",
      "45/45 - 0s - loss: nan - mse: nan - 56ms/epoch - 1ms/step\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(21, input_shape=(43,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "#---------------------------------------------\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "#----------------------------------\n",
    "# model.add(Dense(16))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "#----------------------------\n",
    "model.add(Dense(1))\n",
    "#-------------------------------------------------------------------------------\n",
    "model.compile(loss='mean_squared_error', metrics=['mse'], optimizer='adam')\n",
    "history = model.fit(X, np.ravel(y), epochs=500, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "69c3f806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import ELU, PReLU, LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b4c2ae85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "36/36 - 1s - loss: nan - mse: nan - 562ms/epoch - 16ms/step\n",
      "Epoch 2/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 84ms/epoch - 2ms/step\n",
      "Epoch 3/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 83ms/epoch - 2ms/step\n",
      "Epoch 4/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 5/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 79ms/epoch - 2ms/step\n",
      "Epoch 6/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 7/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 8/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 78ms/epoch - 2ms/step\n",
      "Epoch 9/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 10/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 81ms/epoch - 2ms/step\n",
      "Epoch 11/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 12/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 13/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 14/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 15/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 16/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 17/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 18/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 19/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 20/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 21/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 22/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 23/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 78ms/epoch - 2ms/step\n",
      "Epoch 24/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 25/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 26/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 27/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 78ms/epoch - 2ms/step\n",
      "Epoch 28/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 29/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 30/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 31/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 32/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 33/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 34/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 35/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 36/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 79ms/epoch - 2ms/step\n",
      "Epoch 37/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 38/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 39/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 40/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 41/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 42/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 43/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 44/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 72ms/epoch - 2ms/step\n",
      "Epoch 45/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 86ms/epoch - 2ms/step\n",
      "Epoch 46/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 79ms/epoch - 2ms/step\n",
      "Epoch 47/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 80ms/epoch - 2ms/step\n",
      "Epoch 48/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 79ms/epoch - 2ms/step\n",
      "Epoch 49/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 79ms/epoch - 2ms/step\n",
      "Epoch 50/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 79ms/epoch - 2ms/step\n",
      "Epoch 51/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 81ms/epoch - 2ms/step\n",
      "Epoch 52/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 53/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 78ms/epoch - 2ms/step\n",
      "Epoch 54/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 55/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 56/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 57/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 78ms/epoch - 2ms/step\n",
      "Epoch 58/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 59/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 60/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 61/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 62/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 63/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 64/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 65/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 66/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 67/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 80ms/epoch - 2ms/step\n",
      "Epoch 68/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 69/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 70/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 71/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 84ms/epoch - 2ms/step\n",
      "Epoch 72/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 81ms/epoch - 2ms/step\n",
      "Epoch 73/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 84ms/epoch - 2ms/step\n",
      "Epoch 74/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 82ms/epoch - 2ms/step\n",
      "Epoch 75/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 84ms/epoch - 2ms/step\n",
      "Epoch 76/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 86ms/epoch - 2ms/step\n",
      "Epoch 77/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 85ms/epoch - 2ms/step\n",
      "Epoch 78/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 83ms/epoch - 2ms/step\n",
      "Epoch 79/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 88ms/epoch - 2ms/step\n",
      "Epoch 80/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 85ms/epoch - 2ms/step\n",
      "Epoch 81/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 83ms/epoch - 2ms/step\n",
      "Epoch 82/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 83ms/epoch - 2ms/step\n",
      "Epoch 83/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 82ms/epoch - 2ms/step\n",
      "Epoch 84/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 82ms/epoch - 2ms/step\n",
      "Epoch 85/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 83ms/epoch - 2ms/step\n",
      "Epoch 86/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 82ms/epoch - 2ms/step\n",
      "Epoch 87/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 81ms/epoch - 2ms/step\n",
      "Epoch 88/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 89/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 90/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 91/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 92/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 72ms/epoch - 2ms/step\n",
      "Epoch 93/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 94/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 95/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 96/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 78ms/epoch - 2ms/step\n",
      "Epoch 97/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 98/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 99/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 78ms/epoch - 2ms/step\n",
      "Epoch 100/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 101/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 102/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 79ms/epoch - 2ms/step\n",
      "Epoch 103/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 104/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 105/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 70ms/epoch - 2ms/step\n",
      "Epoch 106/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 81ms/epoch - 2ms/step\n",
      "Epoch 107/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 108/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 72ms/epoch - 2ms/step\n",
      "Epoch 109/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 110/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 79ms/epoch - 2ms/step\n",
      "Epoch 111/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 112/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 113/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 114/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 115/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 116/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 118/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 119/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 78ms/epoch - 2ms/step\n",
      "Epoch 120/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 87ms/epoch - 2ms/step\n",
      "Epoch 121/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 94ms/epoch - 3ms/step\n",
      "Epoch 122/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 79ms/epoch - 2ms/step\n",
      "Epoch 123/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 124/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 125/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 126/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 127/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 128/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 129/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 130/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 131/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 132/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 133/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 134/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 135/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 136/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 137/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 72ms/epoch - 2ms/step\n",
      "Epoch 138/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 139/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 140/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 141/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 72ms/epoch - 2ms/step\n",
      "Epoch 142/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 143/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 144/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 145/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 72ms/epoch - 2ms/step\n",
      "Epoch 146/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 147/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 148/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 149/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 150/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 151/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 152/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 153/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 78ms/epoch - 2ms/step\n",
      "Epoch 154/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 78ms/epoch - 2ms/step\n",
      "Epoch 155/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 156/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 157/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 158/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 159/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 160/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 81ms/epoch - 2ms/step\n",
      "Epoch 161/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 162/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 78ms/epoch - 2ms/step\n",
      "Epoch 163/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 90ms/epoch - 2ms/step\n",
      "Epoch 164/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 87ms/epoch - 2ms/step\n",
      "Epoch 165/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 84ms/epoch - 2ms/step\n",
      "Epoch 166/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 167/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 70ms/epoch - 2ms/step\n",
      "Epoch 168/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 169/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 170/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 171/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 172/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 173/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 174/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 175/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 176/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 177/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 178/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 84ms/epoch - 2ms/step\n",
      "Epoch 179/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 180/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 181/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 182/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 183/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 184/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 185/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 186/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 71ms/epoch - 2ms/step\n",
      "Epoch 187/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 188/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 71ms/epoch - 2ms/step\n",
      "Epoch 189/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 78ms/epoch - 2ms/step\n",
      "Epoch 190/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 80ms/epoch - 2ms/step\n",
      "Epoch 191/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 83ms/epoch - 2ms/step\n",
      "Epoch 192/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 80ms/epoch - 2ms/step\n",
      "Epoch 193/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 83ms/epoch - 2ms/step\n",
      "Epoch 194/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 84ms/epoch - 2ms/step\n",
      "Epoch 195/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 83ms/epoch - 2ms/step\n",
      "Epoch 196/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 81ms/epoch - 2ms/step\n",
      "Epoch 197/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 80ms/epoch - 2ms/step\n",
      "Epoch 198/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 79ms/epoch - 2ms/step\n",
      "Epoch 199/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 200/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 201/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 202/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 203/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 204/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 80ms/epoch - 2ms/step\n",
      "Epoch 205/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 80ms/epoch - 2ms/step\n",
      "Epoch 206/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 207/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 208/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 80ms/epoch - 2ms/step\n",
      "Epoch 209/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 80ms/epoch - 2ms/step\n",
      "Epoch 210/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 211/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 83ms/epoch - 2ms/step\n",
      "Epoch 212/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 213/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 214/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 215/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 216/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 72ms/epoch - 2ms/step\n",
      "Epoch 217/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 218/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 219/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 220/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 79ms/epoch - 2ms/step\n",
      "Epoch 221/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 222/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 79ms/epoch - 2ms/step\n",
      "Epoch 223/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 89ms/epoch - 2ms/step\n",
      "Epoch 224/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 92ms/epoch - 3ms/step\n",
      "Epoch 225/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 88ms/epoch - 2ms/step\n",
      "Epoch 226/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 227/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 228/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 72ms/epoch - 2ms/step\n",
      "Epoch 229/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 80ms/epoch - 2ms/step\n",
      "Epoch 230/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 90ms/epoch - 2ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 231/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 88ms/epoch - 2ms/step\n",
      "Epoch 232/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 233/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 234/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 235/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 236/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 237/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 238/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 78ms/epoch - 2ms/step\n",
      "Epoch 239/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 71ms/epoch - 2ms/step\n",
      "Epoch 240/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 241/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 242/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 243/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 244/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 72ms/epoch - 2ms/step\n",
      "Epoch 245/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 72ms/epoch - 2ms/step\n",
      "Epoch 246/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 247/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 119ms/epoch - 3ms/step\n",
      "Epoch 248/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 79ms/epoch - 2ms/step\n",
      "Epoch 249/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 250/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 70ms/epoch - 2ms/step\n",
      "Epoch 251/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 78ms/epoch - 2ms/step\n",
      "Epoch 252/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 253/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 254/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 79ms/epoch - 2ms/step\n",
      "Epoch 255/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 256/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 257/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 258/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 259/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 260/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 261/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 262/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 72ms/epoch - 2ms/step\n",
      "Epoch 263/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 72ms/epoch - 2ms/step\n",
      "Epoch 264/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 265/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 266/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 267/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 268/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 269/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 270/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 271/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 272/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 78ms/epoch - 2ms/step\n",
      "Epoch 273/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 71ms/epoch - 2ms/step\n",
      "Epoch 274/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 85ms/epoch - 2ms/step\n",
      "Epoch 275/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 82ms/epoch - 2ms/step\n",
      "Epoch 276/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 81ms/epoch - 2ms/step\n",
      "Epoch 277/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 81ms/epoch - 2ms/step\n",
      "Epoch 278/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 84ms/epoch - 2ms/step\n",
      "Epoch 279/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 86ms/epoch - 2ms/step\n",
      "Epoch 280/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 83ms/epoch - 2ms/step\n",
      "Epoch 281/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 83ms/epoch - 2ms/step\n",
      "Epoch 282/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 86ms/epoch - 2ms/step\n",
      "Epoch 283/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 84ms/epoch - 2ms/step\n",
      "Epoch 284/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 82ms/epoch - 2ms/step\n",
      "Epoch 285/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 84ms/epoch - 2ms/step\n",
      "Epoch 286/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 86ms/epoch - 2ms/step\n",
      "Epoch 287/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 82ms/epoch - 2ms/step\n",
      "Epoch 288/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 83ms/epoch - 2ms/step\n",
      "Epoch 289/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 80ms/epoch - 2ms/step\n",
      "Epoch 290/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 82ms/epoch - 2ms/step\n",
      "Epoch 291/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 78ms/epoch - 2ms/step\n",
      "Epoch 292/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 293/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 294/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 295/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 296/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 297/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 298/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 80ms/epoch - 2ms/step\n",
      "Epoch 299/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 300/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 301/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 302/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 303/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 304/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 78ms/epoch - 2ms/step\n",
      "Epoch 305/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 306/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 307/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 308/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 309/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 310/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 311/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 78ms/epoch - 2ms/step\n",
      "Epoch 312/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 313/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 314/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 315/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 316/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 317/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 318/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 319/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 320/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 321/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 322/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 323/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 324/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 325/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 326/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 327/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 328/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 329/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 330/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 331/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 332/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 333/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 78ms/epoch - 2ms/step\n",
      "Epoch 334/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 335/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 336/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 337/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 338/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 339/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 340/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 341/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 342/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 343/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 79ms/epoch - 2ms/step\n",
      "Epoch 344/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 345/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 346/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 347/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 72ms/epoch - 2ms/step\n",
      "Epoch 348/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 72ms/epoch - 2ms/step\n",
      "Epoch 349/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 350/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 351/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 72ms/epoch - 2ms/step\n",
      "Epoch 352/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 353/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 354/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 355/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 356/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 357/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 358/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 359/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 360/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 361/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 362/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 363/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 364/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 365/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 366/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 367/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 368/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 70ms/epoch - 2ms/step\n",
      "Epoch 369/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 370/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 371/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 72ms/epoch - 2ms/step\n",
      "Epoch 372/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 373/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 374/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 375/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 69ms/epoch - 2ms/step\n",
      "Epoch 376/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 377/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 378/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 379/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 380/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 381/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 382/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 72ms/epoch - 2ms/step\n",
      "Epoch 383/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 384/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 385/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 386/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 387/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 388/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 389/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 390/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 391/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 392/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 393/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 394/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 395/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 396/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 79ms/epoch - 2ms/step\n",
      "Epoch 397/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 398/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 72ms/epoch - 2ms/step\n",
      "Epoch 399/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 400/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 401/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 79ms/epoch - 2ms/step\n",
      "Epoch 402/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 403/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 71ms/epoch - 2ms/step\n",
      "Epoch 404/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 405/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 406/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 407/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 408/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 79ms/epoch - 2ms/step\n",
      "Epoch 409/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 410/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 71ms/epoch - 2ms/step\n",
      "Epoch 411/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 412/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 413/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 414/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 415/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 416/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 417/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 418/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 419/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 420/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 79ms/epoch - 2ms/step\n",
      "Epoch 421/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 422/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 423/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 424/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 425/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 426/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 427/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 428/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 69ms/epoch - 2ms/step\n",
      "Epoch 429/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 430/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 431/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 432/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 433/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 434/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 435/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 436/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 437/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 438/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 439/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 440/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 441/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 75ms/epoch - 2ms/step\n",
      "Epoch 442/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 443/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 444/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 445/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 446/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 72ms/epoch - 2ms/step\n",
      "Epoch 447/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 448/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 73ms/epoch - 2ms/step\n",
      "Epoch 449/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 74ms/epoch - 2ms/step\n",
      "Epoch 450/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 451/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 78ms/epoch - 2ms/step\n",
      "Epoch 452/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 78ms/epoch - 2ms/step\n",
      "Epoch 453/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 454/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 80ms/epoch - 2ms/step\n",
      "Epoch 455/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 81ms/epoch - 2ms/step\n",
      "Epoch 456/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 78ms/epoch - 2ms/step\n",
      "Epoch 457/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 82ms/epoch - 2ms/step\n",
      "Epoch 458/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 79ms/epoch - 2ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 459/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 78ms/epoch - 2ms/step\n",
      "Epoch 460/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 78ms/epoch - 2ms/step\n",
      "Epoch 461/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 78ms/epoch - 2ms/step\n",
      "Epoch 462/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 79ms/epoch - 2ms/step\n",
      "Epoch 463/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 464/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 465/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 466/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 78ms/epoch - 2ms/step\n",
      "Epoch 467/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 80ms/epoch - 2ms/step\n",
      "Epoch 468/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 79ms/epoch - 2ms/step\n",
      "Epoch 469/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 470/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 471/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 77ms/epoch - 2ms/step\n",
      "Epoch 472/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 80ms/epoch - 2ms/step\n",
      "Epoch 473/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 82ms/epoch - 2ms/step\n",
      "Epoch 474/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 82ms/epoch - 2ms/step\n",
      "Epoch 475/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 78ms/epoch - 2ms/step\n",
      "Epoch 476/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 79ms/epoch - 2ms/step\n",
      "Epoch 477/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 78ms/epoch - 2ms/step\n",
      "Epoch 478/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 479/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 76ms/epoch - 2ms/step\n",
      "Epoch 480/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 88ms/epoch - 2ms/step\n",
      "Epoch 481/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 87ms/epoch - 2ms/step\n",
      "Epoch 482/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 86ms/epoch - 2ms/step\n",
      "Epoch 483/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 91ms/epoch - 3ms/step\n",
      "Epoch 484/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 88ms/epoch - 2ms/step\n",
      "Epoch 485/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 91ms/epoch - 3ms/step\n",
      "Epoch 486/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 91ms/epoch - 3ms/step\n",
      "Epoch 487/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 93ms/epoch - 3ms/step\n",
      "Epoch 488/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 87ms/epoch - 2ms/step\n",
      "Epoch 489/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 92ms/epoch - 3ms/step\n",
      "Epoch 490/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 92ms/epoch - 3ms/step\n",
      "Epoch 491/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 88ms/epoch - 2ms/step\n",
      "Epoch 492/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 84ms/epoch - 2ms/step\n",
      "Epoch 493/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 87ms/epoch - 2ms/step\n",
      "Epoch 494/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 88ms/epoch - 2ms/step\n",
      "Epoch 495/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 91ms/epoch - 3ms/step\n",
      "Epoch 496/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 89ms/epoch - 2ms/step\n",
      "Epoch 497/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 86ms/epoch - 2ms/step\n",
      "Epoch 498/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 80ms/epoch - 2ms/step\n",
      "Epoch 499/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 81ms/epoch - 2ms/step\n",
      "Epoch 500/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 78ms/epoch - 2ms/step\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#------------------------\n",
    "num_inputs = 43\n",
    "model.add(Dense(800,input_shape=(num_inputs,)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "#--------------------------------\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "#---------------------------\n",
    "model.add(Dense(50))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "#----------------------------------\n",
    "model.add(Dense(12))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "#----------------------------------\n",
    "#----------------------------\n",
    "model.add(Dense(1))\n",
    "#-------------------------------------------------------------------------------\n",
    "model.compile(loss='mean_squared_error', metrics=['mse'], optimizer='adam')\n",
    "history = model.fit(X_train, y_train, epochs=500, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b0ffae08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 281ms/epoch - 8ms/step\n",
      "Epoch 2/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 46ms/epoch - 1ms/step\n",
      "Epoch 3/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 4/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 46ms/epoch - 1ms/step\n",
      "Epoch 5/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 6/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 46ms/epoch - 1ms/step\n",
      "Epoch 7/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 46ms/epoch - 1ms/step\n",
      "Epoch 8/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 9/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 10/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 11/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 12/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 13/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 45ms/epoch - 1ms/step\n",
      "Epoch 14/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 47ms/epoch - 1ms/step\n",
      "Epoch 15/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 16/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 46ms/epoch - 1ms/step\n",
      "Epoch 17/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 18/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 19/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 20/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 21/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 45ms/epoch - 1ms/step\n",
      "Epoch 22/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 23/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 24/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 47ms/epoch - 1ms/step\n",
      "Epoch 25/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 26/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 47ms/epoch - 1ms/step\n",
      "Epoch 27/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 28/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 29/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 30/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 31/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 32/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 33/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 46ms/epoch - 1ms/step\n",
      "Epoch 34/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 35/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 36/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 37/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 38/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 39/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 40/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 41/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 42/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 43/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 44/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 45/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 46/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 47/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 48/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 49/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 46ms/epoch - 1ms/step\n",
      "Epoch 50/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 51/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 52/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 52ms/epoch - 1ms/step\n",
      "Epoch 53/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 54/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 45ms/epoch - 1ms/step\n",
      "Epoch 55/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 56/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 57/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 39ms/epoch - 1ms/step\n",
      "Epoch 58/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 59/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 60/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 61/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 62/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 63/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 38ms/epoch - 1ms/step\n",
      "Epoch 64/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 65/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 66/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 67/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 68/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 69/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 70/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 71/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 51ms/epoch - 1ms/step\n",
      "Epoch 72/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 45ms/epoch - 1ms/step\n",
      "Epoch 73/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 74/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 75/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 76/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 38ms/epoch - 1ms/step\n",
      "Epoch 77/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 78/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 79/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 80/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 81/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 82/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 39ms/epoch - 1ms/step\n",
      "Epoch 83/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 84/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 85/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 36ms/epoch - 997us/step\n",
      "Epoch 86/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 87/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 88/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 89/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 90/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 91/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 92/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 39ms/epoch - 1ms/step\n",
      "Epoch 93/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 94/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 45ms/epoch - 1ms/step\n",
      "Epoch 95/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 96/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 97/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 98/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 99/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 100/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 101/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 102/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 103/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 104/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 45ms/epoch - 1ms/step\n",
      "Epoch 105/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 106/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 107/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 108/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 109/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 110/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 111/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 112/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 113/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 45ms/epoch - 1ms/step\n",
      "Epoch 114/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 45ms/epoch - 1ms/step\n",
      "Epoch 115/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 116/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 45ms/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 118/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 119/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 120/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 121/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 39ms/epoch - 1ms/step\n",
      "Epoch 122/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 123/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 39ms/epoch - 1ms/step\n",
      "Epoch 124/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 125/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 39ms/epoch - 1ms/step\n",
      "Epoch 126/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 127/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 128/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 129/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 130/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 131/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 132/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 133/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 134/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 135/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 136/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 137/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 138/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 139/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 140/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 141/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 142/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 143/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 144/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 145/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 146/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 147/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 148/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 149/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 150/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 151/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 152/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 153/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 154/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 155/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 156/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 157/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 158/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 159/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 160/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 161/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 162/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 163/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 164/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 165/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 45ms/epoch - 1ms/step\n",
      "Epoch 166/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 47ms/epoch - 1ms/step\n",
      "Epoch 167/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 45ms/epoch - 1ms/step\n",
      "Epoch 168/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 45ms/epoch - 1ms/step\n",
      "Epoch 169/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 47ms/epoch - 1ms/step\n",
      "Epoch 170/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 171/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 45ms/epoch - 1ms/step\n",
      "Epoch 172/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 46ms/epoch - 1ms/step\n",
      "Epoch 173/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 174/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 175/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 176/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 46ms/epoch - 1ms/step\n",
      "Epoch 177/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 49ms/epoch - 1ms/step\n",
      "Epoch 178/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 48ms/epoch - 1ms/step\n",
      "Epoch 179/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 180/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 181/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 182/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 183/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 184/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 185/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 186/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 187/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 188/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 37ms/epoch - 1ms/step\n",
      "Epoch 189/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 39ms/epoch - 1ms/step\n",
      "Epoch 190/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 35ms/epoch - 970us/step\n",
      "Epoch 191/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 192/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 193/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 194/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 39ms/epoch - 1ms/step\n",
      "Epoch 195/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 196/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 37ms/epoch - 1ms/step\n",
      "Epoch 197/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 198/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 199/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 200/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 201/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 202/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 203/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 204/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 39ms/epoch - 1ms/step\n",
      "Epoch 205/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 39ms/epoch - 1ms/step\n",
      "Epoch 206/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 207/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 208/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 209/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 210/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 39ms/epoch - 1ms/step\n",
      "Epoch 211/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 212/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 213/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 214/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 215/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 216/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 39ms/epoch - 1ms/step\n",
      "Epoch 217/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 218/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 219/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 220/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 221/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 222/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 223/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 224/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 39ms/epoch - 1ms/step\n",
      "Epoch 225/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 226/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 227/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 228/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 39ms/epoch - 1ms/step\n",
      "Epoch 229/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 230/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 231/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 39ms/epoch - 1ms/step\n",
      "Epoch 232/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 233/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 234/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 38ms/epoch - 1ms/step\n",
      "Epoch 235/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 236/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 38ms/epoch - 1ms/step\n",
      "Epoch 237/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 238/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 239/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 240/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 241/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 242/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 243/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 244/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 245/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 246/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 247/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 248/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 249/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 250/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 251/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 252/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 253/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 254/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 255/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 256/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 257/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 258/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 259/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 260/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 261/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 262/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 263/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 264/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 265/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 266/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 267/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 268/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 269/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 270/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 271/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 272/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 273/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 274/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 275/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 276/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 37ms/epoch - 1ms/step\n",
      "Epoch 277/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 278/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 279/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 280/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 281/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 282/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 283/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 284/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 285/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 286/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 287/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 288/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 289/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 290/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 291/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 292/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 293/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 294/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 53ms/epoch - 1ms/step\n",
      "Epoch 295/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 296/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 297/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 298/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 299/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 300/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 301/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 302/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 303/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 304/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 305/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 50ms/epoch - 1ms/step\n",
      "Epoch 306/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 86ms/epoch - 2ms/step\n",
      "Epoch 307/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 308/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 37ms/epoch - 1ms/step\n",
      "Epoch 309/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 39ms/epoch - 1ms/step\n",
      "Epoch 310/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 39ms/epoch - 1ms/step\n",
      "Epoch 311/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 38ms/epoch - 1ms/step\n",
      "Epoch 312/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 313/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 314/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 315/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 39ms/epoch - 1ms/step\n",
      "Epoch 316/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 317/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 318/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 319/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 320/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 321/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 322/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 323/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 324/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 325/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 326/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 327/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 328/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 329/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 330/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 331/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 332/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 333/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 334/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 335/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 336/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 337/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 338/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 339/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 340/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 341/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 342/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 343/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 344/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 345/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 346/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 347/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 348/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 349/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 350/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 351/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 352/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 353/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 354/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 355/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 356/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 357/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 358/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 359/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 360/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 361/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 362/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 363/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 364/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 365/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 366/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 367/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 368/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 369/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 370/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 38ms/epoch - 1ms/step\n",
      "Epoch 371/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 372/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 373/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 374/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 375/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 376/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 377/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 378/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 39ms/epoch - 1ms/step\n",
      "Epoch 379/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 380/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 381/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 382/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 39ms/epoch - 1ms/step\n",
      "Epoch 383/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 384/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 385/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 386/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 387/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 388/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 389/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 390/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 391/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 392/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 393/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 394/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 395/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 396/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 397/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 398/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 399/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 400/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 45ms/epoch - 1ms/step\n",
      "Epoch 401/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 402/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 403/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 404/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 405/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 406/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 407/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 408/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 409/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 410/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 411/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 412/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 413/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 414/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 415/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 416/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 417/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 418/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 419/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 420/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 421/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 422/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 423/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 424/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 425/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 426/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 427/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 428/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 429/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 430/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 431/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 432/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 433/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 434/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 435/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 436/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 437/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 438/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 439/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 440/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 441/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 442/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 45ms/epoch - 1ms/step\n",
      "Epoch 443/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 444/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 445/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 446/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 447/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 448/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 449/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 450/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 451/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 452/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 453/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 454/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 455/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 39ms/epoch - 1ms/step\n",
      "Epoch 456/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 457/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 458/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 459/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 44ms/epoch - 1ms/step\n",
      "Epoch 460/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 461/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 462/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 463/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 464/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 465/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 37ms/epoch - 1ms/step\n",
      "Epoch 466/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 467/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 468/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 469/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 470/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 471/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 472/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 473/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 474/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 475/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 476/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 477/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 478/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 479/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 480/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 481/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 482/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 483/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 484/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 485/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 486/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 39ms/epoch - 1ms/step\n",
      "Epoch 487/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 42ms/epoch - 1ms/step\n",
      "Epoch 488/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 38ms/epoch - 1ms/step\n",
      "Epoch 489/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 490/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 491/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 39ms/epoch - 1ms/step\n",
      "Epoch 492/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 39ms/epoch - 1ms/step\n",
      "Epoch 493/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 494/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 495/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 43ms/epoch - 1ms/step\n",
      "Epoch 496/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 39ms/epoch - 1ms/step\n",
      "Epoch 497/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 498/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n",
      "Epoch 499/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 41ms/epoch - 1ms/step\n",
      "Epoch 500/500\n",
      "36/36 - 0s - loss: nan - mse: nan - 40ms/epoch - 1ms/step\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#------------------------\n",
    "num_inputs = 43\n",
    "model.add(Dense(10,input_shape=(num_inputs,)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "#---------------------\n",
    "model.add(Dense(1))\n",
    "#-------------------------------------------------------------------------------\n",
    "model.compile(loss='mean_squared_error', metrics=['mse'], optimizer='adam')\n",
    "history = model.fit(X_train, y_train, epochs=500, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3622c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "85bb0e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.layers import MaxPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7b20a4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# #------------------------\n",
    "# num_inputs = 43\n",
    "# model.add(Dense(90,input_shape=(num_inputs,)))\n",
    "# #-------------------------------\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# #--------------------------------------\n",
    "# model.add(MaxPooling1D(pool_size=1))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(128,activation='relu'))\n",
    "# model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "56ef97f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "34facdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_house_extracted_wool_[[\n",
    "        'MSSubClass',\n",
    "        'MasVnrArea','BsmtFinSF1','BsmtUnfSF', '2ndFlrSF', 'BsmtFullBath',\n",
    "        'FullBath', 'HalfBath','Fireplaces','WoodDeckSF','OpenPorchSF', \n",
    "        \n",
    "        'House_Age','Garage_Age','Exterior_Avg',\n",
    "    \n",
    "    \n",
    "       'MSZoning_', 'LotFrontage', 'LotArea',\n",
    "       'LotShape_', 'LotConfig_', 'Neighborhood_',\n",
    "       'HouseStyle_', 'OverallQual', 'OverallCond',\n",
    "       'RoofStyle_', 'MasVnrType_', \n",
    "       'ExterQual_', 'Foundation_', 'BsmtQual_',\n",
    "       'BsmtExposure_', 'BsmtFinType1_', \n",
    "       'TotalBsmtSF', 'HeatingQC_',\n",
    "       '1stFlrSF', 'GrLivArea',\n",
    "        \n",
    "       'BedroomAbvGr', 'KitchenQual_', 'TotRmsAbvGrd',\n",
    "        'FireplaceQu_', 'GarageType_',\n",
    "       'GarageFinish_',\n",
    "       'GarageCars', 'GarageArea', \n",
    "        'SaleCondition_']]\n",
    "y = df_house_extracted_wool_['SalePrice']\n",
    "\n",
    "# split X and y \n",
    "# make sure that 'test' here is different from 'test' in df_house_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6039a5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30, 13, 6\n",
    "model.add(Dense(90, input_dim=43, activation='relu'))\n",
    "model.add(Dense(21, activation='relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3d902de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "832f4cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "116/116 [==============================] - 1s 2ms/step - loss: nan\n",
      "Epoch 2/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 3/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 4/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 5/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 6/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 7/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 8/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 9/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 10/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 11/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 12/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 13/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 14/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 15/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 16/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 17/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 18/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 19/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 20/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 21/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 22/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 23/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 24/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 25/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 26/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 27/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 28/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 29/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 30/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 31/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 32/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 33/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 34/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 35/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 36/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 37/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 38/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 39/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 40/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 41/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 42/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 43/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 44/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 45/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 46/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 47/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 48/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 49/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 50/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 51/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 52/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 53/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 54/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 55/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 56/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 57/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 58/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 59/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 60/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 61/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 62/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 63/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 64/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 65/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 66/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 67/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 68/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 69/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 70/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 71/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 72/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 73/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 74/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 75/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 76/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 77/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 78/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 79/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 80/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 81/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 82/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 83/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 84/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 85/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 86/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 87/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 88/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 89/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 90/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 91/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 92/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 93/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 94/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 95/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 96/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 97/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 98/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 99/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 100/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 101/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 102/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 103/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 104/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 105/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 106/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 107/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 108/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 109/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 110/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 111/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 112/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 113/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 114/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 115/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 116/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 117/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 118/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 119/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 120/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 121/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 122/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 123/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 124/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 125/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 126/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 127/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 128/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 129/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 130/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 131/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 132/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 133/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 134/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 135/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 136/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 137/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 138/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 139/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 140/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 141/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 142/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 143/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 144/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 145/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 146/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 147/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 148/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 149/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 150/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 151/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 152/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 153/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 154/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 155/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 156/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 157/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 158/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 159/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 160/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 161/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 162/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 163/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 164/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 165/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 166/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 167/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 168/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 169/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 170/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 171/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 172/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 173/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 174/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 175/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 176/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 177/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 178/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 179/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 180/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 181/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 182/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 183/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 184/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 185/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 186/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 187/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 188/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 189/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 190/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 191/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 192/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 193/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 194/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 195/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 196/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 197/200\n",
      "116/116 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 198/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 199/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 200/200\n",
      "116/116 [==============================] - 0s 2ms/step - loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2476e2a59d0>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train, epochs=200, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "aa4ea652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan]], dtype=float32)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce9be86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a9a070",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMPLE CODE ###\n",
    "x = df_Train_final\n",
    "x = x.drop(['Id'], axis=1)\n",
    "#x = x.drop(['patientid'], axis=1)\n",
    "x = x.drop(['is_train'], axis=1)\n",
    "x = x.drop(['SalePrice'], axis=1)\n",
    "y = df_Train_final['SalePrice']\n",
    "x_pred = df_Test_final\n",
    "x_pred = x_pred.drop(['Id'], axis=1)\n",
    "#x_pred = x_pred.drop(['patientid'], axis=1)\n",
    "x_pred = x_pred.drop(['is_train'], axis=1)\n",
    "x_pred = x_pred.drop(['SalePrice'], axis=1)\n",
    "###############################################################\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(267,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "#---------------------------------------------\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "#----------------------------------\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "#----------------------------\n",
    "model.add(Dense(1))\n",
    "#-------------------------------------------------------------------------------\n",
    "model.compile(loss='mean_squared_error', metrics=['mse'], optimizer='adam')\n",
    "history = model.fit(x, np.ravel(y), epochs=500, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3af0e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76835c76",
   "metadata": {},
   "source": [
    "### nov09.2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc84fde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### rearrangement of the cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "e10c1b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "77b62333",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "2edc9495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07312961,  0.51496564, -0.22803278, ..., -0.76827512,\n",
       "         0.27171931,  1.29315561],\n",
       "       [-0.87350636,  0.51496564,  0.52895299, ...,  1.75623574,\n",
       "        -0.73833325, -0.41321925],\n",
       "       [ 0.07312961,  0.51496564, -0.07663563, ..., -0.76827512,\n",
       "        -0.04288723,  1.29315561],\n",
       "       ...,\n",
       "       [-0.87350636,  0.51496564,  0.78128158, ...,  2.1882829 ,\n",
       "        -0.73833325, -0.94992519],\n",
       "       [ 0.30978861,  0.51496564, -0.17756706, ..., -0.76827512,\n",
       "         0.25516107, -1.32620272],\n",
       "       [-0.87350636,  0.51496564, -0.07663563, ...,  2.33229862,\n",
       "        -0.73833325, -0.41321925]])"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "91d5bcdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.46627249, 0.46627249, 0.46627249, ..., 0.46627249, 0.46627249,\n",
       "       0.46627249])"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "e8032d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5464476 ,  0.51496564,  0.78128158, ..., -0.76827512,\n",
       "        -0.73833325,  1.29315561],\n",
       "       [-0.87350636,  0.51496564,  0.37755583, ..., -0.76827512,\n",
       "        -0.14223666, -0.50364253],\n",
       "       [-0.87350636,  0.51496564,  0.17569296, ..., -0.76827512,\n",
       "        -0.374052  ,  1.29315561],\n",
       "       ...,\n",
       "       [-0.87350636,  0.51496564, -0.2784985 , ...,  0.24830643,\n",
       "        -0.19191138,  1.29315561],\n",
       "       [-0.87350636,  0.51496564, -0.48036137, ...,  0.65493906,\n",
       "        -0.73833325, -1.49246489],\n",
       "       [-0.87350636,  0.51496564, -0.22803278, ...,  2.75587427,\n",
       "         1.64605313, -0.41321925]])"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "e9006a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5464476 ,  0.51496564,  0.32709011, ...,  0.95144201,\n",
       "        -0.73833325, -0.94992519],\n",
       "       [ 2.43971955, -1.80664394, -2.44852437, ..., -0.76827512,\n",
       "        -0.73833325, -0.42780365],\n",
       "       [ 0.78310659,  0.51496564,  0.88221301, ..., -0.76827512,\n",
       "         2.75545513, -0.94992519],\n",
       "       ...,\n",
       "       [ 2.43971955, -2.18735805, -1.99433291, ..., -0.76827512,\n",
       "         3.13629462,  1.29315561],\n",
       "       [-0.16352938, -1.80664394, -0.98501855, ..., -0.76827512,\n",
       "        -0.34093552, -1.32620272],\n",
       "       [-0.87350636,  0.51496564,  0.02681699, ..., -0.76827512,\n",
       "        -0.73833325, -1.46037921]])"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "ea24aab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.46627249,  0.46627249, -2.08393232, ...,  0.46627249,\n",
       "       -2.14156407,  0.46627249])"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "77aee553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249, -2.14156407,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249, -2.08393232, -2.14156407,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249, -2.14156407,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249, -2.08393232, -2.08393232,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249, -2.32886725,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249, -2.34807783,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "       -2.08393232, -2.14156407, -2.34807783,  0.46627249, -2.14156407,\n",
       "       -2.08393232, -2.08393232,  0.46627249,  0.46627249,  0.46627249,\n",
       "       -2.14156407,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "       -2.08393232,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "       -2.08393232,  0.46627249, -2.32886725, -2.08393232, -2.08393232,\n",
       "        0.46627249,  0.46627249,  0.46627249, -2.14156407,  0.46627249,\n",
       "        0.46627249, -2.14156407, -2.32886725,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249, -2.08393232, -2.14156407,  0.46627249,  0.46627249,\n",
       "       -2.08393232,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "       -2.08393232,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "       -2.32886725,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249, -2.14156407,\n",
       "        0.46627249, -2.32886725,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249, -2.32886725, -2.08393232,  0.46627249,  0.46627249,\n",
       "       -2.14156407,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "       -2.14156407,  0.46627249,  0.46627249,  0.46627249, -2.14156407,\n",
       "        0.46627249,  0.46627249, -2.36728841,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "       -2.14156407,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249, -2.14156407,  0.46627249, -2.08393232, -2.14156407,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249, -2.08393232, -2.32886725,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "       -2.08393232,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "       -2.08393232,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "       -2.08393232,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "       -2.32886725,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249, -2.32886725,  0.46627249,  0.46627249,  0.46627249,\n",
       "       -2.14156407,  0.46627249, -2.14156407,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249, -2.32886725,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249, -2.34807783,  0.46627249, -2.14156407,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249, -2.36728841,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249, -2.08393232,\n",
       "        0.46627249,  0.46627249])"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "edaf7019",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#----------------------\n",
    "# 30, 13, 6\n",
    "model.add(Dense(90, input_dim=42, activation='relu'))\n",
    "model.add(Dense(21, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "#-----------------------------------------------------------\n",
    "model.compile(loss='mean_squared_error',optimizer='adam')\n",
    "#---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "d507af3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.9872\n",
      "Epoch 2/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.8080\n",
      "Epoch 3/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.7408\n",
      "Epoch 4/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.6695\n",
      "Epoch 5/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.6262\n",
      "Epoch 6/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.5787\n",
      "Epoch 7/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.5205\n",
      "Epoch 8/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.4838\n",
      "Epoch 9/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.4376\n",
      "Epoch 10/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.4119\n",
      "Epoch 11/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.3762\n",
      "Epoch 12/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.3411\n",
      "Epoch 13/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.3177\n",
      "Epoch 14/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2832\n",
      "Epoch 15/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.2718\n",
      "Epoch 16/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.2438\n",
      "Epoch 17/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.2309\n",
      "Epoch 18/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.2022\n",
      "Epoch 19/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1926\n",
      "Epoch 20/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1670\n",
      "Epoch 21/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1543\n",
      "Epoch 22/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1454\n",
      "Epoch 23/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1379\n",
      "Epoch 24/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1308\n",
      "Epoch 25/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1232\n",
      "Epoch 26/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1155\n",
      "Epoch 27/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1006\n",
      "Epoch 28/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0969\n",
      "Epoch 29/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1042\n",
      "Epoch 30/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0850\n",
      "Epoch 31/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0853\n",
      "Epoch 32/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0847\n",
      "Epoch 33/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0736\n",
      "Epoch 34/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0829\n",
      "Epoch 35/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0779\n",
      "Epoch 36/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0737\n",
      "Epoch 37/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0734\n",
      "Epoch 38/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0663\n",
      "Epoch 39/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0614\n",
      "Epoch 40/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0571\n",
      "Epoch 41/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0608\n",
      "Epoch 42/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0631\n",
      "Epoch 43/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0587\n",
      "Epoch 44/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0610\n",
      "Epoch 45/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0620\n",
      "Epoch 46/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0603\n",
      "Epoch 47/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0632\n",
      "Epoch 48/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0530\n",
      "Epoch 49/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0599\n",
      "Epoch 50/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0539\n",
      "Epoch 51/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0605\n",
      "Epoch 52/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0730\n",
      "Epoch 53/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0766\n",
      "Epoch 54/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0520\n",
      "Epoch 55/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0470\n",
      "Epoch 56/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0486\n",
      "Epoch 57/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0483\n",
      "Epoch 58/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0492\n",
      "Epoch 59/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0414\n",
      "Epoch 60/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0400\n",
      "Epoch 61/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0474\n",
      "Epoch 62/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0501\n",
      "Epoch 63/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0542\n",
      "Epoch 64/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0597\n",
      "Epoch 65/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0572\n",
      "Epoch 66/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0488\n",
      "Epoch 67/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0407\n",
      "Epoch 68/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0384\n",
      "Epoch 69/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0454\n",
      "Epoch 70/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0568\n",
      "Epoch 71/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0487\n",
      "Epoch 72/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0543\n",
      "Epoch 73/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0491\n",
      "Epoch 74/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0401\n",
      "Epoch 75/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0450\n",
      "Epoch 76/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0591\n",
      "Epoch 77/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0556\n",
      "Epoch 78/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0508\n",
      "Epoch 79/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0460\n",
      "Epoch 80/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0308\n",
      "Epoch 81/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0311\n",
      "Epoch 82/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0311\n",
      "Epoch 83/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0389\n",
      "Epoch 84/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0493\n",
      "Epoch 85/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0451\n",
      "Epoch 86/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0397\n",
      "Epoch 87/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0399\n",
      "Epoch 88/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0363\n",
      "Epoch 89/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0403\n",
      "Epoch 90/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0433\n",
      "Epoch 91/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0472\n",
      "Epoch 92/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0547\n",
      "Epoch 93/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0540\n",
      "Epoch 94/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0466\n",
      "Epoch 95/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0392\n",
      "Epoch 96/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0333\n",
      "Epoch 97/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0419\n",
      "Epoch 98/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0301\n",
      "Epoch 99/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0336\n",
      "Epoch 100/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0356\n",
      "Epoch 101/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0423\n",
      "Epoch 102/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0370\n",
      "Epoch 103/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0320\n",
      "Epoch 104/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0384\n",
      "Epoch 105/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0367\n",
      "Epoch 106/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0329\n",
      "Epoch 107/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0296\n",
      "Epoch 108/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0368\n",
      "Epoch 109/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0372\n",
      "Epoch 110/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0337\n",
      "Epoch 111/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0430\n",
      "Epoch 112/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0413\n",
      "Epoch 113/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0407\n",
      "Epoch 114/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0385\n",
      "Epoch 115/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0329\n",
      "Epoch 116/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0364\n",
      "Epoch 117/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0296\n",
      "Epoch 118/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0404\n",
      "Epoch 119/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0374\n",
      "Epoch 120/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0352\n",
      "Epoch 121/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0334\n",
      "Epoch 122/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0336\n",
      "Epoch 123/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0343\n",
      "Epoch 124/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0258\n",
      "Epoch 125/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0344\n",
      "Epoch 126/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0373\n",
      "Epoch 127/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0367\n",
      "Epoch 128/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0317\n",
      "Epoch 129/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0377\n",
      "Epoch 130/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0347\n",
      "Epoch 131/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0318\n",
      "Epoch 132/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0393\n",
      "Epoch 133/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0394\n",
      "Epoch 134/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0308\n",
      "Epoch 135/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0280\n",
      "Epoch 136/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0279\n",
      "Epoch 137/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0326\n",
      "Epoch 138/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0293\n",
      "Epoch 139/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0305\n",
      "Epoch 140/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0295\n",
      "Epoch 141/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0400\n",
      "Epoch 142/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0381\n",
      "Epoch 143/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0356\n",
      "Epoch 144/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0319\n",
      "Epoch 145/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0334\n",
      "Epoch 146/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0270\n",
      "Epoch 147/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0264\n",
      "Epoch 148/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0294\n",
      "Epoch 149/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0352\n",
      "Epoch 150/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0392\n",
      "Epoch 151/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0315\n",
      "Epoch 152/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0286\n",
      "Epoch 153/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0326\n",
      "Epoch 154/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0327\n",
      "Epoch 155/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0246\n",
      "Epoch 156/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0245\n",
      "Epoch 157/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0263\n",
      "Epoch 158/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0248\n",
      "Epoch 159/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0312\n",
      "Epoch 160/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0279\n",
      "Epoch 161/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0246\n",
      "Epoch 162/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0314\n",
      "Epoch 163/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0297\n",
      "Epoch 164/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0275\n",
      "Epoch 165/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0336\n",
      "Epoch 166/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0391\n",
      "Epoch 167/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0341\n",
      "Epoch 168/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0290\n",
      "Epoch 169/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0242\n",
      "Epoch 170/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0228\n",
      "Epoch 171/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0253\n",
      "Epoch 172/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0242\n",
      "Epoch 173/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0207\n",
      "Epoch 174/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0196\n",
      "Epoch 175/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0233\n",
      "Epoch 176/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0237\n",
      "Epoch 177/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0237\n",
      "Epoch 178/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0322\n",
      "Epoch 179/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0345\n",
      "Epoch 180/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0326\n",
      "Epoch 181/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0322\n",
      "Epoch 182/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0276\n",
      "Epoch 183/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0266\n",
      "Epoch 184/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0220\n",
      "Epoch 185/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0249\n",
      "Epoch 186/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0261\n",
      "Epoch 187/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0237\n",
      "Epoch 188/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0223\n",
      "Epoch 189/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0268\n",
      "Epoch 190/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0353\n",
      "Epoch 191/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0503\n",
      "Epoch 192/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0385\n",
      "Epoch 193/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0311\n",
      "Epoch 194/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0303\n",
      "Epoch 195/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0254\n",
      "Epoch 196/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0214\n",
      "Epoch 197/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0178\n",
      "Epoch 198/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0211\n",
      "Epoch 199/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0215\n",
      "Epoch 200/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0201\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2475ab039a0>"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train, epochs=200, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "e490ce86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 1.0413\n",
      "Epoch 2/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.8621\n",
      "Epoch 3/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.8043\n",
      "Epoch 4/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.7428\n",
      "Epoch 5/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.6950\n",
      "Epoch 6/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.6530\n",
      "Epoch 7/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.6049\n",
      "Epoch 8/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.5594\n",
      "Epoch 9/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.5211\n",
      "Epoch 10/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.4853\n",
      "Epoch 11/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.4614\n",
      "Epoch 12/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.4198\n",
      "Epoch 13/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.3867\n",
      "Epoch 14/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.3496\n",
      "Epoch 15/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.3357\n",
      "Epoch 16/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.3087\n",
      "Epoch 17/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.2870\n",
      "Epoch 18/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.2637\n",
      "Epoch 19/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.2528\n",
      "Epoch 20/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.2332\n",
      "Epoch 21/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.2072\n",
      "Epoch 22/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1968\n",
      "Epoch 23/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1834\n",
      "Epoch 24/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1859\n",
      "Epoch 25/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1652\n",
      "Epoch 26/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1484\n",
      "Epoch 27/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1458\n",
      "Epoch 28/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1309\n",
      "Epoch 29/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1335\n",
      "Epoch 30/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1194\n",
      "Epoch 31/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1250\n",
      "Epoch 32/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1040\n",
      "Epoch 33/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1017\n",
      "Epoch 34/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0987\n",
      "Epoch 35/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0971\n",
      "Epoch 36/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0870\n",
      "Epoch 37/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0815\n",
      "Epoch 38/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0878\n",
      "Epoch 39/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0804\n",
      "Epoch 40/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0809\n",
      "Epoch 41/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0737\n",
      "Epoch 42/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0785\n",
      "Epoch 43/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0713\n",
      "Epoch 44/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0779\n",
      "Epoch 45/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0729\n",
      "Epoch 46/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0640\n",
      "Epoch 47/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0655\n",
      "Epoch 48/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0634\n",
      "Epoch 49/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0695\n",
      "Epoch 50/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0605\n",
      "Epoch 51/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0638\n",
      "Epoch 52/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0584\n",
      "Epoch 53/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0622\n",
      "Epoch 54/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0667\n",
      "Epoch 55/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0630\n",
      "Epoch 56/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0542\n",
      "Epoch 57/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0537\n",
      "Epoch 58/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0605\n",
      "Epoch 59/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0585\n",
      "Epoch 60/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0605\n",
      "Epoch 61/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0521\n",
      "Epoch 62/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0475\n",
      "Epoch 63/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0518\n",
      "Epoch 64/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0502\n",
      "Epoch 65/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0469\n",
      "Epoch 66/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0467\n",
      "Epoch 67/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0472\n",
      "Epoch 68/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0503\n",
      "Epoch 69/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0466\n",
      "Epoch 70/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0385\n",
      "Epoch 71/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0322\n",
      "Epoch 72/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0416\n",
      "Epoch 73/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0436\n",
      "Epoch 74/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0484\n",
      "Epoch 75/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0596\n",
      "Epoch 76/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0547\n",
      "Epoch 77/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0420\n",
      "Epoch 78/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0427\n",
      "Epoch 79/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0399\n",
      "Epoch 80/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0363\n",
      "Epoch 81/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0381\n",
      "Epoch 82/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0409\n",
      "Epoch 83/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0460\n",
      "Epoch 84/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0511\n",
      "Epoch 85/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0567\n",
      "Epoch 86/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0424\n",
      "Epoch 87/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0405\n",
      "Epoch 88/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0393\n",
      "Epoch 89/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0368\n",
      "Epoch 90/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0324\n",
      "Epoch 91/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0497\n",
      "Epoch 92/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0342\n",
      "Epoch 93/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0307\n",
      "Epoch 94/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0390\n",
      "Epoch 95/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0371\n",
      "Epoch 96/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0310\n",
      "Epoch 97/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0368\n",
      "Epoch 98/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0455\n",
      "Epoch 99/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0425\n",
      "Epoch 100/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0453\n",
      "Epoch 101/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0363\n",
      "Epoch 102/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0394\n",
      "Epoch 103/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0362\n",
      "Epoch 104/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0434\n",
      "Epoch 105/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0353\n",
      "Epoch 106/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0369\n",
      "Epoch 107/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0303\n",
      "Epoch 108/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0296\n",
      "Epoch 109/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0290\n",
      "Epoch 110/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0282\n",
      "Epoch 111/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0367\n",
      "Epoch 112/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0376\n",
      "Epoch 113/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0470\n",
      "Epoch 114/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0345\n",
      "Epoch 115/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0319\n",
      "Epoch 116/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0343\n",
      "Epoch 117/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0289\n",
      "Epoch 118/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0284\n",
      "Epoch 119/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0295\n",
      "Epoch 120/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0371\n",
      "Epoch 121/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0342\n",
      "Epoch 122/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0386\n",
      "Epoch 123/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0472\n",
      "Epoch 124/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0275\n",
      "Epoch 125/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0267\n",
      "Epoch 126/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0287\n",
      "Epoch 127/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0288\n",
      "Epoch 128/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0259\n",
      "Epoch 129/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0356\n",
      "Epoch 130/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0329\n",
      "Epoch 131/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0225\n",
      "Epoch 132/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0311\n",
      "Epoch 133/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0314\n",
      "Epoch 134/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0305\n",
      "Epoch 135/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0321\n",
      "Epoch 136/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0287\n",
      "Epoch 137/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0305\n",
      "Epoch 138/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0270\n",
      "Epoch 139/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0242\n",
      "Epoch 140/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0293\n",
      "Epoch 141/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0314\n",
      "Epoch 142/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0289\n",
      "Epoch 143/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 144/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0321\n",
      "Epoch 145/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0284\n",
      "Epoch 146/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0290\n",
      "Epoch 147/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0240\n",
      "Epoch 148/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0215\n",
      "Epoch 149/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0243\n",
      "Epoch 150/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0281\n",
      "Epoch 151/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0269\n",
      "Epoch 152/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0399\n",
      "Epoch 153/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0496\n",
      "Epoch 154/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0360\n",
      "Epoch 155/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0273\n",
      "Epoch 156/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0190\n",
      "Epoch 157/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0203\n",
      "Epoch 158/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0184\n",
      "Epoch 159/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0224\n",
      "Epoch 160/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0215\n",
      "Epoch 161/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0204\n",
      "Epoch 162/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0197\n",
      "Epoch 163/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0225\n",
      "Epoch 164/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0267\n",
      "Epoch 165/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0226\n",
      "Epoch 166/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0281\n",
      "Epoch 167/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0287\n",
      "Epoch 168/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0220\n",
      "Epoch 169/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0214\n",
      "Epoch 170/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0225\n",
      "Epoch 171/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0291\n",
      "Epoch 172/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0281\n",
      "Epoch 173/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0349\n",
      "Epoch 174/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0273\n",
      "Epoch 175/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0231\n",
      "Epoch 176/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0221\n",
      "Epoch 177/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0193\n",
      "Epoch 178/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0237\n",
      "Epoch 179/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0308\n",
      "Epoch 180/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0255\n",
      "Epoch 181/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0248\n",
      "Epoch 182/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0186\n",
      "Epoch 183/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0220\n",
      "Epoch 184/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0190\n",
      "Epoch 185/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0246\n",
      "Epoch 186/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0283\n",
      "Epoch 187/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0301\n",
      "Epoch 188/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0263\n",
      "Epoch 189/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0236\n",
      "Epoch 190/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0287\n",
      "Epoch 191/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0257\n",
      "Epoch 192/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0335\n",
      "Epoch 193/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0255\n",
      "Epoch 194/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0207\n",
      "Epoch 195/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0174\n",
      "Epoch 196/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0201\n",
      "Epoch 197/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0176\n",
      "Epoch 198/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0189\n",
      "Epoch 199/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0227\n",
      "Epoch 200/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0216\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train, epochs=200, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "9aef7b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.0322\n",
      "Epoch 1: loss improved from inf to 0.03249, saving model to C:/Users/thsong/dl_model\\01-0.0325.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0325\n",
      "Epoch 2/200\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.0474\n",
      "Epoch 2: loss did not improve from 0.03249\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0465\n",
      "Epoch 3/200\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.0264\n",
      "Epoch 3: loss improved from 0.03249 to 0.02630, saving model to C:/Users/thsong/dl_model\\03-0.0263.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0263\n",
      "Epoch 4/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0183\n",
      "Epoch 4: loss improved from 0.02630 to 0.01824, saving model to C:/Users/thsong/dl_model\\04-0.0182.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0182\n",
      "Epoch 5/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0127\n",
      "Epoch 5: loss improved from 0.01824 to 0.01258, saving model to C:/Users/thsong/dl_model\\05-0.0126.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0126\n",
      "Epoch 6/200\n",
      " 99/115 [========================>.....] - ETA: 0s - loss: 0.0126\n",
      "Epoch 6: loss improved from 0.01258 to 0.01201, saving model to C:/Users/thsong/dl_model\\06-0.0120.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0120\n",
      "Epoch 7/200\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.0109\n",
      "Epoch 7: loss improved from 0.01201 to 0.01141, saving model to C:/Users/thsong/dl_model\\07-0.0114.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0114\n",
      "Epoch 8/200\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.0144\n",
      "Epoch 8: loss did not improve from 0.01141\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0141\n",
      "Epoch 9/200\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.0114\n",
      "Epoch 9: loss did not improve from 0.01141\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0117\n",
      "Epoch 10/200\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.0136\n",
      "Epoch 10: loss did not improve from 0.01141\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0135\n",
      "Epoch 11/200\n",
      " 99/115 [========================>.....] - ETA: 0s - loss: 0.0276\n",
      "Epoch 11: loss did not improve from 0.01141\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0263\n",
      "Epoch 12/200\n",
      "101/115 [=========================>....] - ETA: 0s - loss: 0.0241\n",
      "Epoch 12: loss did not improve from 0.01141\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0240\n",
      "Epoch 13/200\n",
      "103/115 [=========================>....] - ETA: 0s - loss: 0.0250\n",
      "Epoch 13: loss did not improve from 0.01141\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0242\n",
      "Epoch 14/200\n",
      "100/115 [=========================>....] - ETA: 0s - loss: 0.0239\n",
      "Epoch 14: loss did not improve from 0.01141\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0234\n",
      "Epoch 15/200\n",
      "102/115 [=========================>....] - ETA: 0s - loss: 0.0205\n",
      "Epoch 15: loss did not improve from 0.01141\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0204\n",
      "Epoch 16/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0204\n",
      "Epoch 16: loss did not improve from 0.01141\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0202\n",
      "Epoch 17/200\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.0196\n",
      "Epoch 17: loss did not improve from 0.01141\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0197\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train, epochs=200, batch_size=10, callbacks=[early_stopping_callback,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "0a1a7d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " 77/115 [===================>..........] - ETA: 0s - loss: 0.0239WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0268\n",
      "Epoch 2/200\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.0237WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0236\n",
      "Epoch 3/200\n",
      " 77/115 [===================>..........] - ETA: 0s - loss: 0.0195WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0178\n",
      "Epoch 4/200\n",
      " 79/115 [===================>..........] - ETA: 0s - loss: 0.0202WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0192\n",
      "Epoch 5/200\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.0164WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0161\n",
      "Epoch 6/200\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.0138WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0135\n",
      "Epoch 7/200\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.0128WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0130\n",
      "Epoch 8/200\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.0176WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0175\n",
      "Epoch 9/200\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.0159WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0163\n",
      "Epoch 10/200\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.0149WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0147\n",
      "Epoch 11/200\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.0258WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0258\n",
      "Epoch 12/200\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0299WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0299\n",
      "Epoch 13/200\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.0258WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0254\n",
      "Epoch 14/200\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.0234WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0232\n",
      "Epoch 15/200\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.0189WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0189\n",
      "Epoch 16/200\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.0169WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0169\n",
      "Epoch 17/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0162WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0164\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train, epochs=200, batch_size=10, callbacks=[early_stopping_callback,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "dc7e1539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " 79/115 [===================>..........] - ETA: 0s - loss: 0.0087WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0125\n",
      "Epoch 2/200\n",
      " 77/115 [===================>..........] - ETA: 0s - loss: 0.0155WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0145\n",
      "Epoch 3/200\n",
      " 78/115 [===================>..........] - ETA: 0s - loss: 0.0163WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0170\n",
      "Epoch 4/200\n",
      " 88/115 [=====================>........] - ETA: 0s - loss: 0.0223WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0215\n",
      "Epoch 5/200\n",
      " 81/115 [====================>.........] - ETA: 0s - loss: 0.0213WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0184\n",
      "Epoch 6/200\n",
      " 78/115 [===================>..........] - ETA: 0s - loss: 0.0146WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0148\n",
      "Epoch 7/200\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.0103WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0104\n",
      "Epoch 8/200\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.0103WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0101\n",
      "Epoch 9/200\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.0073  WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0073\n",
      "Epoch 10/200\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0066WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0066\n",
      "Epoch 11/200\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.0103WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0103\n",
      "Epoch 12/200\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.0085WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0085\n",
      "Epoch 13/200\n",
      "102/115 [=========================>....] - ETA: 0s - loss: 0.0088WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 14/200\n",
      "103/115 [=========================>....] - ETA: 0s - loss: 0.0108WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0102\n",
      "Epoch 15/200\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0095WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0095\n",
      "Epoch 16/200\n",
      " 77/115 [===================>..........] - ETA: 0s - loss: 0.0071WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0100\n",
      "Epoch 17/200\n",
      " 77/115 [===================>..........] - ETA: 0s - loss: 0.0141WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0118\n",
      "Epoch 18/200\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.0163WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0160\n",
      "Epoch 19/200\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.0154WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0155\n",
      "Epoch 20/200\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.0152WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0150\n",
      "Epoch 21/200\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.0112WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0111\n",
      "Epoch 22/200\n",
      "105/115 [==========================>...] - ETA: 0s - loss: 0.0122WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0119\n",
      "Epoch 23/200\n",
      " 77/115 [===================>..........] - ETA: 0s - loss: 0.0113WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0104\n",
      "Epoch 24/200\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.0118WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0117\n",
      "Epoch 25/200\n",
      " 80/115 [===================>..........] - ETA: 0s - loss: 0.0141WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0144\n",
      "Epoch 26/200\n",
      " 82/115 [====================>.........] - ETA: 0s - loss: 0.0183WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0151\n",
      "Epoch 27/200\n",
      " 78/115 [===================>..........] - ETA: 0s - loss: 0.0164WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0141\n",
      "Epoch 28/200\n",
      " 80/115 [===================>..........] - ETA: 0s - loss: 0.0136WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0117\n",
      "Epoch 29/200\n",
      " 77/115 [===================>..........] - ETA: 0s - loss: 0.0126WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0117\n",
      "Epoch 30/200\n",
      " 78/115 [===================>..........] - ETA: 0s - loss: 0.0106WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0099\n",
      "Epoch 31/200\n",
      " 79/115 [===================>..........] - ETA: 0s - loss: 0.0078WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0114\n",
      "Epoch 32/200\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0136WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0136\n",
      "Epoch 33/200\n",
      " 77/115 [===================>..........] - ETA: 0s - loss: 0.0126WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0112\n",
      "Epoch 34/200\n",
      " 81/115 [====================>.........] - ETA: 0s - loss: 0.0140WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0120\n",
      "Epoch 35/200\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.0117WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0116\n",
      "Epoch 36/200\n",
      " 79/115 [===================>..........] - ETA: 0s - loss: 0.0126WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0106\n",
      "Epoch 37/200\n",
      " 78/115 [===================>..........] - ETA: 0s - loss: 0.0126WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0112\n",
      "Epoch 38/200\n",
      " 79/115 [===================>..........] - ETA: 0s - loss: 0.0093WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0117\n",
      "Epoch 39/200\n",
      " 82/115 [====================>.........] - ETA: 0s - loss: 0.0152WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0128\n",
      "Epoch 40/200\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0105WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0105\n",
      "Epoch 41/200\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.0101WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0114\n",
      "Epoch 42/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0108WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0107\n",
      "Epoch 43/200\n",
      " 78/115 [===================>..........] - ETA: 0s - loss: 0.0143WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0129\n",
      "Epoch 44/200\n",
      "106/115 [==========================>...] - ETA: 0s - loss: 0.0138WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0134\n",
      "Epoch 45/200\n",
      " 80/115 [===================>..........] - ETA: 0s - loss: 0.0122WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0110\n",
      "Epoch 46/200\n",
      " 82/115 [====================>.........] - ETA: 0s - loss: 0.0087WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0103\n",
      "Epoch 47/200\n",
      " 83/115 [====================>.........] - ETA: 0s - loss: 0.0110WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0097\n",
      "Epoch 48/200\n",
      " 79/115 [===================>..........] - ETA: 0s - loss: 0.0100WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0104\n",
      "Epoch 49/200\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.0116WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0115\n",
      "Epoch 50/200\n",
      "105/115 [==========================>...] - ETA: 0s - loss: 0.0101WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0098\n",
      "Epoch 51/200\n",
      "101/115 [=========================>....] - ETA: 0s - loss: 0.0105WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0103\n",
      "Epoch 52/200\n",
      "105/115 [==========================>...] - ETA: 0s - loss: 0.0108WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0105\n",
      "Epoch 53/200\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.0102WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0101\n",
      "Epoch 54/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0149WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0151\n",
      "Epoch 55/200\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0160WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0160\n",
      "Epoch 56/200\n",
      " 88/115 [=====================>........] - ETA: 0s - loss: 0.0142WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0149\n",
      "Epoch 57/200\n",
      " 81/115 [====================>.........] - ETA: 0s - loss: 0.0153WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0155\n",
      "Epoch 58/200\n",
      " 82/115 [====================>.........] - ETA: 0s - loss: 0.0153WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0146\n",
      "Epoch 59/200\n",
      " 82/115 [====================>.........] - ETA: 0s - loss: 0.0112WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0118\n",
      "Epoch 60/200\n",
      " 82/115 [====================>.........] - ETA: 0s - loss: 0.0074WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0105\n",
      "Epoch 61/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0083WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0084\n",
      "Epoch 62/200\n",
      " 77/115 [===================>..........] - ETA: 0s - loss: 0.0091WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0090\n",
      "Epoch 63/200\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0098WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0098\n",
      "Epoch 64/200\n",
      " 77/115 [===================>..........] - ETA: 0s - loss: 0.0100WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0081\n",
      "Epoch 65/200\n",
      " 80/115 [===================>..........] - ETA: 0s - loss: 0.0060WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0076\n",
      "Epoch 66/200\n",
      " 81/115 [====================>.........] - ETA: 0s - loss: 0.0086WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0082\n",
      "Epoch 67/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0116WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0115\n",
      "Epoch 68/200\n",
      " 78/115 [===================>..........] - ETA: 0s - loss: 0.0097WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0126\n",
      "Epoch 69/200\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0143WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0143\n",
      "Epoch 70/200\n",
      " 80/115 [===================>..........] - ETA: 0s - loss: 0.0166WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0140\n",
      "Epoch 71/200\n",
      " 79/115 [===================>..........] - ETA: 0s - loss: 0.0129WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0114\n",
      "Epoch 72/200\n",
      " 78/115 [===================>..........] - ETA: 0s - loss: 0.0096WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0125\n",
      "Epoch 73/200\n",
      " 79/115 [===================>..........] - ETA: 0s - loss: 0.0135WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0116\n",
      "Epoch 74/200\n",
      " 79/115 [===================>..........] - ETA: 0s - loss: 0.0107WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0101\n",
      "Epoch 75/200\n",
      " 81/115 [====================>.........] - ETA: 0s - loss: 0.0112WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0103\n",
      "Epoch 76/200\n",
      " 75/115 [==================>...........] - ETA: 0s - loss: 0.0130WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0105\n",
      "Epoch 77/200\n",
      " 77/115 [===================>..........] - ETA: 0s - loss: 0.0114WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0102\n",
      "Epoch 78/200\n",
      " 77/115 [===================>..........] - ETA: 0s - loss: 0.0140WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0114\n",
      "Epoch 79/200\n",
      " 80/115 [===================>..........] - ETA: 0s - loss: 0.0114WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0104\n",
      "Epoch 80/200\n",
      " 77/115 [===================>..........] - ETA: 0s - loss: 0.0070   WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0104\n",
      "Epoch 81/200\n",
      " 81/115 [====================>.........] - ETA: 0s - loss: 0.0106WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0100\n",
      "Epoch 82/200\n",
      " 80/115 [===================>..........] - ETA: 0s - loss: 0.0105WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0093\n",
      "Epoch 83/200\n",
      " 81/115 [====================>.........] - ETA: 0s - loss: 0.0087WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0105\n",
      "Epoch 84/200\n",
      " 80/115 [===================>..........] - ETA: 0s - loss: 0.0149WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0130\n",
      "Epoch 85/200\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0143WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0143\n",
      "Epoch 86/200\n",
      " 78/115 [===================>..........] - ETA: 0s - loss: 0.0071WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0105\n",
      "Epoch 87/200\n",
      " 80/115 [===================>..........] - ETA: 0s - loss: 0.0126WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0102\n",
      "Epoch 88/200\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.0094WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0093\n",
      "Epoch 89/200\n",
      "107/115 [==========================>...] - ETA: 0s - loss: 0.0112WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0112\n",
      "Epoch 90/200\n",
      "106/115 [==========================>...] - ETA: 0s - loss: 0.0096WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0134\n",
      "Epoch 91/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0137WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0136\n",
      "Epoch 92/200\n",
      "102/115 [=========================>....] - ETA: 0s - loss: 0.0124WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0121\n",
      "Epoch 93/200\n",
      " 76/115 [==================>...........] - ETA: 0s - loss: 0.0130WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0124\n",
      "Epoch 94/200\n",
      "102/115 [=========================>....] - ETA: 0s - loss: 0.0109WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0103\n",
      "Epoch 95/200\n",
      " 78/115 [===================>..........] - ETA: 0s - loss: 0.0137WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0114\n",
      "Epoch 96/200\n",
      "106/115 [==========================>...] - ETA: 0s - loss: 0.0100WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0097\n",
      "Epoch 97/200\n",
      " 78/115 [===================>..........] - ETA: 0s - loss: 0.0130WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0104\n",
      "Epoch 98/200\n",
      "101/115 [=========================>....] - ETA: 0s - loss: 0.0088WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0087\n",
      "Epoch 99/200\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.0103WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0102\n",
      "Epoch 100/200\n",
      "106/115 [==========================>...] - ETA: 0s - loss: 0.0092WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0090\n",
      "Epoch 101/200\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.0108WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0107\n",
      "Epoch 102/200\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.0087WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0086\n",
      "Epoch 103/200\n",
      "107/115 [==========================>...] - ETA: 0s - loss: 0.0102WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0103\n",
      "Epoch 104/200\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.0126WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0127\n",
      "Epoch 105/200\n",
      "104/115 [==========================>...] - ETA: 0s - loss: 0.0130WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0130\n",
      "Epoch 106/200\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.0134WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0134\n",
      "Epoch 107/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0126WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0125\n",
      "Epoch 108/200\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.0103WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0101\n",
      "Epoch 109/200\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.0093WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0092\n",
      "Epoch 110/200\n",
      "102/115 [=========================>....] - ETA: 0s - loss: 0.0068WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 111/200\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.0108WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0107\n",
      "Epoch 112/200\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.0093WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0092\n",
      "Epoch 113/200\n",
      "105/115 [==========================>...] - ETA: 0s - loss: 0.0107WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0103\n",
      "Epoch 114/200\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.0063WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0091\n",
      "Epoch 115/200\n",
      " 79/115 [===================>..........] - ETA: 0s - loss: 0.0087WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0088\n",
      "Epoch 116/200\n",
      "107/115 [==========================>...] - ETA: 0s - loss: 0.0092WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0093\n",
      "Epoch 117/200\n",
      " 79/115 [===================>..........] - ETA: 0s - loss: 0.0114WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0101\n",
      "Epoch 118/200\n",
      " 78/115 [===================>..........] - ETA: 0s - loss: 0.0102WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0081\n",
      "Epoch 119/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0102WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0102\n",
      "Epoch 120/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/115 [============================>.] - ETA: 0s - loss: 0.0116WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0115\n",
      "Epoch 121/200\n",
      " 81/115 [====================>.........] - ETA: 0s - loss: 0.0159WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0137\n",
      "Epoch 122/200\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.0099WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0097\n",
      "Epoch 123/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0116WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0115\n",
      "Epoch 124/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0100WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0100\n",
      "Epoch 125/200\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.0092WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0094\n",
      "Epoch 126/200\n",
      " 75/115 [==================>...........] - ETA: 0s - loss: 0.0124WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0126\n",
      "Epoch 127/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0139WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0139\n",
      "Epoch 128/200\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.0110WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0108\n",
      "Epoch 129/200\n",
      " 80/115 [===================>..........] - ETA: 0s - loss: 0.0153WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0151\n",
      "Epoch 130/200\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0110WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0110\n",
      "Epoch 131/200\n",
      "100/115 [=========================>....] - ETA: 0s - loss: 0.0058WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 132/200\n",
      " 76/115 [==================>...........] - ETA: 0s - loss: 0.0136WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0115\n",
      "Epoch 133/200\n",
      " 80/115 [===================>..........] - ETA: 0s - loss: 0.0072WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0112\n",
      "Epoch 134/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0109WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0106\n",
      "Epoch 135/200\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.0085WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0085\n",
      "Epoch 136/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0059  WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0075\n",
      "Epoch 137/200\n",
      " 80/115 [===================>..........] - ETA: 0s - loss: 0.0081WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0095\n",
      "Epoch 138/200\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.0079WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0077\n",
      "Epoch 139/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0090WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0090\n",
      "Epoch 140/200\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.0085WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0083\n",
      "Epoch 141/200\n",
      " 77/115 [===================>..........] - ETA: 0s - loss: 0.0090WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0094\n",
      "Epoch 142/200\n",
      "106/115 [==========================>...] - ETA: 0s - loss: 0.0096WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0093\n",
      "Epoch 143/200\n",
      "106/115 [==========================>...] - ETA: 0s - loss: 0.0089WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0091\n",
      "Epoch 144/200\n",
      "100/115 [=========================>....] - ETA: 0s - loss: 0.0079WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0099\n",
      "Epoch 145/200\n",
      "102/115 [=========================>....] - ETA: 0s - loss: 0.0123WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 146/200\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.0137WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0136\n",
      "Epoch 147/200\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.0109WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0107\n",
      "Epoch 148/200\n",
      "104/115 [==========================>...] - ETA: 0s - loss: 0.0086WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0085\n",
      "Epoch 149/200\n",
      " 96/115 [========================>.....] - ETA: 0s - loss: 0.0100WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0091\n",
      "Epoch 150/200\n",
      "100/115 [=========================>....] - ETA: 0s - loss: 0.0103WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0095\n",
      "Epoch 151/200\n",
      "102/115 [=========================>....] - ETA: 0s - loss: 0.0092WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0086\n",
      "Epoch 152/200\n",
      "107/115 [==========================>...] - ETA: 0s - loss: 0.0088WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0085\n",
      "Epoch 153/200\n",
      "102/115 [=========================>....] - ETA: 0s - loss: 0.0097WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0094\n",
      "Epoch 154/200\n",
      " 83/115 [====================>.........] - ETA: 0s - loss: 0.0104WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0090\n",
      "Epoch 155/200\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.0080WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0079\n",
      "Epoch 156/200\n",
      " 80/115 [===================>..........] - ETA: 0s - loss: 0.0071WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0068\n",
      "Epoch 157/200\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.0082WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0082\n",
      "Epoch 158/200\n",
      " 78/115 [===================>..........] - ETA: 0s - loss: 0.0095WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0088\n",
      "Epoch 159/200\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.0112WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0113\n",
      "Epoch 160/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0119WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0116\n",
      "Epoch 161/200\n",
      " 79/115 [===================>..........] - ETA: 0s - loss: 0.0071   WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0096\n",
      "Epoch 162/200\n",
      "106/115 [==========================>...] - ETA: 0s - loss: 0.0099WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0104\n",
      "Epoch 163/200\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.0116WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0115\n",
      "Epoch 164/200\n",
      "104/115 [==========================>...] - ETA: 0s - loss: 0.0124WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0119\n",
      "Epoch 165/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0124WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0122\n",
      "Epoch 166/200\n",
      "106/115 [==========================>...] - ETA: 0s - loss: 0.0086WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0088\n",
      "Epoch 167/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0073WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0073\n",
      "Epoch 168/200\n",
      " 82/115 [====================>.........] - ETA: 0s - loss: 0.0073WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0066\n",
      "Epoch 169/200\n",
      " 85/115 [=====================>........] - ETA: 0s - loss: 0.0051WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0079\n",
      "Epoch 170/200\n",
      " 83/115 [====================>.........] - ETA: 0s - loss: 0.0092WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0085\n",
      "Epoch 171/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0120WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0114\n",
      "Epoch 172/200\n",
      "104/115 [==========================>...] - ETA: 0s - loss: 0.0093WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0088\n",
      "Epoch 173/200\n",
      " 99/115 [========================>.....] - ETA: 0s - loss: 0.0096WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0089\n",
      "Epoch 174/200\n",
      "105/115 [==========================>...] - ETA: 0s - loss: 0.0084WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0088\n",
      "Epoch 175/200\n",
      " 75/115 [==================>...........] - ETA: 0s - loss: 0.0104WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0093\n",
      "Epoch 176/200\n",
      " 87/115 [=====================>........] - ETA: 0s - loss: 0.0072WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0094\n",
      "Epoch 177/200\n",
      " 78/115 [===================>..........] - ETA: 0s - loss: 0.0065WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0098\n",
      "Epoch 178/200\n",
      " 82/115 [====================>.........] - ETA: 0s - loss: 0.0145WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0131\n",
      "Epoch 179/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0145WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0142\n",
      "Epoch 180/200\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.0116WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0115\n",
      "Epoch 181/200\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.0083  WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0082\n",
      "Epoch 182/200\n",
      " 82/115 [====================>.........] - ETA: 0s - loss: 0.0075WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0064\n",
      "Epoch 183/200\n",
      " 78/115 [===================>..........] - ETA: 0s - loss: 0.0103WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0083\n",
      "Epoch 184/200\n",
      " 79/115 [===================>..........] - ETA: 0s - loss: 0.0082WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0078\n",
      "Epoch 185/200\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.0082WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0081\n",
      "Epoch 186/200\n",
      " 95/115 [=======================>......] - ETA: 0s - loss: 0.0073WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0065\n",
      "Epoch 187/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0080WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0079\n",
      "Epoch 188/200\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.0069WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0067\n",
      "Epoch 189/200\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.0099WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0098\n",
      "Epoch 190/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0123WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0121\n",
      "Epoch 191/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/115 [===========================>..] - ETA: 0s - loss: 0.0146WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0145\n",
      "Epoch 192/200\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.0167WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0166\n",
      "Epoch 193/200\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.0154WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0152\n",
      "Epoch 194/200\n",
      "107/115 [==========================>...] - ETA: 0s - loss: 0.0091WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0092\n",
      "Epoch 195/200\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0091WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0091\n",
      "Epoch 196/200\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.0076WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0075\n",
      "Epoch 197/200\n",
      "105/115 [==========================>...] - ETA: 0s - loss: 0.0030WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0057\n",
      "Epoch 198/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0067WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0065\n",
      "Epoch 199/200\n",
      "100/115 [=========================>....] - ETA: 0s - loss: 0.0060WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0055\n",
      "Epoch 200/200\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.0060WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0059\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train, epochs=200, batch_size=10, callbacks=[early_stopping_callback,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "c0fd37f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " 76/115 [==================>...........] - ETA: 0s - loss: 0.0026\n",
      "Epoch 1: val_loss improved from inf to 1.47489, saving model to C:/Users/thsong/dl_model\\1-1.4748884439468384.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 1.4749\n",
      "Epoch 2/200\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.0072  \n",
      "Epoch 2: val_loss did not improve from 1.47489\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0071 - val_loss: 1.4759\n",
      "Epoch 3/200\n",
      " 77/115 [===================>..........] - ETA: 0s - loss: 0.0059\n",
      "Epoch 3: val_loss did not improve from 1.47489\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0056 - val_loss: 1.4959\n",
      "Epoch 4/200\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.0074\n",
      "Epoch 4: val_loss did not improve from 1.47489\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0074 - val_loss: 1.4823\n",
      "Epoch 5/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0049\n",
      "Epoch 5: val_loss did not improve from 1.47489\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 1.4996\n",
      "Epoch 6/200\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.0057\n",
      "Epoch 6: val_loss improved from 1.47489 to 1.46815, saving model to C:/Users/thsong/dl_model\\6-1.4681525230407715.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0056 - val_loss: 1.4682\n",
      "Epoch 7/200\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.0051  \n",
      "Epoch 7: val_loss did not improve from 1.46815\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0052 - val_loss: 1.5018\n",
      "Epoch 8/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0061\n",
      "Epoch 8: val_loss did not improve from 1.46815\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0060 - val_loss: 1.4960\n",
      "Epoch 9/200\n",
      "105/115 [==========================>...] - ETA: 0s - loss: 0.0053\n",
      "Epoch 9: val_loss did not improve from 1.46815\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 1.4856\n",
      "Epoch 10/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0049\n",
      "Epoch 10: val_loss improved from 1.46815 to 1.46489, saving model to C:/Users/thsong/dl_model\\10-1.4648925065994263.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0048 - val_loss: 1.4649\n",
      "Epoch 11/200\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.0066  \n",
      "Epoch 11: val_loss did not improve from 1.46489\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0065 - val_loss: 1.4654\n",
      "Epoch 12/200\n",
      " 76/115 [==================>...........] - ETA: 0s - loss: 0.0060   \n",
      "Epoch 12: val_loss did not improve from 1.46489\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0052 - val_loss: 1.5047\n",
      "Epoch 13/200\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.0040  \n",
      "Epoch 13: val_loss did not improve from 1.46489\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 1.5055\n",
      "Epoch 14/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0061\n",
      "Epoch 14: val_loss did not improve from 1.46489\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 1.4808\n",
      "Epoch 15/200\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.0064\n",
      "Epoch 15: val_loss did not improve from 1.46489\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0063 - val_loss: 1.4776\n",
      "Epoch 16/200\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.0058  \n",
      "Epoch 16: val_loss did not improve from 1.46489\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 1.5103\n",
      "Epoch 17/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0040  \n",
      "Epoch 17: val_loss did not improve from 1.46489\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 1.4961\n",
      "Epoch 18/200\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.0041\n",
      "Epoch 18: val_loss did not improve from 1.46489\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 1.4701\n",
      "Epoch 19/200\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0051  \n",
      "Epoch 19: val_loss did not improve from 1.46489\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 1.4887\n",
      "Epoch 20/200\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.0052  \n",
      "Epoch 20: val_loss did not improve from 1.46489\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0052 - val_loss: 1.4716\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train, validation_data=(X_test,y_test), epochs=200, batch_size=10, callbacks=[early_stopping_callback,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66daac3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train,y_train, validation_data=(X_test,y_test), epochs=200, batch_size=10, callbacks=[early_stopping_callback,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "4f0ec08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 1ms/step - loss: 1.4716\n",
      "Test Accuracy : 1.4716\n"
     ]
    }
   ],
   "source": [
    "print('Test Accuracy : %.4f' % (model.evaluate(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "82c92c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "8cca502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_DIR = './model/'\n",
    "# if not os.path.exists(MODEL_DIR):\n",
    "#     os.mkdir(MODEL_DIR)\n",
    "# modelpath = './model/{epoch:02d}-{val_loss:.4f}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "e90a83c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = 'C:/Users/thsong/dl_model/{epoch}-{val_loss}.hdf5'\n",
    "#modelpath = 'C:/Users/thsong/dl_model/random.hdf5'\n",
    "#modelpath = 'C:/Users/thsong/dl_model/{epoch}-{loss}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "960d6d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "19c93e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True) #  #save_freq='epoch'\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience = 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "ca947508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss of test set\n",
    "#y_vloss = history.history['val_loss']\n",
    "# loss of training set\n",
    "y_loss = history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "0cc0e15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0324941910803318,\n",
       " 0.04654156044125557,\n",
       " 0.026299603283405304,\n",
       " 0.018241239711642265,\n",
       " 0.012584234587848186,\n",
       " 0.012008728459477425,\n",
       " 0.011411734856665134,\n",
       " 0.014065620489418507,\n",
       " 0.011749878525733948,\n",
       " 0.01345842145383358,\n",
       " 0.026285512372851372,\n",
       " 0.02403472736477852,\n",
       " 0.024234069511294365,\n",
       " 0.023437168449163437,\n",
       " 0.020394176244735718,\n",
       " 0.020210735499858856,\n",
       " 0.019699253141880035]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "633a489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss of test set\n",
    "y_vloss = history.history['val_loss']\n",
    "# loss of training set\n",
    "y_loss = history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "9ec34840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.026801198720932007,\n",
       "  0.023621873930096626,\n",
       "  0.01780463568866253,\n",
       "  0.019237937405705452,\n",
       "  0.016094204038381577,\n",
       "  0.013501581735908985,\n",
       "  0.013044033199548721,\n",
       "  0.01751595549285412,\n",
       "  0.016307048499584198,\n",
       "  0.0147242471575737,\n",
       "  0.025768235325813293,\n",
       "  0.029856465756893158,\n",
       "  0.025385793298482895,\n",
       "  0.023202912881970406,\n",
       "  0.01885855942964554,\n",
       "  0.016912415623664856,\n",
       "  0.016396844759583473]}"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "9b3be2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.004051957279443741,\n",
       "  0.00706836674362421,\n",
       "  0.005629725754261017,\n",
       "  0.0073941550217568874,\n",
       "  0.004703929182142019,\n",
       "  0.005571009125560522,\n",
       "  0.005213751923292875,\n",
       "  0.005996508989483118,\n",
       "  0.005072294268757105,\n",
       "  0.004781967028975487,\n",
       "  0.006491395644843578,\n",
       "  0.00517394719645381,\n",
       "  0.003895056201145053,\n",
       "  0.005907593294978142,\n",
       "  0.006314829457551241,\n",
       "  0.005870186258107424,\n",
       "  0.003996329847723246,\n",
       "  0.004061590880155563,\n",
       "  0.005110586527734995,\n",
       "  0.005158072337508202],\n",
       " 'val_loss': [1.4748884439468384,\n",
       "  1.4759284257888794,\n",
       "  1.4958972930908203,\n",
       "  1.4823068380355835,\n",
       "  1.4995769262313843,\n",
       "  1.4681525230407715,\n",
       "  1.5018482208251953,\n",
       "  1.495990514755249,\n",
       "  1.485611081123352,\n",
       "  1.4648925065994263,\n",
       "  1.4654250144958496,\n",
       "  1.50474214553833,\n",
       "  1.5054603815078735,\n",
       "  1.48075270652771,\n",
       "  1.4776403903961182,\n",
       "  1.5102522373199463,\n",
       "  1.4961082935333252,\n",
       "  1.470075249671936,\n",
       "  1.4886554479599,\n",
       "  1.4715557098388672]}"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "ce9e8bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# draw a graph\n",
    "x_len = np.arange(len(y_loss))\n",
    "plt.plot(x_len,y_vloss, marker='.', c='red', label='Testset_loss')\n",
    "plt.plot(x_len,y_loss, marker='.', c='blue', label='Trainset_loss')\n",
    "# labeling\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0f1894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "440116d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 1ms/step\n",
      "actual value : 0.46627249488941863, predicted value : 0.2206415832042694\n",
      "actual value : 0.46627249488941863, predicted value : -0.5318472385406494\n",
      "actual value : 0.46627249488941863, predicted value : -1.30612051486969\n",
      "actual value : 0.46627249488941863, predicted value : 0.3162066340446472\n",
      "actual value : 0.46627249488941863, predicted value : 0.46330755949020386\n",
      "actual value : 0.46627249488941863, predicted value : -0.6501566171646118\n",
      "actual value : 0.46627249488941863, predicted value : -1.0501976013183594\n",
      "actual value : 0.46627249488941863, predicted value : 0.3004269301891327\n",
      "actual value : 0.46627249488941863, predicted value : 0.07270151376724243\n",
      "actual value : 0.46627249488941863, predicted value : 0.2996103763580322\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test).flatten()\n",
    "for i in range(10):\n",
    "    label = y_test[i]\n",
    "    prediction = y_pred[i]\n",
    "    print('actual value : {}, predicted value : {}'.format(label,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "dceab891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22064158"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "c40c8c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.53184724"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "c83fd85a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46627249488941863"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "ba1c75ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46627249488941863"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "4fdee17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249, -2.14156407,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249, -2.08393232, -2.14156407,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249, -2.14156407,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249, -2.08393232, -2.08393232,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249, -2.32886725,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249, -2.34807783,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "       -2.08393232, -2.14156407, -2.34807783,  0.46627249, -2.14156407,\n",
       "       -2.08393232, -2.08393232,  0.46627249,  0.46627249,  0.46627249,\n",
       "       -2.14156407,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "       -2.08393232,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "       -2.08393232,  0.46627249, -2.32886725, -2.08393232, -2.08393232,\n",
       "        0.46627249,  0.46627249,  0.46627249, -2.14156407,  0.46627249,\n",
       "        0.46627249, -2.14156407, -2.32886725,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249, -2.08393232, -2.14156407,  0.46627249,  0.46627249,\n",
       "       -2.08393232,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "       -2.08393232,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "       -2.32886725,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249, -2.14156407,\n",
       "        0.46627249, -2.32886725,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249, -2.32886725, -2.08393232,  0.46627249,  0.46627249,\n",
       "       -2.14156407,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "       -2.14156407,  0.46627249,  0.46627249,  0.46627249, -2.14156407,\n",
       "        0.46627249,  0.46627249, -2.36728841,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "       -2.14156407,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249, -2.14156407,  0.46627249, -2.08393232, -2.14156407,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249, -2.08393232, -2.32886725,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "       -2.08393232,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "       -2.08393232,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "       -2.08393232,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "       -2.32886725,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249,  0.46627249,\n",
       "        0.46627249, -2.32886725,  0.46627249,  0.46627249,  0.46627249,\n",
       "       -2.14156407,  0.46627249, -2.14156407,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249, -2.32886725,  0.46627249,  0.46627249,\n",
       "        0.46627249,  0.46627249, -2.34807783,  0.46627249, -2.14156407,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249, -2.36728841,\n",
       "        0.46627249,  0.46627249,  0.46627249,  0.46627249, -2.08393232,\n",
       "        0.46627249,  0.46627249])"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c852f0e",
   "metadata": {},
   "source": [
    "### nov09.2022 2nd trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "da8eded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "83540bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_,y,test_size=0.2,random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "65008331",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_,y,test_size=0.2) # ,random_state=seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "c260ad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# 30, 13, 6\n",
    "model.add(Dense(90, input_dim=43, activation='relu'))\n",
    "model.add(Dense(21, activation='relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "a762a06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "5f7ed557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1432, 43)"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "116e591e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 55.7655\n",
      "Epoch 1: val_loss did not improve from 1.46489\n",
      "115/115 [==============================] - 1s 3ms/step - loss: 52.9388 - val_loss: 4.3332\n",
      "Epoch 2/200\n",
      "105/115 [==========================>...] - ETA: 0s - loss: 2.5750\n",
      "Epoch 2: val_loss did not improve from 1.46489\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 2.5313 - val_loss: 1.8953\n",
      "Epoch 3/200\n",
      " 94/115 [=======================>......] - ETA: 0s - loss: 1.4456\n",
      "Epoch 3: val_loss improved from 1.46489 to 1.44328, saving model to C:/Users/thsong/dl_model\\3-1.4432822465896606.hdf5\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 1.4158 - val_loss: 1.4433\n",
      "Epoch 4/200\n",
      " 92/115 [=======================>......] - ETA: 0s - loss: 1.0744\n",
      "Epoch 4: val_loss improved from 1.44328 to 1.27315, saving model to C:/Users/thsong/dl_model\\4-1.2731479406356812.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 1.0722 - val_loss: 1.2731\n",
      "Epoch 5/200\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.8748\n",
      "Epoch 5: val_loss improved from 1.27315 to 1.09175, saving model to C:/Users/thsong/dl_model\\5-1.0917490720748901.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.8748 - val_loss: 1.0917\n",
      "Epoch 6/200\n",
      " 77/115 [===================>..........] - ETA: 0s - loss: 0.7903\n",
      "Epoch 6: val_loss improved from 1.09175 to 0.99090, saving model to C:/Users/thsong/dl_model\\6-0.9909048080444336.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.7403 - val_loss: 0.9909\n",
      "Epoch 7/200\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.6288\n",
      "Epoch 7: val_loss improved from 0.99090 to 0.91477, saving model to C:/Users/thsong/dl_model\\7-0.9147690534591675.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6211 - val_loss: 0.9148\n",
      "Epoch 8/200\n",
      " 94/115 [=======================>......] - ETA: 0s - loss: 0.5380\n",
      "Epoch 8: val_loss improved from 0.91477 to 0.84054, saving model to C:/Users/thsong/dl_model\\8-0.8405444622039795.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.5393 - val_loss: 0.8405\n",
      "Epoch 9/200\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.4672\n",
      "Epoch 9: val_loss improved from 0.84054 to 0.82107, saving model to C:/Users/thsong/dl_model\\9-0.8210709095001221.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.4757 - val_loss: 0.8211\n",
      "Epoch 10/200\n",
      "107/115 [==========================>...] - ETA: 0s - loss: 0.4037\n",
      "Epoch 10: val_loss improved from 0.82107 to 0.73170, saving model to C:/Users/thsong/dl_model\\10-0.7317041754722595.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.4121 - val_loss: 0.7317\n",
      "Epoch 11/200\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.3611\n",
      "Epoch 11: val_loss improved from 0.73170 to 0.68028, saving model to C:/Users/thsong/dl_model\\11-0.6802774667739868.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.3653 - val_loss: 0.6803\n",
      "Epoch 12/200\n",
      " 93/115 [=======================>......] - ETA: 0s - loss: 0.3285\n",
      "Epoch 12: val_loss did not improve from 0.68028\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.3256 - val_loss: 0.6842\n",
      "Epoch 13/200\n",
      "102/115 [=========================>....] - ETA: 0s - loss: 0.3165\n",
      "Epoch 13: val_loss improved from 0.68028 to 0.61849, saving model to C:/Users/thsong/dl_model\\13-0.6184942126274109.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.3159 - val_loss: 0.6185\n",
      "Epoch 14/200\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.2671\n",
      "Epoch 14: val_loss improved from 0.61849 to 0.57574, saving model to C:/Users/thsong/dl_model\\14-0.5757433176040649.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2665 - val_loss: 0.5757\n",
      "Epoch 15/200\n",
      " 93/115 [=======================>......] - ETA: 0s - loss: 0.2413\n",
      "Epoch 15: val_loss did not improve from 0.57574\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2489 - val_loss: 0.5829\n",
      "Epoch 16/200\n",
      " 97/115 [========================>.....] - ETA: 0s - loss: 0.2160\n",
      "Epoch 16: val_loss improved from 0.57574 to 0.54195, saving model to C:/Users/thsong/dl_model\\16-0.5419465899467468.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2237 - val_loss: 0.5419\n",
      "Epoch 17/200\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.2018\n",
      "Epoch 17: val_loss improved from 0.54195 to 0.53200, saving model to C:/Users/thsong/dl_model\\17-0.5319950580596924.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2010 - val_loss: 0.5320\n",
      "Epoch 18/200\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.1807\n",
      "Epoch 18: val_loss did not improve from 0.53200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1835 - val_loss: 0.5476\n",
      "Epoch 19/200\n",
      " 99/115 [========================>.....] - ETA: 0s - loss: 0.1823\n",
      "Epoch 19: val_loss improved from 0.53200 to 0.49011, saving model to C:/Users/thsong/dl_model\\19-0.49011358618736267.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1795 - val_loss: 0.4901\n",
      "Epoch 20/200\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.1585\n",
      "Epoch 20: val_loss improved from 0.49011 to 0.46734, saving model to C:/Users/thsong/dl_model\\20-0.4673435389995575.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1578 - val_loss: 0.4673\n",
      "Epoch 21/200\n",
      " 94/115 [=======================>......] - ETA: 0s - loss: 0.1456\n",
      "Epoch 21: val_loss did not improve from 0.46734\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1447 - val_loss: 0.4845\n",
      "Epoch 22/200\n",
      "104/115 [==========================>...] - ETA: 0s - loss: 0.1349\n",
      "Epoch 22: val_loss improved from 0.46734 to 0.44985, saving model to C:/Users/thsong/dl_model\\22-0.4498513638973236.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1341 - val_loss: 0.4499\n",
      "Epoch 23/200\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.1189\n",
      "Epoch 23: val_loss improved from 0.44985 to 0.42133, saving model to C:/Users/thsong/dl_model\\23-0.4213322103023529.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1187 - val_loss: 0.4213\n",
      "Epoch 24/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.1121\n",
      "Epoch 24: val_loss improved from 0.42133 to 0.41314, saving model to C:/Users/thsong/dl_model\\24-0.4131365418434143.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1131 - val_loss: 0.4131\n",
      "Epoch 25/200\n",
      "103/115 [=========================>....] - ETA: 0s - loss: 0.1063\n",
      "Epoch 25: val_loss did not improve from 0.41314\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1066 - val_loss: 0.4143\n",
      "Epoch 26/200\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.0982\n",
      "Epoch 26: val_loss improved from 0.41314 to 0.39382, saving model to C:/Users/thsong/dl_model\\26-0.3938181400299072.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0991 - val_loss: 0.3938\n",
      "Epoch 27/200\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.0892\n",
      "Epoch 27: val_loss improved from 0.39382 to 0.39310, saving model to C:/Users/thsong/dl_model\\27-0.3930957615375519.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0889 - val_loss: 0.3931\n",
      "Epoch 28/200\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.0852\n",
      "Epoch 28: val_loss improved from 0.39310 to 0.39237, saving model to C:/Users/thsong/dl_model\\28-0.3923683166503906.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0853 - val_loss: 0.3924\n",
      "Epoch 29/200\n",
      "107/115 [==========================>...] - ETA: 0s - loss: 0.0744\n",
      "Epoch 29: val_loss improved from 0.39237 to 0.38069, saving model to C:/Users/thsong/dl_model\\29-0.3806867003440857.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0750 - val_loss: 0.3807\n",
      "Epoch 30/200\n",
      "105/115 [==========================>...] - ETA: 0s - loss: 0.0726\n",
      "Epoch 30: val_loss improved from 0.38069 to 0.35975, saving model to C:/Users/thsong/dl_model\\30-0.3597477972507477.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0723 - val_loss: 0.3597\n",
      "Epoch 31/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 90/115 [======================>.......] - ETA: 0s - loss: 0.0734\n",
      "Epoch 31: val_loss did not improve from 0.35975\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0729 - val_loss: 0.3606\n",
      "Epoch 32/200\n",
      " 79/115 [===================>..........] - ETA: 0s - loss: 0.0597\n",
      "Epoch 32: val_loss improved from 0.35975 to 0.34362, saving model to C:/Users/thsong/dl_model\\32-0.34362176060676575.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0618 - val_loss: 0.3436\n",
      "Epoch 33/200\n",
      " 80/115 [===================>..........] - ETA: 0s - loss: 0.0548\n",
      "Epoch 33: val_loss did not improve from 0.34362\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0581 - val_loss: 0.3521\n",
      "Epoch 34/200\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.0549\n",
      "Epoch 34: val_loss did not improve from 0.34362\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0548 - val_loss: 0.3468\n",
      "Epoch 35/200\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.0555\n",
      "Epoch 35: val_loss did not improve from 0.34362\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0555 - val_loss: 0.3535\n",
      "Epoch 36/200\n",
      " 78/115 [===================>..........] - ETA: 0s - loss: 0.0442\n",
      "Epoch 36: val_loss did not improve from 0.34362\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0463 - val_loss: 0.3459\n",
      "Epoch 37/200\n",
      " 77/115 [===================>..........] - ETA: 0s - loss: 0.0500\n",
      "Epoch 37: val_loss improved from 0.34362 to 0.32431, saving model to C:/Users/thsong/dl_model\\37-0.32430604100227356.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0475 - val_loss: 0.3243\n",
      "Epoch 38/200\n",
      " 79/115 [===================>..........] - ETA: 0s - loss: 0.0411\n",
      "Epoch 38: val_loss improved from 0.32431 to 0.32026, saving model to C:/Users/thsong/dl_model\\38-0.32026395201683044.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0417 - val_loss: 0.3203\n",
      "Epoch 39/200\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.0385\n",
      "Epoch 39: val_loss did not improve from 0.32026\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0387 - val_loss: 0.3211\n",
      "Epoch 40/200\n",
      " 76/115 [==================>...........] - ETA: 0s - loss: 0.0345\n",
      "Epoch 40: val_loss did not improve from 0.32026\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0368 - val_loss: 0.3215\n",
      "Epoch 41/200\n",
      " 78/115 [===================>..........] - ETA: 0s - loss: 0.0332\n",
      "Epoch 41: val_loss improved from 0.32026 to 0.30928, saving model to C:/Users/thsong/dl_model\\41-0.3092835545539856.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0353 - val_loss: 0.3093\n",
      "Epoch 42/200\n",
      " 77/115 [===================>..........] - ETA: 0s - loss: 0.0413\n",
      "Epoch 42: val_loss did not improve from 0.30928\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0413 - val_loss: 0.3101\n",
      "Epoch 43/200\n",
      " 77/115 [===================>..........] - ETA: 0s - loss: 0.0364\n",
      "Epoch 43: val_loss improved from 0.30928 to 0.30747, saving model to C:/Users/thsong/dl_model\\43-0.3074738085269928.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0367 - val_loss: 0.3075\n",
      "Epoch 44/200\n",
      "105/115 [==========================>...] - ETA: 0s - loss: 0.0303\n",
      "Epoch 44: val_loss improved from 0.30747 to 0.29494, saving model to C:/Users/thsong/dl_model\\44-0.2949441373348236.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0298 - val_loss: 0.2949\n",
      "Epoch 45/200\n",
      " 79/115 [===================>..........] - ETA: 0s - loss: 0.0271\n",
      "Epoch 45: val_loss did not improve from 0.29494\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0302 - val_loss: 0.3080\n",
      "Epoch 46/200\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.0303\n",
      "Epoch 46: val_loss did not improve from 0.29494\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0305 - val_loss: 0.2972\n",
      "Epoch 47/200\n",
      " 79/115 [===================>..........] - ETA: 0s - loss: 0.0363\n",
      "Epoch 47: val_loss improved from 0.29494 to 0.29103, saving model to C:/Users/thsong/dl_model\\47-0.29102808237075806.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0378 - val_loss: 0.2910\n",
      "Epoch 48/200\n",
      " 80/115 [===================>..........] - ETA: 0s - loss: 0.0284\n",
      "Epoch 48: val_loss improved from 0.29103 to 0.28812, saving model to C:/Users/thsong/dl_model\\48-0.2881207764148712.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0288 - val_loss: 0.2881\n",
      "Epoch 49/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0272\n",
      "Epoch 49: val_loss improved from 0.28812 to 0.27634, saving model to C:/Users/thsong/dl_model\\49-0.2763412594795227.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.2763\n",
      "Epoch 50/200\n",
      " 77/115 [===================>..........] - ETA: 0s - loss: 0.0236\n",
      "Epoch 50: val_loss did not improve from 0.27634\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.2875\n",
      "Epoch 51/200\n",
      " 78/115 [===================>..........] - ETA: 0s - loss: 0.0256\n",
      "Epoch 51: val_loss did not improve from 0.27634\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.2891\n",
      "Epoch 52/200\n",
      " 80/115 [===================>..........] - ETA: 0s - loss: 0.0300\n",
      "Epoch 52: val_loss did not improve from 0.27634\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.2778\n",
      "Epoch 53/200\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.0369\n",
      "Epoch 53: val_loss did not improve from 0.27634\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0370 - val_loss: 0.3186\n",
      "Epoch 54/200\n",
      "105/115 [==========================>...] - ETA: 0s - loss: 0.0372\n",
      "Epoch 54: val_loss did not improve from 0.27634\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0367 - val_loss: 0.2821\n",
      "Epoch 55/200\n",
      "107/115 [==========================>...] - ETA: 0s - loss: 0.0319\n",
      "Epoch 55: val_loss improved from 0.27634 to 0.26963, saving model to C:/Users/thsong/dl_model\\55-0.2696264088153839.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0315 - val_loss: 0.2696\n",
      "Epoch 56/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0233\n",
      "Epoch 56: val_loss did not improve from 0.26963\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.2869\n",
      "Epoch 57/200\n",
      " 98/115 [========================>.....] - ETA: 0s - loss: 0.0223\n",
      "Epoch 57: val_loss did not improve from 0.26963\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.2832\n",
      "Epoch 58/200\n",
      "100/115 [=========================>....] - ETA: 0s - loss: 0.0201\n",
      "Epoch 58: val_loss improved from 0.26963 to 0.26698, saving model to C:/Users/thsong/dl_model\\58-0.26698142290115356.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0204 - val_loss: 0.2670\n",
      "Epoch 59/200\n",
      "107/115 [==========================>...] - ETA: 0s - loss: 0.0284\n",
      "Epoch 59: val_loss improved from 0.26698 to 0.26564, saving model to C:/Users/thsong/dl_model\\59-0.26564013957977295.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.2656\n",
      "Epoch 60/200\n",
      "101/115 [=========================>....] - ETA: 0s - loss: 0.0274\n",
      "Epoch 60: val_loss did not improve from 0.26564\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.2758\n",
      "Epoch 61/200\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.0286\n",
      "Epoch 61: val_loss improved from 0.26564 to 0.26115, saving model to C:/Users/thsong/dl_model\\61-0.2611534297466278.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.2612\n",
      "Epoch 62/200\n",
      " 79/115 [===================>..........] - ETA: 0s - loss: 0.0242\n",
      "Epoch 62: val_loss did not improve from 0.26115\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.2836\n",
      "Epoch 63/200\n",
      " 78/115 [===================>..........] - ETA: 0s - loss: 0.0309\n",
      "Epoch 63: val_loss improved from 0.26115 to 0.24849, saving model to C:/Users/thsong/dl_model\\63-0.24849054217338562.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0288 - val_loss: 0.2485\n",
      "Epoch 64/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 78/115 [===================>..........] - ETA: 0s - loss: 0.0321\n",
      "Epoch 64: val_loss did not improve from 0.24849\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 0.2796\n",
      "Epoch 65/200\n",
      " 77/115 [===================>..........] - ETA: 0s - loss: 0.0431\n",
      "Epoch 65: val_loss did not improve from 0.24849\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0443 - val_loss: 0.2519\n",
      "Epoch 66/200\n",
      " 79/115 [===================>..........] - ETA: 0s - loss: 0.0472\n",
      "Epoch 66: val_loss did not improve from 0.24849\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0476 - val_loss: 0.2680\n",
      "Epoch 67/200\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.0346\n",
      "Epoch 67: val_loss did not improve from 0.24849\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0347 - val_loss: 0.2817\n",
      "Epoch 68/200\n",
      "101/115 [=========================>....] - ETA: 0s - loss: 0.0314\n",
      "Epoch 68: val_loss did not improve from 0.24849\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0305 - val_loss: 0.2493\n",
      "Epoch 69/200\n",
      " 77/115 [===================>..........] - ETA: 0s - loss: 0.0210\n",
      "Epoch 69: val_loss did not improve from 0.24849\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.2552\n",
      "Epoch 70/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0267\n",
      "Epoch 70: val_loss did not improve from 0.24849\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.2546\n",
      "Epoch 71/200\n",
      " 79/115 [===================>..........] - ETA: 0s - loss: 0.0232\n",
      "Epoch 71: val_loss improved from 0.24849 to 0.24190, saving model to C:/Users/thsong/dl_model\\71-0.24190093576908112.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.2419\n",
      "Epoch 72/200\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.0206\n",
      "Epoch 72: val_loss did not improve from 0.24190\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0207 - val_loss: 0.2525\n",
      "Epoch 73/200\n",
      " 95/115 [=======================>......] - ETA: 0s - loss: 0.0225\n",
      "Epoch 73: val_loss did not improve from 0.24190\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.2434\n",
      "Epoch 74/200\n",
      " 76/115 [==================>...........] - ETA: 0s - loss: 0.0246\n",
      "Epoch 74: val_loss did not improve from 0.24190\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.2554\n",
      "Epoch 75/200\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.0270\n",
      "Epoch 75: val_loss did not improve from 0.24190\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.2507\n",
      "Epoch 76/200\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.0321\n",
      "Epoch 76: val_loss did not improve from 0.24190\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0320 - val_loss: 0.2509\n",
      "Epoch 77/200\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0295\n",
      "Epoch 77: val_loss improved from 0.24190 to 0.23536, saving model to C:/Users/thsong/dl_model\\77-0.2353609949350357.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.2354\n",
      "Epoch 78/200\n",
      " 82/115 [====================>.........] - ETA: 0s - loss: 0.0200\n",
      "Epoch 78: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.2408\n",
      "Epoch 79/200\n",
      "106/115 [==========================>...] - ETA: 0s - loss: 0.0239\n",
      "Epoch 79: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.2448\n",
      "Epoch 80/200\n",
      "102/115 [=========================>....] - ETA: 0s - loss: 0.0229\n",
      "Epoch 80: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.2808\n",
      "Epoch 81/200\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.0430\n",
      "Epoch 81: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0421 - val_loss: 0.2449\n",
      "Epoch 82/200\n",
      " 82/115 [====================>.........] - ETA: 0s - loss: 0.0289\n",
      "Epoch 82: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 0.2594\n",
      "Epoch 83/200\n",
      " 77/115 [===================>..........] - ETA: 0s - loss: 0.0305\n",
      "Epoch 83: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.2385\n",
      "Epoch 84/200\n",
      " 77/115 [===================>..........] - ETA: 0s - loss: 0.0246\n",
      "Epoch 84: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0313 - val_loss: 0.2609\n",
      "Epoch 85/200\n",
      " 78/115 [===================>..........] - ETA: 0s - loss: 0.0380\n",
      "Epoch 85: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0354 - val_loss: 0.2406\n",
      "Epoch 86/200\n",
      " 77/115 [===================>..........] - ETA: 0s - loss: 0.0169\n",
      "Epoch 86: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.2398\n",
      "Epoch 87/200\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 87: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0166 - val_loss: 0.2463\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train, validation_data=(X_test,y_test), epochs=200, batch_size=10, callbacks=[early_stopping_callback,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "acfb4208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "112/115 [============================>.] - ETA: 0s - loss: 41.3867\n",
      "Epoch 1: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 1s 3ms/step - loss: 40.6009 - val_loss: 3.6395\n",
      "Epoch 2/200\n",
      " 92/115 [=======================>......] - ETA: 0s - loss: 2.3430\n",
      "Epoch 2: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 2.2854 - val_loss: 1.7998\n",
      "Epoch 3/200\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 1.3699\n",
      "Epoch 3: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 1.3932 - val_loss: 1.3698\n",
      "Epoch 4/200\n",
      "102/115 [=========================>....] - ETA: 0s - loss: 1.0609\n",
      "Epoch 4: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 1.0504 - val_loss: 1.2254\n",
      "Epoch 5/200\n",
      "101/115 [=========================>....] - ETA: 0s - loss: 0.8287\n",
      "Epoch 5: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.8410 - val_loss: 1.0255\n",
      "Epoch 6/200\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.7019\n",
      "Epoch 6: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6981 - val_loss: 0.9260\n",
      "Epoch 7/200\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.5826\n",
      "Epoch 7: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.5813 - val_loss: 0.8960\n",
      "Epoch 8/200\n",
      " 78/115 [===================>..........] - ETA: 0s - loss: 0.5320\n",
      "Epoch 8: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.5018 - val_loss: 0.7996\n",
      "Epoch 9/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.4497\n",
      "Epoch 9: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.4488 - val_loss: 0.7630\n",
      "Epoch 10/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.3828\n",
      "Epoch 10: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.3842 - val_loss: 0.7331\n",
      "Epoch 11/200\n",
      "103/115 [=========================>....] - ETA: 0s - loss: 0.3433\n",
      "Epoch 11: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.3430 - val_loss: 0.6602\n",
      "Epoch 12/200\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.3009\n",
      "Epoch 12: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2988 - val_loss: 0.7391\n",
      "Epoch 13/200\n",
      " 99/115 [========================>.....] - ETA: 0s - loss: 0.2987\n",
      "Epoch 13: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2962 - val_loss: 0.6015\n",
      "Epoch 14/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.2415\n",
      "Epoch 14: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2477 - val_loss: 0.5834\n",
      "Epoch 15/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.2330\n",
      "Epoch 15: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2337 - val_loss: 0.6085\n",
      "Epoch 16/200\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.2096\n",
      "Epoch 16: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2090 - val_loss: 0.5574\n",
      "Epoch 17/200\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.1920\n",
      "Epoch 17: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1924 - val_loss: 0.5488\n",
      "Epoch 18/200\n",
      " 76/115 [==================>...........] - ETA: 0s - loss: 0.1765\n",
      "Epoch 18: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1776 - val_loss: 0.5316\n",
      "Epoch 19/200\n",
      " 98/115 [========================>.....] - ETA: 0s - loss: 0.1691\n",
      "Epoch 19: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1708 - val_loss: 0.5079\n",
      "Epoch 20/200\n",
      "102/115 [=========================>....] - ETA: 0s - loss: 0.1421\n",
      "Epoch 20: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1462 - val_loss: 0.4693\n",
      "Epoch 21/200\n",
      " 79/115 [===================>..........] - ETA: 0s - loss: 0.1328\n",
      "Epoch 21: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1354 - val_loss: 0.4854\n",
      "Epoch 22/200\n",
      " 96/115 [========================>.....] - ETA: 0s - loss: 0.1282\n",
      "Epoch 22: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1267 - val_loss: 0.4711\n",
      "Epoch 23/200\n",
      "104/115 [==========================>...] - ETA: 0s - loss: 0.1196\n",
      "Epoch 23: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1196 - val_loss: 0.4706\n",
      "Epoch 24/200\n",
      "101/115 [=========================>....] - ETA: 0s - loss: 0.1040\n",
      "Epoch 24: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.1096 - val_loss: 0.4814\n",
      "Epoch 25/200\n",
      "103/115 [=========================>....] - ETA: 0s - loss: 0.1001\n",
      "Epoch 25: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1020 - val_loss: 0.4869\n",
      "Epoch 26/200\n",
      "101/115 [=========================>....] - ETA: 0s - loss: 0.0900\n",
      "Epoch 26: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0925 - val_loss: 0.4223\n",
      "Epoch 27/200\n",
      "107/115 [==========================>...] - ETA: 0s - loss: 0.0833\n",
      "Epoch 27: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0831 - val_loss: 0.4458\n",
      "Epoch 28/200\n",
      "107/115 [==========================>...] - ETA: 0s - loss: 0.0858\n",
      "Epoch 28: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0849 - val_loss: 0.4818\n",
      "Epoch 29/200\n",
      " 84/115 [====================>.........] - ETA: 0s - loss: 0.0854\n",
      "Epoch 29: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0842 - val_loss: 0.4250\n",
      "Epoch 30/200\n",
      " 99/115 [========================>.....] - ETA: 0s - loss: 0.0756\n",
      "Epoch 30: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0764 - val_loss: 0.4401\n",
      "Epoch 31/200\n",
      " 81/115 [====================>.........] - ETA: 0s - loss: 0.0756\n",
      "Epoch 31: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0726 - val_loss: 0.4036\n",
      "Epoch 32/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0684\n",
      "Epoch 32: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0690 - val_loss: 0.3907\n",
      "Epoch 33/200\n",
      " 90/115 [======================>.......] - ETA: 0s - loss: 0.0629\n",
      "Epoch 33: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0630 - val_loss: 0.3680\n",
      "Epoch 34/200\n",
      " 77/115 [===================>..........] - ETA: 0s - loss: 0.0550\n",
      "Epoch 34: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0561 - val_loss: 0.3946\n",
      "Epoch 35/200\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.0587\n",
      "Epoch 35: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0590 - val_loss: 0.3910\n",
      "Epoch 36/200\n",
      "106/115 [==========================>...] - ETA: 0s - loss: 0.0523\n",
      "Epoch 36: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0518 - val_loss: 0.3824\n",
      "Epoch 37/200\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.0497\n",
      "Epoch 37: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0495 - val_loss: 0.3478\n",
      "Epoch 38/200\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.0419\n",
      "Epoch 38: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0419 - val_loss: 0.3514\n",
      "Epoch 39/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 80/115 [===================>..........] - ETA: 0s - loss: 0.0419\n",
      "Epoch 39: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0448 - val_loss: 0.3600\n",
      "Epoch 40/200\n",
      " 81/115 [====================>.........] - ETA: 0s - loss: 0.0543\n",
      "Epoch 40: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0538 - val_loss: 0.3663\n",
      "Epoch 41/200\n",
      " 78/115 [===================>..........] - ETA: 0s - loss: 0.0404\n",
      "Epoch 41: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0416 - val_loss: 0.3328\n",
      "Epoch 42/200\n",
      "105/115 [==========================>...] - ETA: 0s - loss: 0.0462\n",
      "Epoch 42: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0459 - val_loss: 0.3467\n",
      "Epoch 43/200\n",
      " 74/115 [==================>...........] - ETA: 0s - loss: 0.0398\n",
      "Epoch 43: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0399 - val_loss: 0.3648\n",
      "Epoch 44/200\n",
      " 76/115 [==================>...........] - ETA: 0s - loss: 0.0345\n",
      "Epoch 44: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0370 - val_loss: 0.3254\n",
      "Epoch 45/200\n",
      "106/115 [==========================>...] - ETA: 0s - loss: 0.0327\n",
      "Epoch 45: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 0.3313\n",
      "Epoch 46/200\n",
      " 86/115 [=====================>........] - ETA: 0s - loss: 0.0315\n",
      "Epoch 46: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0315 - val_loss: 0.3517\n",
      "Epoch 47/200\n",
      "100/115 [=========================>....] - ETA: 0s - loss: 0.0368\n",
      "Epoch 47: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0371 - val_loss: 0.3325\n",
      "Epoch 48/200\n",
      " 96/115 [========================>.....] - ETA: 0s - loss: 0.0346\n",
      "Epoch 48: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0357 - val_loss: 0.3681\n",
      "Epoch 49/200\n",
      "106/115 [==========================>...] - ETA: 0s - loss: 0.0311\n",
      "Epoch 49: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0320 - val_loss: 0.3374\n",
      "Epoch 50/200\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.0329\n",
      "Epoch 50: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0330 - val_loss: 0.3322\n",
      "Epoch 51/200\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.0280\n",
      "Epoch 51: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.3114\n",
      "Epoch 52/200\n",
      "103/115 [=========================>....] - ETA: 0s - loss: 0.0373\n",
      "Epoch 52: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0376 - val_loss: 0.3126\n",
      "Epoch 53/200\n",
      "107/115 [==========================>...] - ETA: 0s - loss: 0.0303\n",
      "Epoch 53: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.3309\n",
      "Epoch 54/200\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.0268\n",
      "Epoch 54: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.3238\n",
      "Epoch 55/200\n",
      "101/115 [=========================>....] - ETA: 0s - loss: 0.0342\n",
      "Epoch 55: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0362 - val_loss: 0.3457\n",
      "Epoch 56/200\n",
      "104/115 [==========================>...] - ETA: 0s - loss: 0.0457\n",
      "Epoch 56: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0442 - val_loss: 0.3069\n",
      "Epoch 57/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0350\n",
      "Epoch 57: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0348 - val_loss: 0.3179\n",
      "Epoch 58/200\n",
      "102/115 [=========================>....] - ETA: 0s - loss: 0.0375\n",
      "Epoch 58: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0372 - val_loss: 0.3039\n",
      "Epoch 59/200\n",
      "103/115 [=========================>....] - ETA: 0s - loss: 0.0409\n",
      "Epoch 59: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0401 - val_loss: 0.3195\n",
      "Epoch 60/200\n",
      " 96/115 [========================>.....] - ETA: 0s - loss: 0.0492\n",
      "Epoch 60: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0470 - val_loss: 0.3076\n",
      "Epoch 61/200\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.0364\n",
      "Epoch 61: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0363 - val_loss: 0.2869\n",
      "Epoch 62/200\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.0255\n",
      "Epoch 62: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.2948\n",
      "Epoch 63/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0233\n",
      "Epoch 63: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.2891\n",
      "Epoch 64/200\n",
      "107/115 [==========================>...] - ETA: 0s - loss: 0.0190\n",
      "Epoch 64: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.3333\n",
      "Epoch 65/200\n",
      "106/115 [==========================>...] - ETA: 0s - loss: 0.0255\n",
      "Epoch 65: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.2990\n",
      "Epoch 66/200\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.0320\n",
      "Epoch 66: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0318 - val_loss: 0.3087\n",
      "Epoch 67/200\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.0308\n",
      "Epoch 67: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0308 - val_loss: 0.3142\n",
      "Epoch 68/200\n",
      " 96/115 [========================>.....] - ETA: 0s - loss: 0.0327\n",
      "Epoch 68: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0319 - val_loss: 0.3012\n",
      "Epoch 69/200\n",
      "104/115 [==========================>...] - ETA: 0s - loss: 0.0367\n",
      "Epoch 69: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0365 - val_loss: 0.2989\n",
      "Epoch 70/200\n",
      " 98/115 [========================>.....] - ETA: 0s - loss: 0.0277\n",
      "Epoch 70: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.2926\n",
      "Epoch 71/200\n",
      "102/115 [=========================>....] - ETA: 0s - loss: 0.0293\n",
      "Epoch 71: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.2728\n",
      "Epoch 72/200\n",
      " 94/115 [=======================>......] - ETA: 0s - loss: 0.0189\n",
      "Epoch 72: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.2881\n",
      "Epoch 73/200\n",
      " 95/115 [=======================>......] - ETA: 0s - loss: 0.0169\n",
      "Epoch 73: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0174 - val_loss: 0.2739\n",
      "Epoch 74/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0184\n",
      "Epoch 74: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.3024\n",
      "Epoch 75/200\n",
      "107/115 [==========================>...] - ETA: 0s - loss: 0.0328\n",
      "Epoch 75: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.2805\n",
      "Epoch 76/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0532\n",
      "Epoch 76: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0535 - val_loss: 0.3047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/200\n",
      "106/115 [==========================>...] - ETA: 0s - loss: 0.0495\n",
      "Epoch 77: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0499 - val_loss: 0.3136\n",
      "Epoch 78/200\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.0290\n",
      "Epoch 78: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.2729\n",
      "Epoch 79/200\n",
      "106/115 [==========================>...] - ETA: 0s - loss: 0.0219\n",
      "Epoch 79: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.2851\n",
      "Epoch 80/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0214\n",
      "Epoch 80: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0211 - val_loss: 0.2717\n",
      "Epoch 81/200\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.0164\n",
      "Epoch 81: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 0.2773\n",
      "Epoch 82/200\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.0188\n",
      "Epoch 82: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0189 - val_loss: 0.2727\n",
      "Epoch 83/200\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.0264\n",
      "Epoch 83: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.2721\n",
      "Epoch 84/200\n",
      " 99/115 [========================>.....] - ETA: 0s - loss: 0.0494\n",
      "Epoch 84: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0527 - val_loss: 0.2933\n",
      "Epoch 85/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0566\n",
      "Epoch 85: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0553 - val_loss: 0.2830\n",
      "Epoch 86/200\n",
      " 97/115 [========================>.....] - ETA: 0s - loss: 0.0340\n",
      "Epoch 86: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0341 - val_loss: 0.2958\n",
      "Epoch 87/200\n",
      " 99/115 [========================>.....] - ETA: 0s - loss: 0.0229\n",
      "Epoch 87: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0222 - val_loss: 0.2545\n",
      "Epoch 88/200\n",
      "105/115 [==========================>...] - ETA: 0s - loss: 0.0179\n",
      "Epoch 88: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.2535\n",
      "Epoch 89/200\n",
      " 99/115 [========================>.....] - ETA: 0s - loss: 0.0174\n",
      "Epoch 89: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0172 - val_loss: 0.2514\n",
      "Epoch 90/200\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.0166\n",
      "Epoch 90: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0171 - val_loss: 0.2550\n",
      "Epoch 91/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0249\n",
      "Epoch 91: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.2606\n",
      "Epoch 92/200\n",
      "101/115 [=========================>....] - ETA: 0s - loss: 0.0197\n",
      "Epoch 92: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0214 - val_loss: 0.2790\n",
      "Epoch 93/200\n",
      "101/115 [=========================>....] - ETA: 0s - loss: 0.0282\n",
      "Epoch 93: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.2597\n",
      "Epoch 94/200\n",
      "107/115 [==========================>...] - ETA: 0s - loss: 0.0292\n",
      "Epoch 94: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0303 - val_loss: 0.2599\n",
      "Epoch 95/200\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.0378\n",
      "Epoch 95: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0370 - val_loss: 0.2552\n",
      "Epoch 96/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0254\n",
      "Epoch 96: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.2419\n",
      "Epoch 97/200\n",
      " 93/115 [=======================>......] - ETA: 0s - loss: 0.0164\n",
      "Epoch 97: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0162 - val_loss: 0.2540\n",
      "Epoch 98/200\n",
      "100/115 [=========================>....] - ETA: 0s - loss: 0.0143\n",
      "Epoch 98: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0147 - val_loss: 0.2420\n",
      "Epoch 99/200\n",
      "103/115 [=========================>....] - ETA: 0s - loss: 0.0245\n",
      "Epoch 99: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.2736\n",
      "Epoch 100/200\n",
      "102/115 [=========================>....] - ETA: 0s - loss: 0.0229\n",
      "Epoch 100: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0222 - val_loss: 0.2465\n",
      "Epoch 101/200\n",
      " 98/115 [========================>.....] - ETA: 0s - loss: 0.0174\n",
      "Epoch 101: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0168 - val_loss: 0.2498\n",
      "Epoch 102/200\n",
      "103/115 [=========================>....] - ETA: 0s - loss: 0.0160\n",
      "Epoch 102: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.2405\n",
      "Epoch 103/200\n",
      "104/115 [==========================>...] - ETA: 0s - loss: 0.0169\n",
      "Epoch 103: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0172 - val_loss: 0.2501\n",
      "Epoch 104/200\n",
      "102/115 [=========================>....] - ETA: 0s - loss: 0.0162\n",
      "Epoch 104: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0166 - val_loss: 0.2396\n",
      "Epoch 105/200\n",
      "104/115 [==========================>...] - ETA: 0s - loss: 0.0171\n",
      "Epoch 105: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0168 - val_loss: 0.2507\n",
      "Epoch 106/200\n",
      "100/115 [=========================>....] - ETA: 0s - loss: 0.0149\n",
      "Epoch 106: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.2619\n",
      "Epoch 107/200\n",
      "102/115 [=========================>....] - ETA: 0s - loss: 0.0327\n",
      "Epoch 107: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0314 - val_loss: 0.2671\n",
      "Epoch 108/200\n",
      "104/115 [==========================>...] - ETA: 0s - loss: 0.0286\n",
      "Epoch 108: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0289 - val_loss: 0.2430\n",
      "Epoch 109/200\n",
      "102/115 [=========================>....] - ETA: 0s - loss: 0.0418\n",
      "Epoch 109: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0418 - val_loss: 0.2491\n",
      "Epoch 110/200\n",
      " 97/115 [========================>.....] - ETA: 0s - loss: 0.0375\n",
      "Epoch 110: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0360 - val_loss: 0.2472\n",
      "Epoch 111/200\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.0298\n",
      "Epoch 111: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0297 - val_loss: 0.2436\n",
      "Epoch 112/200\n",
      " 97/115 [========================>.....] - ETA: 0s - loss: 0.0252\n",
      "Epoch 112: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0246 - val_loss: 0.2464\n",
      "Epoch 113/200\n",
      "104/115 [==========================>...] - ETA: 0s - loss: 0.0230\n",
      "Epoch 113: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.2483\n",
      "Epoch 114/200\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.0220\n",
      "Epoch 114: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.2588\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train, validation_data=(X_test,y_test), epochs=200, batch_size=10, callbacks=[early_stopping_callback,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "e42e0a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss of test set\n",
    "y_vloss = history.history['val_loss']\n",
    "# loss of training set\n",
    "y_loss = history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "52ed14d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss of test set\n",
    "y_vloss = history.history['val_loss']\n",
    "# loss of training set\n",
    "y_loss = history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "ff6fd236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [52.93877410888672,\n",
       "  2.531266450881958,\n",
       "  1.415804147720337,\n",
       "  1.072220802307129,\n",
       "  0.8748476505279541,\n",
       "  0.7403120398521423,\n",
       "  0.621107280254364,\n",
       "  0.5393144488334656,\n",
       "  0.47571030259132385,\n",
       "  0.41207951307296753,\n",
       "  0.3652612864971161,\n",
       "  0.32559388875961304,\n",
       "  0.3158716559410095,\n",
       "  0.2665134370326996,\n",
       "  0.24888798594474792,\n",
       "  0.22370178997516632,\n",
       "  0.20095960795879364,\n",
       "  0.18349283933639526,\n",
       "  0.1794971227645874,\n",
       "  0.15784239768981934,\n",
       "  0.1446932703256607,\n",
       "  0.13413041830062866,\n",
       "  0.11865273118019104,\n",
       "  0.11305079609155655,\n",
       "  0.10659519582986832,\n",
       "  0.09905378520488739,\n",
       "  0.08894309401512146,\n",
       "  0.08534999936819077,\n",
       "  0.07503041625022888,\n",
       "  0.07232360541820526,\n",
       "  0.07289407402276993,\n",
       "  0.06184305250644684,\n",
       "  0.05809566006064415,\n",
       "  0.05477094650268555,\n",
       "  0.055542394518852234,\n",
       "  0.04630221426486969,\n",
       "  0.04752421751618385,\n",
       "  0.04169488325715065,\n",
       "  0.03869469836354256,\n",
       "  0.03684030845761299,\n",
       "  0.03530373424291611,\n",
       "  0.04130774363875389,\n",
       "  0.03669031709432602,\n",
       "  0.02977844700217247,\n",
       "  0.030184993520379066,\n",
       "  0.03047564998269081,\n",
       "  0.037847261875867844,\n",
       "  0.02881346084177494,\n",
       "  0.027662286534905434,\n",
       "  0.02694944106042385,\n",
       "  0.027679376304149628,\n",
       "  0.02858535200357437,\n",
       "  0.03695136308670044,\n",
       "  0.03665623441338539,\n",
       "  0.03153211250901222,\n",
       "  0.02337658777832985,\n",
       "  0.022340554744005203,\n",
       "  0.020424524322152138,\n",
       "  0.02844569832086563,\n",
       "  0.02862153761088848,\n",
       "  0.02872779592871666,\n",
       "  0.0265844464302063,\n",
       "  0.028807276859879494,\n",
       "  0.03394472226500511,\n",
       "  0.044316425919532776,\n",
       "  0.04760553687810898,\n",
       "  0.03466359153389931,\n",
       "  0.030472218990325928,\n",
       "  0.020166240632534027,\n",
       "  0.026804914698004723,\n",
       "  0.02275756187736988,\n",
       "  0.020730091258883476,\n",
       "  0.023402100428938866,\n",
       "  0.02264145389199257,\n",
       "  0.02690926566720009,\n",
       "  0.03203427419066429,\n",
       "  0.029479550197720528,\n",
       "  0.019814539700746536,\n",
       "  0.023646574467420578,\n",
       "  0.024851135909557343,\n",
       "  0.04212172329425812,\n",
       "  0.03392619639635086,\n",
       "  0.028047790750861168,\n",
       "  0.031285401433706284,\n",
       "  0.03543955832719803,\n",
       "  0.015703028067946434,\n",
       "  0.016564330086112022],\n",
       " 'val_loss': [4.333220481872559,\n",
       "  1.8953425884246826,\n",
       "  1.4432822465896606,\n",
       "  1.2731479406356812,\n",
       "  1.0917490720748901,\n",
       "  0.9909048080444336,\n",
       "  0.9147690534591675,\n",
       "  0.8405444622039795,\n",
       "  0.8210709095001221,\n",
       "  0.7317041754722595,\n",
       "  0.6802774667739868,\n",
       "  0.6841565370559692,\n",
       "  0.6184942126274109,\n",
       "  0.5757433176040649,\n",
       "  0.5828835964202881,\n",
       "  0.5419465899467468,\n",
       "  0.5319950580596924,\n",
       "  0.5476365685462952,\n",
       "  0.49011358618736267,\n",
       "  0.4673435389995575,\n",
       "  0.48452773690223694,\n",
       "  0.4498513638973236,\n",
       "  0.4213322103023529,\n",
       "  0.4131365418434143,\n",
       "  0.4143045246601105,\n",
       "  0.3938181400299072,\n",
       "  0.3930957615375519,\n",
       "  0.3923683166503906,\n",
       "  0.3806867003440857,\n",
       "  0.3597477972507477,\n",
       "  0.3606206178665161,\n",
       "  0.34362176060676575,\n",
       "  0.35210609436035156,\n",
       "  0.3467947840690613,\n",
       "  0.3535388112068176,\n",
       "  0.34592846035957336,\n",
       "  0.32430604100227356,\n",
       "  0.32026395201683044,\n",
       "  0.3210739195346832,\n",
       "  0.32150140404701233,\n",
       "  0.3092835545539856,\n",
       "  0.3100610375404358,\n",
       "  0.3074738085269928,\n",
       "  0.2949441373348236,\n",
       "  0.30801913142204285,\n",
       "  0.2971896529197693,\n",
       "  0.29102808237075806,\n",
       "  0.2881207764148712,\n",
       "  0.2763412594795227,\n",
       "  0.2875116467475891,\n",
       "  0.28907284140586853,\n",
       "  0.27782654762268066,\n",
       "  0.3185720443725586,\n",
       "  0.28209051489830017,\n",
       "  0.2696264088153839,\n",
       "  0.2868923544883728,\n",
       "  0.2832331955432892,\n",
       "  0.26698142290115356,\n",
       "  0.26564013957977295,\n",
       "  0.2757507860660553,\n",
       "  0.2611534297466278,\n",
       "  0.2835979461669922,\n",
       "  0.24849054217338562,\n",
       "  0.2795729339122772,\n",
       "  0.25191089510917664,\n",
       "  0.267962783575058,\n",
       "  0.2816506028175354,\n",
       "  0.24925604462623596,\n",
       "  0.25519993901252747,\n",
       "  0.25455304980278015,\n",
       "  0.24190093576908112,\n",
       "  0.2525283098220825,\n",
       "  0.24340809881687164,\n",
       "  0.2553997039794922,\n",
       "  0.2507071793079376,\n",
       "  0.25089022517204285,\n",
       "  0.2353609949350357,\n",
       "  0.2408405840396881,\n",
       "  0.24476498365402222,\n",
       "  0.2808026075363159,\n",
       "  0.2448899745941162,\n",
       "  0.2593935430049896,\n",
       "  0.23854675889015198,\n",
       "  0.26086047291755676,\n",
       "  0.24062402546405792,\n",
       "  0.23979716002941132,\n",
       "  0.24625922739505768]}"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "7bcfe416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [40.60085678100586,\n",
       "  2.2854065895080566,\n",
       "  1.3931750059127808,\n",
       "  1.0503723621368408,\n",
       "  0.8409615159034729,\n",
       "  0.6980590224266052,\n",
       "  0.5812558531761169,\n",
       "  0.5017775893211365,\n",
       "  0.44884246587753296,\n",
       "  0.38420331478118896,\n",
       "  0.3430429995059967,\n",
       "  0.29882195591926575,\n",
       "  0.2961505353450775,\n",
       "  0.24765321612358093,\n",
       "  0.23367559909820557,\n",
       "  0.2090335637331009,\n",
       "  0.19235515594482422,\n",
       "  0.17759694159030914,\n",
       "  0.17084649205207825,\n",
       "  0.1461707502603531,\n",
       "  0.13542336225509644,\n",
       "  0.12670913338661194,\n",
       "  0.11961156874895096,\n",
       "  0.10961097478866577,\n",
       "  0.1020369753241539,\n",
       "  0.09247467666864395,\n",
       "  0.08314376324415207,\n",
       "  0.08489452302455902,\n",
       "  0.08416040986776352,\n",
       "  0.07643108814954758,\n",
       "  0.07261424511671066,\n",
       "  0.06903411448001862,\n",
       "  0.06296268105506897,\n",
       "  0.0560808926820755,\n",
       "  0.05896708369255066,\n",
       "  0.051750365644693375,\n",
       "  0.04946022480726242,\n",
       "  0.041940078139305115,\n",
       "  0.04481746256351471,\n",
       "  0.05379512533545494,\n",
       "  0.04159554839134216,\n",
       "  0.045909732580184937,\n",
       "  0.03988318145275116,\n",
       "  0.037026822566986084,\n",
       "  0.033287253230810165,\n",
       "  0.03148460388183594,\n",
       "  0.03706417977809906,\n",
       "  0.035655707120895386,\n",
       "  0.03195925056934357,\n",
       "  0.03297463804483414,\n",
       "  0.02789202705025673,\n",
       "  0.037588123232126236,\n",
       "  0.029626434668898582,\n",
       "  0.026744907721877098,\n",
       "  0.036232661455869675,\n",
       "  0.04418523982167244,\n",
       "  0.03481036052107811,\n",
       "  0.03724392503499985,\n",
       "  0.040108077228069305,\n",
       "  0.04698712378740311,\n",
       "  0.03630147874355316,\n",
       "  0.02557002380490303,\n",
       "  0.023057427257299423,\n",
       "  0.01942199282348156,\n",
       "  0.026042241603136063,\n",
       "  0.03175363317131996,\n",
       "  0.030783740803599358,\n",
       "  0.03186621516942978,\n",
       "  0.036504507064819336,\n",
       "  0.02789176255464554,\n",
       "  0.028676794841885567,\n",
       "  0.018813157454133034,\n",
       "  0.01735658198595047,\n",
       "  0.01870727725327015,\n",
       "  0.032126061618328094,\n",
       "  0.053451523184776306,\n",
       "  0.04988056421279907,\n",
       "  0.029042134061455727,\n",
       "  0.02237154357135296,\n",
       "  0.021107137203216553,\n",
       "  0.016315404325723648,\n",
       "  0.018937179818749428,\n",
       "  0.027190877124667168,\n",
       "  0.052669670432806015,\n",
       "  0.055292099714279175,\n",
       "  0.03407975658774376,\n",
       "  0.02224552445113659,\n",
       "  0.017706193029880524,\n",
       "  0.017184942960739136,\n",
       "  0.017079543322324753,\n",
       "  0.0247794259339571,\n",
       "  0.02140549197793007,\n",
       "  0.02712084725499153,\n",
       "  0.03031366690993309,\n",
       "  0.03698451444506645,\n",
       "  0.02530287392437458,\n",
       "  0.016163237392902374,\n",
       "  0.014719655737280846,\n",
       "  0.02491544373333454,\n",
       "  0.02219889871776104,\n",
       "  0.016785988584160805,\n",
       "  0.016064031049609184,\n",
       "  0.017233097925782204,\n",
       "  0.016572535037994385,\n",
       "  0.016775915399193764,\n",
       "  0.015211103484034538,\n",
       "  0.03136998414993286,\n",
       "  0.0289144329726696,\n",
       "  0.041763290762901306,\n",
       "  0.035958435386419296,\n",
       "  0.029672756791114807,\n",
       "  0.024604251608252525,\n",
       "  0.023056229576468468,\n",
       "  0.021595418453216553],\n",
       " 'val_loss': [3.6395416259765625,\n",
       "  1.7998100519180298,\n",
       "  1.3698447942733765,\n",
       "  1.2254173755645752,\n",
       "  1.0254878997802734,\n",
       "  0.9259834885597229,\n",
       "  0.8959627151489258,\n",
       "  0.799601137638092,\n",
       "  0.7629759311676025,\n",
       "  0.7331013679504395,\n",
       "  0.6601967811584473,\n",
       "  0.7390508055686951,\n",
       "  0.6015151143074036,\n",
       "  0.5833860635757446,\n",
       "  0.6085196137428284,\n",
       "  0.5574122667312622,\n",
       "  0.5488319993019104,\n",
       "  0.5315526723861694,\n",
       "  0.5079320073127747,\n",
       "  0.4692646563053131,\n",
       "  0.48543721437454224,\n",
       "  0.4711446166038513,\n",
       "  0.47062453627586365,\n",
       "  0.48138779401779175,\n",
       "  0.4868646264076233,\n",
       "  0.42230224609375,\n",
       "  0.4457574188709259,\n",
       "  0.48176708817481995,\n",
       "  0.4249970614910126,\n",
       "  0.4400877058506012,\n",
       "  0.40360668301582336,\n",
       "  0.3906557559967041,\n",
       "  0.3679552376270294,\n",
       "  0.39458438754081726,\n",
       "  0.3909749686717987,\n",
       "  0.3823501169681549,\n",
       "  0.3478468954563141,\n",
       "  0.3513907492160797,\n",
       "  0.36002886295318604,\n",
       "  0.36631935834884644,\n",
       "  0.33280834555625916,\n",
       "  0.34670794010162354,\n",
       "  0.36479562520980835,\n",
       "  0.32536938786506653,\n",
       "  0.33132052421569824,\n",
       "  0.3517189025878906,\n",
       "  0.33248135447502136,\n",
       "  0.36805641651153564,\n",
       "  0.3373611271381378,\n",
       "  0.33218392729759216,\n",
       "  0.3114088773727417,\n",
       "  0.3125654458999634,\n",
       "  0.33087071776390076,\n",
       "  0.3237558901309967,\n",
       "  0.34574124217033386,\n",
       "  0.30688929557800293,\n",
       "  0.31790295243263245,\n",
       "  0.30393946170806885,\n",
       "  0.319518506526947,\n",
       "  0.30760514736175537,\n",
       "  0.2868843674659729,\n",
       "  0.29484719038009644,\n",
       "  0.28911352157592773,\n",
       "  0.33331298828125,\n",
       "  0.2990104556083679,\n",
       "  0.3086962401866913,\n",
       "  0.3141857981681824,\n",
       "  0.3011743724346161,\n",
       "  0.29885998368263245,\n",
       "  0.29258325695991516,\n",
       "  0.27283331751823425,\n",
       "  0.2881370186805725,\n",
       "  0.27392876148223877,\n",
       "  0.3023938834667206,\n",
       "  0.28053104877471924,\n",
       "  0.3047468364238739,\n",
       "  0.31356656551361084,\n",
       "  0.27291229367256165,\n",
       "  0.28513801097869873,\n",
       "  0.27169740200042725,\n",
       "  0.2773183286190033,\n",
       "  0.27270689606666565,\n",
       "  0.2720913290977478,\n",
       "  0.293315589427948,\n",
       "  0.28304868936538696,\n",
       "  0.2958233654499054,\n",
       "  0.2544739842414856,\n",
       "  0.25353121757507324,\n",
       "  0.2514405846595764,\n",
       "  0.2549803853034973,\n",
       "  0.2605522572994232,\n",
       "  0.27895334362983704,\n",
       "  0.259734183549881,\n",
       "  0.25989675521850586,\n",
       "  0.25517913699150085,\n",
       "  0.24191023409366608,\n",
       "  0.253960520029068,\n",
       "  0.24197661876678467,\n",
       "  0.2736339271068573,\n",
       "  0.24653099477291107,\n",
       "  0.2497716248035431,\n",
       "  0.2404526025056839,\n",
       "  0.25010454654693604,\n",
       "  0.23964500427246094,\n",
       "  0.2506518065929413,\n",
       "  0.26190656423568726,\n",
       "  0.2671055793762207,\n",
       "  0.24301008880138397,\n",
       "  0.2490806132555008,\n",
       "  0.24716877937316895,\n",
       "  0.24355603754520416,\n",
       "  0.2464122772216797,\n",
       "  0.24825917184352875,\n",
       "  0.25879114866256714]}"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "0ce4ecd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### 2nd -1 trial #####\n",
    "# draw a graph\n",
    "x_len = np.arange(len(y_loss))\n",
    "plt.plot(x_len,y_vloss, marker='.', c='red', label='Testset_loss')\n",
    "plt.plot(x_len,y_loss, marker='.', c='blue', label='Trainset_loss')\n",
    "#------------------\n",
    "plt.xlim(-1,3)\n",
    "#------------------\n",
    "# labeling\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "b6c25558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### 2nd - 2 trial #####\n",
    "# draw a graph\n",
    "x_len = np.arange(len(y_loss))\n",
    "plt.plot(x_len,y_vloss, marker='.', c='magenta', label='Testset_loss')\n",
    "plt.plot(x_len,y_loss, marker='.', c='cyan', label='Trainset_loss')\n",
    "#------------------\n",
    "plt.xlim(-0.5,3)\n",
    "#------------------\n",
    "# labeling\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "994a249a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1149    12.049425\n",
       "233     11.402005\n",
       "1310    11.813037\n",
       "202     11.608245\n",
       "362     11.790565\n",
       "          ...    \n",
       "666     11.763692\n",
       "991     11.813037\n",
       "896     12.065281\n",
       "511     11.976666\n",
       "1248    12.132162\n",
       "Name: SalePrice(log), Length: 287, dtype: float64"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "1009249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_ = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "c184ab53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      12.049425\n",
       "1      11.402005\n",
       "2      11.813037\n",
       "3      11.608245\n",
       "4      11.790565\n",
       "         ...    \n",
       "282    11.763692\n",
       "283    11.813037\n",
       "284    12.065281\n",
       "285    11.976666\n",
       "286    12.132162\n",
       "Name: SalePrice(log), Length: 287, dtype: float64"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "c1eba419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 1ms/step\n",
      "actual value : 12.049424683420915, predicted value : 11.20196533203125\n",
      "RMSLE : 0.06714711959246733\n",
      "actual value : 11.402005077384885, predicted value : 11.205520629882812\n",
      "RMSLE : 0.015969884181608496\n",
      "actual value : 11.813037464800539, predicted value : 11.878381729125977\n",
      "RMSLE : 0.005086893928138103\n",
      "actual value : 11.608244735642321, predicted value : 11.644471168518066\n",
      "RMSLE : 0.002869057154430088\n",
      "actual value : 11.790564777297387, predicted value : 12.102509498596191\n",
      "RMSLE : 0.024095963392623876\n",
      "actual value : 11.652696102959753, predicted value : 12.014906883239746\n",
      "RMSLE : 0.02822502903968127\n",
      "actual value : 11.744045122410057, predicted value : 11.444244384765625\n",
      "RMSLE : 0.023805997511801902\n",
      "actual value : 11.914054519303125, predicted value : 12.145221710205078\n",
      "RMSLE : 0.017742171840128496\n",
      "actual value : 12.433212202169729, predicted value : 13.06308650970459\n",
      "RMSLE : 0.045823332371440983\n",
      "actual value : 12.614868865984631, predicted value : 12.513822555541992\n",
      "RMSLE : 0.0074494589119007415\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test).flatten()\n",
    "for i in range(10):\n",
    "    label = y_test_[i]\n",
    "    prediction = y_pred[i]\n",
    "    print('actual value : {}, predicted value : {}'.format(label,prediction))\n",
    "    #---------------------\n",
    "    log_y = np.log1p(label)\n",
    "    log_pred = np.log1p(prediction)\n",
    "    squared_error = (log_y - log_pred)**2 \n",
    "    rmsle = np.sqrt(np.mean(squared_error))\n",
    "    # print RMSLE value\n",
    "    print('RMSLE : {}'.format(rmsle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "2bd49ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 1ms/step\n",
      "actual value : 12.049424683420915, predicted value : 11.418205261230469\n",
      "RMSLE : 0.049580438684264205\n",
      "actual value : 11.402005077384885, predicted value : 11.806051254272461\n",
      "RMSLE : 0.03205953857840127\n",
      "actual value : 11.813037464800539, predicted value : 12.287766456604004\n",
      "RMSLE : 0.03638052452811369\n",
      "actual value : 11.608244735642321, predicted value : 11.782094955444336\n",
      "RMSLE : 0.013694452738536533\n",
      "actual value : 11.790564777297387, predicted value : 12.115583419799805\n",
      "RMSLE : 0.025093268309005712\n",
      "actual value : 11.652696102959753, predicted value : 12.331023216247559\n",
      "RMSLE : 0.05222352795630725\n",
      "actual value : 11.744045122410057, predicted value : 11.4934720993042\n",
      "RMSLE : 0.019857785841880027\n",
      "actual value : 11.914054519303125, predicted value : 12.26473617553711\n",
      "RMSLE : 0.02679277952140291\n",
      "actual value : 12.433212202169729, predicted value : 12.49976921081543\n",
      "RMSLE : 0.004942413868633366\n",
      "actual value : 12.614868865984631, predicted value : 12.794310569763184\n",
      "RMSLE : 0.01309363953780629\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test).flatten()\n",
    "for i in range(10):\n",
    "    label = y_test_[i]\n",
    "    prediction = y_pred[i]\n",
    "    print('actual value : {}, predicted value : {}'.format(label,prediction))\n",
    "    #---------------------\n",
    "    log_y = np.log1p(label)\n",
    "    log_pred = np.log1p(prediction)\n",
    "    squared_error = (log_y - log_pred)**2 \n",
    "    rmsle = np.sqrt(np.mean(squared_error))\n",
    "    # print RMSLE value\n",
    "    print('RMSLE : {}'.format(rmsle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "2a56fbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 1ms/step - loss: 0.2463\n",
      "Test Accuracy : 0.2463\n"
     ]
    }
   ],
   "source": [
    "print('Test Accuracy : %.4f' % (model.evaluate(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "0f83b7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 1ms/step - loss: 0.2588\n",
      "Test Accuracy : 0.2588\n"
     ]
    }
   ],
   "source": [
    "print('Test Accuracy : %.4f' % (model.evaluate(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "cd142071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MSSubClass(scaled)', 'MSZoning_(scaled)', 'LotFrontage(scaled)',\n",
       "       'LotArea(scaled)', 'LotShape_(scaled)', 'LotConfig_(scaled)',\n",
       "       'Neighborhood_(scaled)', 'HouseStyle_(scaled)', 'OverallQual(scaled)',\n",
       "       'OverallCond(scaled)', 'RoofStyle_(scaled)', 'MasVnrType_(scaled)',\n",
       "       'MasVnrArea(scaled)', 'ExterQual_(scaled)', 'Foundation_(scaled)',\n",
       "       'BsmtQual_(scaled)', 'BsmtExposure_(scaled)', 'BsmtFinType1_(scaled)',\n",
       "       'BsmtFinSF1(scaled)', 'BsmtUnfSF(scaled)', 'TotalBsmtSF(scaled)',\n",
       "       'HeatingQC_(scaled)', '1stFlrSF(scaled)', '2ndFlrSF(scaled)',\n",
       "       'GrLivArea(scaled)', 'BsmtFullBath(scaled)', 'FullBath(scaled)',\n",
       "       'HalfBath(scaled)', 'BedroomAbvGr(scaled)', 'KitchenQual_(scaled)',\n",
       "       'TotRmsAbvGrd(scaled)', 'Fireplaces(scaled)', 'FireplaceQu_(scaled)',\n",
       "       'GarageType_(scaled)', 'House_Age(scaled)', 'Garage_Age(scaled)',\n",
       "       'GarageFinish_(scaled)', 'GarageCars(scaled)', 'GarageArea(scaled)',\n",
       "       'WoodDeckSF(scaled)', 'OpenPorchSF(scaled)', 'Exterior_Avg(scaled)',\n",
       "       'SaleCondition_(scaled)', 'SalePrice(log)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_house4.columns[106:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "88614c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, 42)"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_house4.columns[106:]),len(df_house_test_extracted.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "3753d6a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MSSubClass', 'MSZoning_', 'LotFrontage', 'LotArea', 'LotShape_',\n",
       "       'LotConfig_', 'Neighborhood_', 'HouseStyle_', 'OverallQual',\n",
       "       'OverallCond', 'RoofStyle_', 'MasVnrType_', 'MasVnrArea', 'ExterQual_',\n",
       "       'Foundation_', 'BsmtQual_', 'BsmtExposure_', 'BsmtFinType1_',\n",
       "       'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF', 'HeatingQC_', '1stFlrSF',\n",
       "       '2ndFlrSF', 'GrLivArea', 'BsmtFullBath', 'FullBath', 'HalfBath',\n",
       "       'BedroomAbvGr', 'KitchenQual_', 'TotRmsAbvGrd', 'Fireplaces',\n",
       "       'FireplaceQu_', 'GarageType_', 'House_Age', 'Garage_Age',\n",
       "       'GarageFinish_', 'GarageCars', 'GarageArea', 'WoodDeckSF',\n",
       "       'OpenPorchSF', 'Exterior_Avg'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_house_test_extracted.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "3356b7f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n",
       "       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n",
       "       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
       "       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n",
       "       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n",
       "       'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n",
       "       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',\n",
       "       'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',\n",
       "       'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF',\n",
       "       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
       "       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',\n",
       "       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',\n",
       "       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',\n",
       "       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n",
       "       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',\n",
       "       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',\n",
       "       'SaleCondition', 'SaleCondition_'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_house_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "2afcd3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_house_test_extracted_ = pd.concat([df_house_test_extracted,df_house_test[['SaleCondition_']]],axis=1)\n",
    "#--------------------------------------------------------------------------\n",
    "col_lst = [\n",
    "        'MSSubClass', 'MSZoning_', 'LotFrontage', 'LotArea', 'LotShape_',\n",
    "       'LotConfig_', 'Neighborhood_', 'HouseStyle_', 'OverallQual',\n",
    "       'OverallCond', 'RoofStyle_', 'MasVnrType_', 'MasVnrArea', 'ExterQual_',\n",
    "       'Foundation_', 'BsmtQual_', 'BsmtExposure_', 'BsmtFinType1_',\n",
    "       'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF', 'HeatingQC_', '1stFlrSF',\n",
    "       '2ndFlrSF', 'GrLivArea', 'BsmtFullBath', 'FullBath', 'HalfBath',\n",
    "       'BedroomAbvGr', 'KitchenQual_', 'TotRmsAbvGrd', 'Fireplaces',\n",
    "       'FireplaceQu_', 'GarageType_', 'House_Age', 'Garage_Age',\n",
    "       'GarageFinish_', 'GarageCars', 'GarageArea', 'WoodDeckSF',\n",
    "       'OpenPorchSF', 'Exterior_Avg','SaleCondition_'\n",
    "] \n",
    "#--------------------------------------------------------------\n",
    "#------------\n",
    "# create column and contain standardized data from the columns above, in sequence\n",
    "for i in range(len(col_lst)):\n",
    "    # standardize the data, using the library\n",
    "    array_tst = df_house_test_extracted_[col_lst[i]].values.reshape(-1,1)\n",
    "    scaled_data_tst = scaler.fit_transform(array_tst) \n",
    "    # making new column's name\n",
    "    new_col = col_lst[i] + '(scaled)'\n",
    "    # create column for containing the standardized data\n",
    "    df_house_test_extracted_[new_col] = scaled_data_tst\n",
    "###====================================================================\n",
    "###===================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "8e4eb1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MSSubClass', 'MSZoning_', 'LotFrontage', 'LotArea', 'LotShape_',\n",
       "       'LotConfig_', 'Neighborhood_', 'HouseStyle_', 'OverallQual',\n",
       "       'OverallCond', 'RoofStyle_', 'MasVnrType_', 'MasVnrArea', 'ExterQual_',\n",
       "       'Foundation_', 'BsmtQual_', 'BsmtExposure_', 'BsmtFinType1_',\n",
       "       'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF', 'HeatingQC_', '1stFlrSF',\n",
       "       '2ndFlrSF', 'GrLivArea', 'BsmtFullBath', 'FullBath', 'HalfBath',\n",
       "       'BedroomAbvGr', 'KitchenQual_', 'TotRmsAbvGrd', 'Fireplaces',\n",
       "       'FireplaceQu_', 'GarageType_', 'House_Age', 'Garage_Age',\n",
       "       'GarageFinish_', 'GarageCars', 'GarageArea', 'WoodDeckSF',\n",
       "       'OpenPorchSF', 'Exterior_Avg', 'SaleCondition_', 'MSSubClass(scaled)',\n",
       "       'MSZoning_(scaled)', 'LotFrontage(scaled)', 'LotArea(scaled)',\n",
       "       'LotShape_(scaled)', 'LotConfig_(scaled)', 'Neighborhood_(scaled)',\n",
       "       'HouseStyle_(scaled)', 'OverallQual(scaled)', 'OverallCond(scaled)',\n",
       "       'RoofStyle_(scaled)', 'MasVnrType_(scaled)', 'MasVnrArea(scaled)',\n",
       "       'ExterQual_(scaled)', 'Foundation_(scaled)', 'BsmtQual_(scaled)',\n",
       "       'BsmtExposure_(scaled)', 'BsmtFinType1_(scaled)', 'BsmtFinSF1(scaled)',\n",
       "       'BsmtUnfSF(scaled)', 'TotalBsmtSF(scaled)', 'HeatingQC_(scaled)',\n",
       "       '1stFlrSF(scaled)', '2ndFlrSF(scaled)', 'GrLivArea(scaled)',\n",
       "       'BsmtFullBath(scaled)', 'FullBath(scaled)', 'HalfBath(scaled)',\n",
       "       'BedroomAbvGr(scaled)', 'KitchenQual_(scaled)', 'TotRmsAbvGrd(scaled)',\n",
       "       'Fireplaces(scaled)', 'FireplaceQu_(scaled)', 'GarageType_(scaled)',\n",
       "       'House_Age(scaled)', 'Garage_Age(scaled)', 'GarageFinish_(scaled)',\n",
       "       'GarageCars(scaled)', 'GarageArea(scaled)', 'WoodDeckSF(scaled)',\n",
       "       'OpenPorchSF(scaled)', 'Exterior_Avg(scaled)',\n",
       "       'SaleCondition_(scaled)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_house_test_extracted_.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe23eef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "6031e126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>171624.828125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>168963.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>324405.468750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>281467.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>148777.015625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2915</td>\n",
       "      <td>103821.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2916</td>\n",
       "      <td>80317.828125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2917</td>\n",
       "      <td>248136.578125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2918</td>\n",
       "      <td>171947.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2919</td>\n",
       "      <td>354651.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id      SalePrice\n",
       "0     1461  171624.828125\n",
       "1     1462  168963.531250\n",
       "2     1463  324405.468750\n",
       "3     1464  281467.343750\n",
       "4     1465  148777.015625\n",
       "...    ...            ...\n",
       "1454  2915  103821.000000\n",
       "1455  2916   80317.828125\n",
       "1456  2917  248136.578125\n",
       "1457  2918  171947.906250\n",
       "1458  2919  354651.750000\n",
       "\n",
       "[1459 rows x 2 columns]"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###================================================\n",
    "df_house_test_extracted2b_predicted = df_house_test_extracted_[[\n",
    "    'MSSubClass(scaled)', 'MSZoning_(scaled)',\n",
    "       'LotFrontage(scaled)', 'LotArea(scaled)', 'LotShape_(scaled)',\n",
    "       'LotConfig_(scaled)', 'Neighborhood_(scaled)', 'HouseStyle_(scaled)',\n",
    "       'OverallQual(scaled)', 'OverallCond(scaled)', 'RoofStyle_(scaled)',\n",
    "       'MasVnrType_(scaled)', 'MasVnrArea(scaled)', 'ExterQual_(scaled)',\n",
    "       'Foundation_(scaled)', 'BsmtQual_(scaled)', 'BsmtExposure_(scaled)',\n",
    "       'BsmtFinType1_(scaled)', 'BsmtFinSF1(scaled)', 'BsmtUnfSF(scaled)',\n",
    "       'TotalBsmtSF(scaled)', 'HeatingQC_(scaled)', '1stFlrSF(scaled)',\n",
    "       '2ndFlrSF(scaled)', 'GrLivArea(scaled)', 'BsmtFullBath(scaled)',\n",
    "       'FullBath(scaled)', 'HalfBath(scaled)', 'BedroomAbvGr(scaled)',\n",
    "       'KitchenQual_(scaled)', 'TotRmsAbvGrd(scaled)', 'Fireplaces(scaled)',\n",
    "       'FireplaceQu_(scaled)', 'GarageType_(scaled)', 'House_Age(scaled)',\n",
    "       'Garage_Age(scaled)', 'GarageFinish_(scaled)', 'GarageCars(scaled)',\n",
    "       'GarageArea(scaled)', 'WoodDeckSF(scaled)', 'OpenPorchSF(scaled)',\n",
    "       'Exterior_Avg(scaled)', 'SaleCondition_(scaled)'   \n",
    "]]\n",
    "\n",
    "result_from_model = model.predict(df_house_test_extracted2b_predicted)\n",
    "df_house_test['SalePrice'] = 0\n",
    "df_house_test['SalePrice'] = np.expm1(result_from_model)\n",
    "df_house_test[['Id','SalePrice']].to_csv('C:/Users/thsong/submission15_1.csv',index=False)\n",
    "df_house_test[['Id','SalePrice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "de8e4d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>325292.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>187595.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>256414.281250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>482726.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>145303.265625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2915</td>\n",
       "      <td>37518.449219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2916</td>\n",
       "      <td>94276.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2917</td>\n",
       "      <td>353294.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2918</td>\n",
       "      <td>89503.507812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2919</td>\n",
       "      <td>393721.968750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id      SalePrice\n",
       "0     1461  325292.437500\n",
       "1     1462  187595.000000\n",
       "2     1463  256414.281250\n",
       "3     1464  482726.125000\n",
       "4     1465  145303.265625\n",
       "...    ...            ...\n",
       "1454  2915   37518.449219\n",
       "1455  2916   94276.750000\n",
       "1456  2917  353294.687500\n",
       "1457  2918   89503.507812\n",
       "1458  2919  393721.968750\n",
       "\n",
       "[1459 rows x 2 columns]"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_from_model = model.predict(df_house_test_extracted2b_predicted)\n",
    "df_house_test['SalePrice'] = 0\n",
    "df_house_test['SalePrice'] = np.expm1(result_from_model)\n",
    "df_house_test[['Id','SalePrice']].to_csv('C:/Users/thsong/submission15_2.csv',index=False)\n",
    "df_house_test[['Id','SalePrice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119dc50f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ffd71e0",
   "metadata": {},
   "source": [
    "### nov09.2022 3rd trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "ff2cabb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "b6d33871",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "#--------------------------------------------\n",
    "model3.add(Dense(128, input_shape=(43,)))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(Dropout(0.5))\n",
    "#----------------------------------\n",
    "model3.add(Dense(64))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(Dropout(0.5))\n",
    "#----------------------------------\n",
    "model3.add(Dense(16))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(Dropout(0.5))\n",
    "#----------------------------------\n",
    "model3.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "9c4a21e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(loss='mean_squared_error', metrics=['mse'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "d12d378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "4198f755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "103/115 [=========================>....] - ETA: 0s - loss: 73.0300 - mse: 73.0300 \n",
      "Epoch 1: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 1s 3ms/step - loss: 70.0254 - mse: 70.0254 - val_loss: 17.3411 - val_mse: 17.3411\n",
      "Epoch 2/200\n",
      " 99/115 [========================>.....] - ETA: 0s - loss: 33.3807 - mse: 33.3807\n",
      "Epoch 2: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 32.8859 - mse: 32.8859 - val_loss: 7.4582 - val_mse: 7.4582\n",
      "Epoch 3/200\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 25.5984 - mse: 25.5984\n",
      "Epoch 3: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 25.3744 - mse: 25.3744 - val_loss: 5.0611 - val_mse: 5.0611\n",
      "Epoch 4/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 24.9384 - mse: 24.9384\n",
      "Epoch 4: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 24.6645 - mse: 24.6645 - val_loss: 8.8970 - val_mse: 8.8970\n",
      "Epoch 5/200\n",
      " 86/115 [=====================>........] - ETA: 0s - loss: 24.5684 - mse: 24.5684\n",
      "Epoch 5: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 23.8813 - mse: 23.8813 - val_loss: 6.1396 - val_mse: 6.1396\n",
      "Epoch 6/200\n",
      " 95/115 [=======================>......] - ETA: 0s - loss: 24.8796 - mse: 24.8796\n",
      "Epoch 6: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 24.4728 - mse: 24.4728 - val_loss: 5.7839 - val_mse: 5.7839\n",
      "Epoch 7/200\n",
      " 92/115 [=======================>......] - ETA: 0s - loss: 22.4227 - mse: 22.4227\n",
      "Epoch 7: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 22.0210 - mse: 22.0210 - val_loss: 9.3269 - val_mse: 9.3269\n",
      "Epoch 8/200\n",
      " 93/115 [=======================>......] - ETA: 0s - loss: 20.2690 - mse: 20.2690\n",
      "Epoch 8: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 20.9838 - mse: 20.9838 - val_loss: 7.4545 - val_mse: 7.4545\n",
      "Epoch 9/200\n",
      " 98/115 [========================>.....] - ETA: 0s - loss: 21.0916 - mse: 21.0916\n",
      "Epoch 9: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 21.3801 - mse: 21.3801 - val_loss: 8.1335 - val_mse: 8.1335\n",
      "Epoch 10/200\n",
      " 93/115 [=======================>......] - ETA: 0s - loss: 19.6971 - mse: 19.6971\n",
      "Epoch 10: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 19.6487 - mse: 19.6487 - val_loss: 5.6240 - val_mse: 5.6240\n",
      "Epoch 11/200\n",
      " 94/115 [=======================>......] - ETA: 0s - loss: 17.2812 - mse: 17.2812\n",
      "Epoch 11: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 17.0735 - mse: 17.0735 - val_loss: 5.3456 - val_mse: 5.3456\n",
      "Epoch 12/200\n",
      " 98/115 [========================>.....] - ETA: 0s - loss: 16.9405 - mse: 16.9405\n",
      "Epoch 12: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 17.3423 - mse: 17.3423 - val_loss: 4.9452 - val_mse: 4.9452\n",
      "Epoch 13/200\n",
      " 98/115 [========================>.....] - ETA: 0s - loss: 18.4571 - mse: 18.4571\n",
      "Epoch 13: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 18.5504 - mse: 18.5504 - val_loss: 7.1352 - val_mse: 7.1352\n",
      "Epoch 14/200\n",
      " 94/115 [=======================>......] - ETA: 0s - loss: 17.6610 - mse: 17.6610\n",
      "Epoch 14: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 16.9213 - mse: 16.9213 - val_loss: 3.2211 - val_mse: 3.2211\n",
      "Epoch 15/200\n",
      " 94/115 [=======================>......] - ETA: 0s - loss: 16.7429 - mse: 16.7429\n",
      "Epoch 15: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 16.4648 - mse: 16.4648 - val_loss: 3.2977 - val_mse: 3.2977\n",
      "Epoch 16/200\n",
      " 96/115 [========================>.....] - ETA: 0s - loss: 16.4575 - mse: 16.4575\n",
      "Epoch 16: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 16.2106 - mse: 16.2106 - val_loss: 2.3098 - val_mse: 2.3098\n",
      "Epoch 17/200\n",
      " 95/115 [=======================>......] - ETA: 0s - loss: 15.2752 - mse: 15.2752\n",
      "Epoch 17: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 15.7296 - mse: 15.7296 - val_loss: 6.0763 - val_mse: 6.0763\n",
      "Epoch 18/200\n",
      " 97/115 [========================>.....] - ETA: 0s - loss: 16.0047 - mse: 16.0047\n",
      "Epoch 18: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 16.3512 - mse: 16.3512 - val_loss: 6.3215 - val_mse: 6.3215\n",
      "Epoch 19/200\n",
      " 97/115 [========================>.....] - ETA: 0s - loss: 14.6039 - mse: 14.6039\n",
      "Epoch 19: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 14.4779 - mse: 14.4779 - val_loss: 6.2989 - val_mse: 6.2989\n",
      "Epoch 20/200\n",
      " 95/115 [=======================>......] - ETA: 0s - loss: 15.0740 - mse: 15.0740\n",
      "Epoch 20: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 14.9886 - mse: 14.9886 - val_loss: 3.9618 - val_mse: 3.9618\n",
      "Epoch 21/200\n",
      " 93/115 [=======================>......] - ETA: 0s - loss: 13.3997 - mse: 13.3997\n",
      "Epoch 21: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 13.5543 - mse: 13.5543 - val_loss: 3.8519 - val_mse: 3.8519\n",
      "Epoch 22/200\n",
      " 91/115 [======================>.......] - ETA: 0s - loss: 13.8792 - mse: 13.8792\n",
      "Epoch 22: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 13.7962 - mse: 13.7962 - val_loss: 3.0764 - val_mse: 3.0764\n",
      "Epoch 23/200\n",
      " 95/115 [=======================>......] - ETA: 0s - loss: 14.2082 - mse: 14.2082\n",
      "Epoch 23: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 14.0039 - mse: 14.0039 - val_loss: 5.0234 - val_mse: 5.0234\n",
      "Epoch 24/200\n",
      " 92/115 [=======================>......] - ETA: 0s - loss: 12.9832 - mse: 12.9832\n",
      "Epoch 24: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 12.9688 - mse: 12.9688 - val_loss: 3.8344 - val_mse: 3.8344\n",
      "Epoch 25/200\n",
      " 92/115 [=======================>......] - ETA: 0s - loss: 12.4364 - mse: 12.4364\n",
      "Epoch 25: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 12.5849 - mse: 12.5849 - val_loss: 5.0357 - val_mse: 5.0357\n",
      "Epoch 26/200\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 12.7306 - mse: 12.7306\n",
      "Epoch 26: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 12.7749 - mse: 12.7749 - val_loss: 3.1312 - val_mse: 3.1312\n"
     ]
    }
   ],
   "source": [
    "history2 = model3.fit(X_train,y_train, validation_data=(X_test,y_test), epochs=200, batch_size=10, callbacks=[early_stopping_callback,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "eae96d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss of test set\n",
    "y_vloss2 = history2.history['val_loss']\n",
    "# loss of training set\n",
    "y_loss2 = history2.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "617c8e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEGCAYAAABiq/5QAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA2tElEQVR4nO3deXhU5fXA8e9JCIR9N6wWUKSVtYILP1yCFFCpFa2orQvu2rrgDmpt3aHu+4KKUkXRYhXrhmgTKZK2guKKgiKyKChbIAghy/n9cWZIgEAmk7mZZO75PM99ZsnMve87Mzn3ve9933NFVXHOORceackugHPOuZrlgd8550LGA79zzoWMB37nnAsZD/zOORcy9ZJdgFi0adNGu3TpEtd7N23aROPGjRNboFrO6xwOXudwqE6d582bt1pV2+74fJ0I/F26dGHu3LlxvTc3N5fs7OzEFqiW8zqHg9c5HKpTZxH5tqLnvavHOedCxgO/c86FjAd+55wLmTrRx++cq32KiopYvnw5W7ZsqbFtNm/enAULFtTY9mqDWOqcmZlJp06dyMjIiGmdHvidc3FZvnw5TZs2pUuXLohIjWxz48aNNG3atEa2VVtUVmdVZc2aNSxfvpyuXbvGtM7AunpEpIeIzC+3bBCRS0SklYjMFJFFkduWQZXBORecLVu20Lp16xoL+q5iIkLr1q2rdOQVWOBX1S9VtZ+q9gP6Az8BLwHjgHdUtTvwTuRxIPLyYMqUPcnLC2oLzoWbB/3aoarfQ02d3B0CfK2q3wLHAJMjz08GRgaxwbw8yM6GJ57oypAhePB3zrmImurjPwl4LnI/S1W/B1DV70Vkj4reICLnAucCZGVlkZubW6UNTpmyJ1u3dgWEwsJSJk1aQmHh0njLX6cUFBRU+fOq67zONa958+Zs3LixRrdZUlJS49tMtljrvGXLlth/D6oa6ALUB1ZjAR9g/Q5/X1fZOvr3769VNWeOakaGKqg2bGiPwyInJyfZRahxXuea9/nnn9f4Njds2LDt/urVq7Vv377at29fzcrK0g4dOmx7XFhYWOm6cnJy9L333ourHN98841OmTKl0vWPGDEirvWXV77Ou1PR9wHM1Qpiak109RwJfKCqqyKPV4lIe4DI7Q9BbHTgQPjzn+3+xIn22DmXZHl5MH58QvpeW7duzfz585k/fz7nn38+l1566bbH9evXr/T9ubm5zJkzJ65tL1myhGeffTau99YGNdHV8zvKunkAXgFGAxMit9OD2vCQIXDdddC6dVBbcM4BcMklMH/+7l+Tnw8ffwylpZCWBn36QPPmu359v35wzz1VKsa8efO47LLLKCgooE2bNjz11FO0b9+e++67j0ceeYR69eqx7777MmHCBB555BHS09N55plnuP/++1m5ciU33HAD6enpNG/enFmzZlFSUsK4cePIzc2lsLCQCy64gPPOO49x48axYMEC+vXrx+jRo7n00kt3W661a9dy5plnsnjxYho1asTEiRPp06cP7777LmPGjAHsBO2sWbMoKCjgxBNPZMOGDRQXF3PnnXcyfPjwKn0OlQk08ItII2AocF65pycAL4jIWcBSYFRQ2+/c2W6XLQtqC865mOXnW9AHu83P333gryJV5aKLLmL69Om0bduW559/nmuvvZZJkyYxYcIEvvnmGxo0aMD69etp0aIF559/Pk2aNOGKK64AoHfv3syYMYOOHTuyfv16AJ544gmaN2/O+++/T2FhIYMGDWLYsGFMmDCBO+64g1dffTWmsv3lL3/hl7/8JS+//DL/+te/OO2005g/fz533HEHDz74IIMGDaKgoIDMzEwmTpzI8OHDufbaaykpKWHVqlWVb6CKAg38qvoT0HqH59Zgo3wC164dpKUpy5b5kDPnAhVLyzwvzw7Dt26F+vVhypSE9sEWFhby6aefMnToUMBOirZv3x6APn36cPLJJzNy5EhGjhxZ4fsHDRrE6aefzgknnMBxxx0HwFtvvcXHH3/MtGnTAMjPz2fRokUxdSWVN3v2bF588UUADj/8cNasWUN+fj6DBg3isssu4+STT+a4446jU6dO7L///px55pkUFRUxcuRI9tprr3g+jt1K6Vw99epB69aF3uJ3rjYYOBDeeQduusluE3ziTVXp2bPntn7+Tz75hLfeeguA1157jQsuuIB58+bRv39/iouLd3r/I488ws0338yyZcvo168fa9asQVW5//77t63zm2++YdiwYXGVbUciwrhx43j88cfZvHkzBx10EF988QWHHnoos2bNomPHjpx66qmBnEtI6cAPsMcehSxfnuxSOOcAC/ZXXx3IaIsGDRrw448/khc5cVxUVMRnn31GaWkpy5YtY/Dgwdx2222sX7+egoICmjZtut0wya+//poDDzyQG2+8kTZt2rBs2TKGDx/Oww8/TFFREQALFy5k06ZNO723MoceeihTpkwB7KRymzZtaNasGV9//TW9e/dm7NixDBgwgC+++IJvv/2WPfbYg3POOYezzjqLjz76KIGfkkn5XD1t2niL37kwSEtLY9q0aVx88cXk5+dTXFzMJZdcwj777MMpp5xCfn4+qsqll15KixYtOProozn++OOZPn06999/P3fffTeLFi1CVRkyZAh9+/alT58+LFmyhP322w9VpW3btrz88sv06dOHevXq0bdvX04//fRKT+5ef/31nHHGGfTp04dGjRoxebLNYb3nnnvIyckhPT2dfffdlyOPPJKpU6dy++23k5GRQZMmTXjooYcS/llJRYcgtc2AAQM03itwnXDCMl59tTObNkFYZpf7VYrCIdl1XrBgAb/4xS9qdJuepG3XKvo+RGSeqg7Y8bWh6OrZvBnWrk12SZxzrnZI+a6etm0tY93y5T6e3zmXeDNmzGDs2LHbPde1a1deeumlJJWociEI/IWAjeXv2zfJhXHOpZzhw4cnfIJV0ELR1QM+ics556JSPvC3bLmVevU88DvnXFTKB/70dOjYER/L75xzESkf+AE6dfIWv3PORYUi8Hfu7IHfuVSzZs0a+vXrR79+/WjXrh0dO3bc9njr1q27fe/cuXO5+OKLE1qep556iu+++263r8nOzibeOUmJlPKjesAC/0svgWp4JnE5Vxvl5UFurl0WtbpZG6L5+MFmxpbPtAlQXFxMvXoVh7gBAwYwYMBO85qq5amnnqJXr1506NAhoesNQmgCf2EhrF4NbdsmuzTOpZ5ako6f008/nVatWvHhhx+y3377ceKJJ3LJJZewefNmGjZsyJNPPkmPHj3Izc3dllb5+uuvZ+nSpSxevJilS5dyySWXcPHFF7Np0yZOOOEEli9fTklJCddddx0nnnhihTn/33vvPebOncvJJ59Mw4YNycvLo2HDhrst63PPPcett96KqjJixAj++te/UlJSwllnncXcuXMREc4880zOPvvsna4nMHXq1Kp9MDsIReDv1Mluly3zwO9csgScjn+bhQsX8vbbb5Oens6GDRuYNWsW9erV4+233+aaa67Zlh65vC+++IKcnBw2btxIjx49+MMf/sCbb75Jhw4deO211yLlz6eoqGiXOf8feOAB7rjjjpiOJL777jvGjh3LvHnzaNmyJcOGDePll1+mc+fOrFixgk8//RRg23UBdryeQHWFIvCXvyDLfvsltyzOpaJakI5/m1GjRpGeng5YsB49ejSLFi1CRLZl2dzRiBEjaNCgAQ0aNGCPPfZg1apV9O7dmyuuuIKxY8fy61//mkMOOYRPP/10lzn/q+L9998nOzubtpGW6Mknn8ysWbO47rrrWLx4MRdddBEjRoxg2LBhbNq0KabrCVRFaE7ugp/gdS6ZAk7Hv03jxo233b/uuusYPHgwn376Kf/85z/ZsmVLhe9p0KDBtvvp6ekUFxezzz77MG/ePHr37s3VV1/NjTfeuNuc/1Wxq+SYLVu25KOPPiI7O5sHH3yQs88+G4jtegJVEYrA37attTB8LL9zyRVgOv4K5efn07FjR8BOvlbFd999R6NGjTjllFO44oor+OCDD+jRo0eFOf+BKuXoP/DAA3n33XdZvXo1JSUlPPfccxx22GGsXr2a0tJSfvvb33LTTTfxwQcf7PJ6AtURiq6etDSbxOUtfufC5aqrrmL06NHcddddHH744VV67yeffMKVV15JWloaGRkZPPzww9SvX7/CnP89e/bk9NNP5/zzz4/p5G779u0ZP348gwcPRlU56qijOOaYY/joo48444wzKI2cDBk/fjwlJSUVXk+gOlI+H380Z/lhh9lwzlmzEly4WijZedqTwetc8zwff82oc/n4RaSFiEwTkS9EZIGIDBSRViIyU0QWRW5bBlmGKJ/E5ZxzJug+/nuBN1X150BfYAEwDnhHVbsD70QeB65zZ1ixomw4mXPOBeXYY4/dNos4usyYMSPZxdomsD5+EWkGHAqcDqCqW4GtInIMkB152WQgFxi78xoSq1MnKCqCH36Adu2C3ppz4aCqiE+H30lNX4Slql32Qbb4uwE/Ak+KyIci8riINAayVPV7gMjtHgGWYRsf0ulcYmVmZrJmzZoqBx2XWKrKmjVryMzMjPk9QY7qqQfsB1ykqv8VkXupQreOiJwLnAuQlZVFbm5uXIUoKCggNzeXVauaAAN4441P2bRpdVzrqiuidQ4Tr3PNExEaN27MshpsTYXxCCOWOpeUlLBp0ya+/fbb2FcaxAK0A5aUe3wI8BrwJdA+8lx74MvK1tW/f3+NV05Ojqqq/vijKqjee2/cq6ozonUOE69zOHidqwaYqxXE1MC6elR1JbBMRHpEnhoCfA68AoyOPDcamB5UGcpr3RoyM72rxznngp7AdREwRUTqA4uBM7DzCi+IyFnAUmBUwGUALB2zX5DFOecCDvyqOh+oKFXdkCC3uys+lt8550KSqyeqc2fP1+Occ6EK/J062SSukpJkl8Q555InVIG/c2cL+itXJrskzjmXPKEL/OD9/M65cAtl4Pd+fudcmIUq8Je/9q5zzoVVqAJ/y5bQqJEHfudcuIUq8Iv4WH7nnAtV4Acfy++cc6EL/J62wTkXdqEL/J07w/ffQ3FxskvinHPJEcrAX1oK332X7JI451xyhDLwg/fzO+fCK3SB38fyO+fCLnSB39M2OOfCLnSBv3lzaNrUA79zLrxCF/jBx/I758ItlIHfx/I758IslIHf0zY458IstIF/1SrYujXZJXHOuZoX2sCv6pO4nHPhFGjgF5ElIvKJiMwXkbmR51qJyEwRWRS5bRlkGSriY/mdc2FWEy3+waraT1UHRB6PA95R1e7AO5HHNcrH8jvnwiwZXT3HAJMj9ycDI2u6AB74nXNhJqoa3MpFvgHWAQo8qqoTRWS9qrYo95p1qrpTd4+InAucC5CVldV/6tSpcZWhoKCAJk2a7PT80UcPYujQVVx88Vdxrbc221WdU5nXORy8zlUzePDgeeV6W8qoamAL0CFyuwfwEXAosH6H16yrbD39+/fXeOXk5FT4fK9eqsccE/dqa7Vd1TmVeZ3DwetcNcBcrSCmBtrVo6rfRW5/AF4CDgBWiUh7gMjtD0GWYVd8LL9zLqwCC/wi0lhEmkbvA8OAT4FXgNGRl40GpgdVht3xwO+cC6t6Aa47C3hJRKLbeVZV3xSR94EXROQsYCkwKsAy7FLnzvDjj7BlC2RmJqMEzjmXHIEFflVdDPSt4Pk1wJCgthur6Fj+FStgr72SWxbnnKtJoZy5Cz6k0zkXXh74PfA750ImtIE/2tXjefmdc2ET2sDfqBG0auUtfudc+IQ28IMP6XTOhZMHfg/8zrmQCX3g9z5+51zYhDrwd+oEa9bATz8luyTOOVdzQh34o0M6vdXvnAsTD/x4P79zLlw88OMtfudcuIQ68HfsaLfe4nfOhUmoA39mJrRt64HfORcuoQ784GP5nXPh44Hfx/I750Im9IG/Uydv8TvnwiX0gb9zZ1i/HgoKkl0S55yrGR74fSy/cy5kQh/4PS+/cy5sQh/4vcXvnAubwAO/iKSLyIci8mrkcSsRmSkiiyK3LYMuw+507AgiHvidc+FREy3+McCCco/HAe+oanfgncjjpKlfH7KyPPA758Ij0MAvIp2AEcDj5Z4+BpgcuT8ZGBlkGWLRqZP38TvnwkNUNbiVi0wDxgNNgStU9dcisl5VW5R7zTpV3am7R0TOBc4FyMrK6j916tS4ylBQUECTJk12+5o//7knS5c24qmn3o9rG7VNLHVONV7ncPA6V83gwYPnqeqAnf6gqoEswK+BhyL3s4FXI/fX7/C6dZWtq3///hqvnJycSl9z8cWqTZvGvYlaJ5Y6pxqvczh4nasGmKsVxNR6ce1GYjMI+I2IHAVkAs1E5BlglYi0V9XvRaQ98EOAZYhJ586wcSPk50Pz5skujXPOBSuwPn5VvVpVO6lqF+Ak4F+qegrwCjA68rLRwPSgyhArH8vvnAuTZIzjnwAMFZFFwNDI46TysfzOuTCJKfCLyBgRaSbmCRH5QESGxboRVc1V1V9H7q9R1SGq2j1yuzbewieKB37nXJjE2uI/U1U3AMOAtsAZ1IKWeqK0bw9paR74nXPhEGvgl8jtUcCTqvpRuefqvIwMaNfO+/idc+EQa+CfJyJvYYF/hog0BUqDK1bN8ytxOefCItbhnGcB/YDFqvqTiLTCuntSRufO8MknyS6Fc84FL9YW/0DgS1VdLyKnAH8C8oMrVs2LtvgDnMjsnHO1QqyB/2HgJxHpC1wFfAv8LbBSJUGnTvDTT3Y1LuecS2WxBv7iyPTfY4B7VfVeLP9OyvAhnc65sIg18G8UkauBU4HXRCQdyAiuWDXPA79zLixiDfwnAoXYeP6VQEfg9sBKlQQe+J1zYRFT4I8E+ylAcxH5NbBFVVOqj79dO0hP97H8zrnUF2vKhhOA/wGjgBOA/4rI8UEWrKalp0OHDt7id86lvljH8V8L7K+qPwCISFvgbWBaUAVLBp/E5ZwLg1j7+NOiQT9iTRXeW2d44HfOhUGswftNEZkhIqeLyOnAa8DrwRUrOdLSYMkSmDMn2SVxzrngxHpy90pgItAH6AtMVNWxQRaspuXlwbRpUFwMQ4bYY+ecS0UxX3pRVV8EXgywLEmVmwulkbRzW7bY44EDk1ki55wLxm4Dv4hsBCrKXiOAqmqzQEqVBNnZUL++BX1VaN062SVyzrlg7LarR1WbqmqzCpamqRT0wVr377wDf/mLBf0nn/SEbc651BRzV08YDBxoS+fOcNZZ1uc/alSyS+Wcc4mVckMyE2H0aOjdG8aNg8LCZJfGOecSK7DALyKZIvI/EflIRD4TkRsiz7cSkZkisihy2zKoMsQrPR3uuAMWL4aHHkp2aZxzLrGCbPEXAoeral/s6l1HiMhBwDjgHVXtDrwTeVzrDBsGw4fDTTfB2rXJLo1zziVOYIFfTUHkYUZkieb0nxx5fjIwMqgyVNftt0N+PtxyS7JL4pxziSMa4NCVSN7+ecDewIOqOlZE1qtqi3KvWaeqO3X3iMi5wLkAWVlZ/adOnRpXGQoKCmjSpElc7wW4/fYevPVWFpMn/48OHbbEvZ6aVN0610Ve53DwOlfN4MGD56nqgJ3+oKqBL0ALIAfoBazf4W/rKnt///79NV45OTlxv1dVdcUK1UaNVEeNqtZqalR161wXeZ3DwetcNcBcrSCm1sioHlVdD+QCRwCrRKQ9QOT2h12/M/k6dICrroK//93TODjnUkOQo3raikiLyP2GwK+AL4BXgNGRl40GpgdVhkS54gpo3x4uv9wndTnn6r4gW/ztgRwR+Rh4H5ipqq8CE4ChIrIIGBp5XKs1bmyje/Ly4MWUzVbknAuLwGbuqurHwC8reH4NMCSo7Qbl9NPhnntg7Fj4zW8sr49zztVFPnM3Rj6pyzmXKjzwV8Hw4Tax68YbfVKXc67u8sBfRbffDuvX+6Qu51zd5YG/ivr0gTPOgPvvt24f55yrazzwx+GmmyAjA66+Otklcc65qvPAH4cOHeDKK+GFF3xSl3Ou7vHAH6crroB27eC888rG+DvnXF3gV+CKU5MmNrZ/wgT45BO7ZGN2Nhx2GPTsCb16wd57Qz3/hJ1ztYyHpWpo2hTS0qC01FI5fPQR5OaWpXWoXx9+/nPbCUR3BoWFsGgRDB5sl3lMtLw8mDJlTxo0CGb9zrm6zwN/NQweDA0awNatFuRffRX69oUvvoBPP7Xls89g9mx49tnt35uWBsceC0ccAb/8pe0YMjPjK8emTfD11/Daa3bkUVzclSlT7OLxHvydczvywF8NAwdacM3NtW6eaJDdbz9bytuwwdI9PPqoHRGUlsI//1mW+yc9HX7xC+jXb/tl4UJb/wEHQOvW8NVXdsTw1Vdl97//fseSCZs3w9NPe+B3zu3MA381DRwYW3Bt1gxOOw0mTy47Qpg5004Qz59ftuTmwjPPlL1PpOKMoO3a2TmE4cOhe3e7/9NP8Mc/wpYtiqrw8MOwZQuMHw9ZWYmpr3Ou7vPAX4N2dYSw117w29+WvW71atsJ3HUXvPGGPScCv/udDSPde287uVyRHj1g0qRvOPHEbsycCXffbUcV118PF15o8w+cc+Hmgb+GxXKE0KYN/OpXlg46N7fsCOHCC637p7L1FxYuJTu7G7/6FZx5JlxyCVx2GTz2GNx3n63bORdePo6/FoseIdx0U/wnanv0gNdfh1desRFFQ4fa0cWSJQkvrnOujvAWfy0X6zmE3RGBo4+2oH/XXZZg7vXX7ZKS2dnwn/9s3/XknEtt3uIPkcxMuOYa+PJLG0p6440wZAhcey0cfrjPPnYuLDzwh1CnTjav4JxzbMSQqo3+GTkSLrjALiz/ww+J215eno0s8h2Lc7WDd/WE2Bln2NDRwkKbR9C1qw03jV5h7Be/KEtDcdhhNoQ0L2/nUUnlbdkCK1eWLe+9B/feCyUlNtnNJ5U5l3we+EOsouGlRUXwwQf23Lvv2iSwhx+21//sZ7B8uU0+q1cPjjvOZiCvXGmTyFautIvU7MrmzbYT6N/fr1nsXDIFFvhFpDPwN6AdUApMVNV7RaQV8DzQBVgCnKCq64Iqh9u9HU8eZ2TAgQfaMnYsFBfbjuDdd+Hxx63lDraDePFF2HNPaN/eUk4MGWL327Uru12+HE480Y4qAJ5/HnJyrJvp3HPt/c65mhVki78YuFxVPxCRpsA8EZkJnA68o6oTRGQcMA4YG2A5XDXUq2fpIg44AA4+2IJ7dF7B22/D//3f7t+/337wr3/ZEcShh0JBATz4INx6q/X7H320nVcYMsSOHpxzwQss8Kvq98D3kfsbRWQB0BE4BsiOvGwykIsH/jphVzOPY3lf+dcOH27zCCZOtKOI6dMt7cQf/mCprr/4ourbcM7FTrSiRDCJ3ohIF2AW0AtYqqotyv1tnaq2rOA95wLnAmRlZfWfOnVqXNsuKCigya7yG6SoulTnrVuFd99ty/TpHfnss+ZkZJRQUiKAkJFRyp13fkTPnhsqXU9dqnOieJ3DoTp1Hjx48DxVHbDTH1Q10AVoAswDjos8Xr/D39dVto7+/ftrvHJycuJ+b11VV+v84Yeq++8fHWCqKqJ6442xvbeu1rk6vM7hUJ06A3O1gpgaaK+qiGQALwJTVPUfkadXiUj7yN/bAwkcMe7qsn79bNRPZmZZVtLHHrPuJedc4gQW+EVEgCeABap6V7k/vQKMjtwfDUwPqgyu7hk40E4G33KLpZeoX9+Syp1yCqxalZwyzZhhSe7mzEnO9p1LtCBb/IOAU4HDRWR+ZDkKmAAMFZFFwNDIY+e2GTgQrr4aLr3Urmd83XXwwguWcO6RR2weQZCil9EcP96uqHbEEZbe+uCDbQewdm2w23cuaIEFflWdraqiqn1UtV9keV1V16jqEFXtHrn1fyO3Sw0bWk6hjz+2oaF/+IMNIZ0/P7Hb2bgRXnrJ5hd06mTdTtdcA2vWWLcT2A7h7ruhQwc4+WSbjxD0Tsi5IKT2yOm8PPacMsWTxKSAn//c+vqffhoWL7bZv5ddZgG7qvLybB7Bc89Zd9KQIXZZy+OOsyOLQYPgySdtNvLf/27nHNLTbSf01FO2c3j9dUtst88+dmSw8+Uvnau9UjdlQ14eHHYYXYuK8CuPpwYR6+sfMcK6gu6+2wL1BRfA4sV7omrdQevWWeqI6G35+19+CW++WTYDGaBXL+tWOuooO5oof5Wydu12nrswejTcdhv84x82D+Gaa6w7asQIOPtsaNECZs/2eQiu9krdwJ+bC8XFCNhU09xc/y9MES1bWl//6NFw6qkWeKErjz+++/c1aWKzg6NBPy0NrrgC/vrX3b+vomsiNGxo3T0nn2wXvJ80yY4SXnnF/i5is55vvx2GDbPuo6ZN46mtc4mXuoE/OxsyM9HNmy34Z2cntzwu4QYOtAyj110HqoIIHHOMXZu4RQvbQURvmze3lnxe3vZpJ0aOrH45une37p4bb7RLXT7zjJ0PKCqyy15GNWtmO4DyS+fOdrt6NXz2mV0sZ/DgsvMKu5KXB1Om7EmDBsG0ZyrLwurqttQN/JH8AhvPOINmS5ZYJ7FLOYcfbkM/CwtLadAgjauu2n2gijftRCwyMuCPf7TkdVu32uM777Sdz/Ll2y+ffGLZTHecOD9hgh2JRHdazZtvf9uiBWzYAH/7GxQXd+Xpp627a8QIOw8Rj6IiS6GxcKEts2bZkUtpqe0cc3Iqz8nk6pbUDfwAAwey8IorGHDOOXZW7tJLk10il2DRQD5p0hLOPLNbTIE8EZezrKw8sexYiorspPCtt9pEtdJSa+kfcohlO42en8jPt4Ccn2+PCwqiaxAKC+0op1496NjRjiD23HPn25UrrUydOtkOKRrkv/zSTpYXF5eVKzOzbLTS1q12tbZbb7VurczMxH9mrualduAHCvbe2wZgP/ggjBnjKSBT0MCBUFi4lIEDuyW7KEDsO5aMDAvKo0dbCz7a/TR+/O7f/+9/23mDrVuVevWEMWNsXUuX2pKXZ6ORiop2vY4GDayLqndv+O1vbXRSdFm40CbNbd1qRxHNmtlJ62uugQsvtCG1bdpU/XNxtUfKB37Afq0nnWRTMI88MtmlcW47Ve1+OuQQm908adI3uzzKKS21mc5Ll8J999nQVVVr91x+eVmXUkXatNm+PAcdZNu78074859txzR6tB1A77NPfHX2cwjJFY7Af+yxdmWQBx7wwO9qpap2P1V2lJOWZj/59u2t3fPSS2VHFMceW/mB747lGTLEls8/t7kPkybBo4/a9RQuv9y6mt59d+dAXlwMK1bYOYRvv7Xlf/+zeRDRK7k99pjtSCo7oe0SJxyBv359OO88uOEG+Oor2HvvZJfIuRqTyBPa++5rcxduucWuzfzQQ3YiOBq009Otm+innyzYr1ix/ZwJsGG10XMIxcU2MuvWW+2g/KSTbBsuWOHp8D73XPtVRi8g61yIRPMfJapbJSvL2lFLl9qQ2Ggy7eJi68YpLbUrro0bZxfceestO5G8ebPdb9jQ/h0zM+01e+5pO5OePaFPH9sRfP119cqYl2fdUrFO3K/q6+uycLT4wY55jz/ejlFvvBEaN052iZyr8xo2hKuustNn0a6kN96Ib0jtypUwbRpMnQrXXmvL/vvbUcAJJ8CyZRXPXSgttdQd+flly3/+Y+8vKrLupFNPtaGwGzfaqKiNG8uWggLLybRmja0vPd2uN3322dClS2p2QYUn8IN1dk6dCs8+awlXnHPVFk9XUkXnNNq1s3/RCy+0I4kXXrB/18svtyUtDUpLuzJpEvzsZxbUN2yw4L27CwkWFVl7r3Fjmz3dtKl1NzVtau3BJk3s6GLtWltPSYkdcdx6qw2Hzc6Gww6z227dUmNHEK7A/3//Z2kXH3jAduep8A06Vwskem7EnntaOo0rrrCUGH/8I7z9NoBQWgqNGsEBB9hQ0+bNd16WLrUdSFGRHYXMnGnJ93ZlxxndEyfakUNuruV2evppe12nTmU7gWbN7JTh4MHBjUwKaoZ2uAK/iP0azj7bsmgdckiyS+Scq0T37tY7+957ZTO0H3us8kC4776xH4Xs6qjlggvsKGDBAhu1lJtrO6ApU8reKwK//KUlCMzKsiOXdu3K7mdlQdu28P77O6+/qMi6mjZtstvosmkTfPAB3HQTlJR0TXieyXAFfrBELldeaa1+D/zO1Qk1MUN7V68XsZ3Ivvva5DVV63q6556yk9pr1tgw1VWrys+srpiIdS8VFtoRRuUk4Xkmwxf4GzWCs86yb23FCpvn7pyr9WrLDG0RGDXKMsRGu4aee64sKG/aZDuAlSvLbqdNs5xH0XMRPXtay79xY9sJNGmy8/2vvrJQtXVrKfXrpyU0z2T4Aj/YbvvOO60j74Ybkl0a51wds7sT2o0b20ngbuX2T/36bX8O4a67Km+9H3AAdO1ataOcWIUz8HfrZukMH33UxnzVr5/sEjnn6piqdCXFO4kuqKOc8Ezg2tGFF9px2IsvJrskzrkQSPQkuuoILPCLyCQR+UFEPi33XCsRmSkiiyK3LYPafqWGDrXhAg88kLQiOOdcMgTZ4n8KOGKH58YB76hqd+CdyOPkSEuzsVpz5ti4KeecC4nAAr+qzgLW7vD0McDkyP3JwMigth+T0aPtTMyDDya1GM45V5NEdzfXuborF+kCvKqqvSKP16tqi3J/X6eqFXb3iMi5wLkAWVlZ/adOnRpXGQoKCmjSpMku/9797rtp9+ab5L3wAsXNm8e1jdqmsjqX1+yzz2gxfz7r+/VjQ8+eAZcsOFWpc6rwOodDdeo8ePDgeao6YKc/qGpgC9AF+LTc4/U7/H1dLOvp37+/xisnJ2f3L/jkE5uDcdttcW+jtqm0zlFz5qg2bKialma3c+YEWq4gxVznFOJ1Dofq1BmYqxXE1Joe1bNKRNoDRG5/qOHt76xXLxtf9dBDOycOD8Ls2bUn9+sLL1ie3NJSu73rrlinEjrn6rCaDvyvAKMj90cD02t4+xW78EK7asTrrwe7nfvuszQR11wDhx+e3OA/c6Zd+ghsKqKITS/s2tWuy7duXfLK5pwLVJDDOZ8D8oAeIrJcRM4CJgBDRWQRMDTyOPmOOcbS7gU5tHPaNLtIadSWLWXJPmrao4/aJSi7dYN//MOugDF7tqUh7NnTBht37gwXXwyLF1d/e2G6woVzdUBgM3dV9Xe7+NOQoLYZt3r14Pzz4U9/sssE9eiR2PU/+qiliejVyxJwbN1qAT/a1fLII9ChQ2K3WZGSErtqxl13WeCfOtVyy5Y3fDh8/LG95pFHbMTTscdaVqpYZp5s3gwLF9rFWT//3HYos2ZZfTMzE5ti0DkXl3CmbKjIOefA9dfbBUDvvDMxwUnVrubwpz/BUUfB3/8OH31k87YPOcTS+V17rbWy77472CtOFxTAySfbBVIvusgCe71dfP19+sBTT1nZH3jAdgAvvggHHQS//rW95sADLfF5NMAvWGC3ixeXHcWkp0PLlmUXWN28GZ55xgO/c8lW0Rnf2rYEOqonas4c1fR0G+FTv77qe+/FvU1VVS0pUb3kElvfKaeobt1a8esWLlQ9+GB73ZFHqi5dWr3tagV1XrZMtV8/G71z//1VX+HGjfa+Dh2iWWi3X+rXV+3VS/WEE1Svv171+edttNSWLduPHIq+/rzzVFevrnY9ywt0tMecOaq33lrrRj35CJdwCGJUT9KDeixLjQT+W2/dPjh166b68cfxbXTrVgv2oDpmjO0EdqekRPW++1QbNVJt1kz1scdUS0vj27buUOe5cy1gN22q+vrrca9TVVVvvrnsMxKxOn75pWpR0e7fFw2cM2eqXnqp7WBbt7Z6VvbZxCiwgPDii2UNgszMWhX8PQiGQyoM56y9srOhQQPrnsjIgB9/tFyqf/iD3Y/VTz9Zn/gzz8DNN1sXTlolH3NamnW/fPIJ7LefdTsNHw7ffludGsHLL8Ohh1qXznvvWb9+dRx+eNlnlJlp18PbZ59ddxlFRbNT/epX1sX04Yd2VYtzzrHLYc6bV71yBWHVKhgzxq7yHR3mu2ULPP98cstVU6p6Qt5P4Ncp3scftWPe1B49rM//oYfsKgt/+Yvl9tldCuf1660PfM4c6xc/77yqlaFbNyvDo4/aSdheveykc8uWVbuwpyrccYetY//9Yfp0uwZcdcWbW3ZHvXvbdeymTLGLqu6/v9Xz5puhVavql7M61q+H22+3EVeFhZa++6237Bp5JSV21e7f/96SpdcleXll39tBB1kDJT+/4uWjj+w3WFJiO/mTTrJRbyLWSIkO/43eX77czgmVlPgJ/LqiosOA2rbUSFfPrnz2merw4Xaov88+qq+9VvHrvvtOtXdv1YwM1RdeqN42VVW/+UZ1wICyricR1R497DzA6NGqV16pevvtqpMnWxfO3Ll2fuDttzW/Rw97z6hRqj/9VP2yBGn9eusOS0tTbdNG9YknVGfPrnKferW/54IC1fHjVVu0sM/upJPs/ItqWVfVtGnWBdikieq//lW97SVAzHXOySnrroLt78eypKfbeZx69ex++S7RipZRoxLWhRd3nVOI9/HHISE/lNJS1VdftcAPqkccofr552V//+or1a5dVRs3tn7sRLnllu371Lt3V+3fX7VzZ/tH3N0/X716FkDrivnzVQcNsrKnpVl9MzLsvMLbb6vOm6e6eLHqunU7B5U5c/Trs8+Or/+9sFD1gQdU27WzbY8Yofrhh7t+/Xffqfbsqdqgger06VXfXgLF9Ntev95+m+UbEIcdpjphgurDD6s++6w1ZmbPthPyS5fab7hhQwvylaXyKC1V/fe/y07gi9h2+vdXzc1NVFW38cBfNR74E6GwUPWuu1SbN7d/iosvtqDRpImdlP3vfxO3LdWyETEV/QOWlqrm56suWmQjkF56SfXYY8v+8dLTrZVal5SUqB5/fOUt0LQ01VatVPfaS/XnP1dNS9PS6M7uggtUn3nGdhaffaa6du3OJ8rnzLEdynXXqXbpYus89NDYd5SrV6secIB9xlOmJP5ziFGlv+0VK1T79ClrsccSyKOqOpIp+vrZs1Wfflq1Uyf7XEeOLDtySoA6PXorzvV74I9DID+UH35QPf/8siAb5IiPqvxYIjuKkrqcdK38zi4z04LIu++qvvyy6pNP2o73T3+yAP/735cdhe1uadBA9Wc/Uz3wQAvw5bs69tlH9c03qz6KasMG1exs+w089FAQn0Sldvvb/vJLq3PjxqozZtT8kNRNm2zn2qSJ7ZDHjFFds6baqw0s8L/wgpUzOjz51VcTt+5Nm1TvuGP79c+aFfPbPfDHIdAWwsUX174WdnW6PWqL6uzsZs60oJebq/rcc6p336161VWqp52mOnSoalbW9kcOt9wSfzl/+kn16KNtXePHx7+eOO3yt/3f/9r5krZtVf/3vxot006+/171nHPss27RwnbchYVxry7h/89z59o5ifKNuOgyYIAdFc6Zo1pcXLX1LlpkQ7SPOMIaMDuuu0UL1XvusTkylfDAH4fADw1j7QutQaHrB63Kzi7R39nWrXbkAapjx1Zr/kVVVfg9v/GGzQfp2jWhXSzV9vHHtuMF66K75RZbqvL5J6pRU1pqDYRf/crK06yZ6qmnWoBOT7cjxLPPVh04sOwcW6tWdsJ/8mTVlSu3lWdbA2XzZjuyGjPGzsVFA3z37vbc3XeX/e7q17eBIKDasqXqNdfYDnIXggj8PpyzOhI1vNFVz8CBLC0spFssn3+iv7OMDHj6act59Ne/2nDIBx+sfO5GEJ5+Gs4804YBv/FGYobwJkrv3jBjhiUC/OMfLVUJ2HDRI4+EPfe0OSINGtiQ6ej96ONly+C22+haVGTDgOMZMlpSAi+9ZNln582zz+e222zYdbNmNmdnx9/F2rU2nPfNN22JXhBqn30sPUlJiQ1pzciw4b8NGtjQ64susnrtvXfZ9g88cPv15+XZsOvx4+32tNMsJ9bPf169zzoWFe0NattSa1v8tZTXOQlKS63FD9ayvemmKrdmqzWE9fbbbduDB9tJ/9rs5pu371pp2tRmcjdpYiO5YhlimpVlM8fvuMNa7z/+uOvtbd6s+uijZS3x7t1VJ06056uipMRGl918s50/KV+egw6y0VGbNlX981i40M4ZRruEjj7azgFEjh69xe9cbSViLcn8fJu8N3Mm3HADnHgi7LGHTQArLrbb6BJ9/MMP8J//WAjJyIB//hOGDYttu6WlcOWVNiP6hBPgb3+zVmdtdvjhlgp861Zrzc+YsX3rvbTUPpfCwrIlLw9OOw0tLETS02GvvSAnx2bIR3XsCH372oz7fv3sfVOmWDLEtWuhf39LlHjssXakUVVpaTazfr/9rA5DhpTV4a674j967N4dHn7Yfi8PPmjLoYfaEcJvfsOeX39t32kCexQ88DuXSHvuaTsBVQvsU6dCw4aW1iIjw5by9zMyYPXqsgymW7dauo4+feDggy2L6yGHWFDbgRQVWffAlCl2MaF7701OF1NVVdbdlpZW1s0TNWoUdOrEN5Mm0e3MM8ves3q1zTSeP7/s9q237LMvv7777rPPKFHZb4Po5t1jDwv+Y8faTOhbboFrr6WriGUPSOCMaA/8ziVSdralLYi2BGP5Z83LK2s91qsHp54KS5da6/2hh+w1XbqU7QQOPhhWrGD/M86AFSssQFx9dXApvYMwcGDVg1hF53LatLHPbki5y3wUFloqkIcesh2qiKUlT/TnE08dYtGokZ0HWbcO/vxnpLTUfhu5uR74nauV4mkJ7uo9xcV2UZx//9uWGTPsBG5EQ7AjhsGD61bQD1qDBpZP6YknynbA2dnJLlXVRbrESgsLSUtwHTzwO5docbZmd3pPvXplfcpjxlj30VdfWev+H/9AVK1Fm8CWYMpIhRF3kTos2bF7KwE88DtXV4jYicDLL4fXXw+kJZhSguqKqUlVGapcBXXgTJBzbjvRluCZZ3oKZBeXpLT4ReQI4F4gHXhcVSckoxzO1VkBtQRdONR4i19E0oEHgSOBfYHfici+NV0O55wLq2R09RwAfKWqi1V1KzAVOCYJ5XDOuVBKRuDvCCwr93h55DnnnHM1IBl9/BUNONadXiRyLnAuQFZWFrm5uXFtrKCgIO731lVe53DwOodDEHVORuBfDnQu97gT8N2OL1LVicBEgAEDBmh2nEPWcnNzife9dZXXORy8zuEQRJ2T0dXzPtBdRLqKSH3gJOCVJJTDOedCSSxzZw1vVOQo4B5sOOckVb2lktf/CHwb5+baAKvjfG9d5XUOB69zOFSnzj9T1bY7PpmUwF+TRGSuqg5Idjlqktc5HLzO4RBEnX3mrnPOhYwHfuecC5kwBP6JyS5AEnidw8HrHA4Jr3PK9/E755zbXhha/M4558rxwO+ccyGT0oFfRI4QkS9F5CsRGZfs8tQEEVkiIp+IyHwRmZvs8gRBRCaJyA8i8mm551qJyEwRWRS5bZnMMibaLup8vYisiHzX8yPzY1KCiHQWkRwRWSAin4nImMjzKfs976bOCf+eU7aPP5L+eSEwFEsT8T7wO1X9PKkFC5iILAEGqGrKTnIRkUOBAuBvqtor8txtwFpVnRDZybdU1bHJLGci7aLO1wMFqnpHMssWBBFpD7RX1Q9EpCkwDxgJnE6Kfs+7qfMJJPh7TuUWv6d/TlGqOgtYu8PTxwCTI/cnY/8wKWMXdU5Zqvq9qn4Qub8RWIBl8U3Z73k3dU64VA78YU3/rMBbIjIvkuE0LLJU9XuwfyBgjySXp6ZcKCIfR7qCUqbbozwR6QL8EvgvIfmed6gzJPh7TuXAH1P65xQ0SFX3w65wdkGki8ClpoeBvYB+wPfAnUktTQBEpAnwInCJqm5IdnlqQgV1Tvj3nMqBP6b0z6lGVb+L3P4AvIR1eYXBqkgfabSv9IcklydwqrpKVUtUtRR4jBT7rkUkAwuAU1T1H5GnU/p7rqjOQXzPqRz4Q5f+WUQaR04KISKNgWHAp7t/V8p4BRgduT8amJ7EstSIaACMOJYU+q5FRIAngAWqele5P6Xs97yrOgfxPafsqB6oevrnuk5EumGtfLCL7DybinUWkeeAbCxd7SrgL8DLwAvAnsBSYJSqpszJ0F3UORs7/FdgCXBetP+7rhORg4F/A58ApZGnr8H6vFPye95NnX9Hgr/nlA78zjnndpbKXT3OOecq4IHfOedCxgO/c86FjAd+55wLGQ/8zjkXMh74nQuYiGSLyKvJLodzUR74nXMuZDzwOxchIqeIyP8iOc8fFZF0ESkQkTtF5AMReUdE2kZe209E/hNJnPVSNHGWiOwtIm+LyEeR9+wVWX0TEZkmIl+IyJTILE3nksIDv3OAiPwCOBFLctcPKAFOBhoDH0QS372LzZgF+BswVlX7YDMto89PAR5U1b7A/2FJtcAyLV4C7At0AwYFXCXndqlesgvgXC0xBOgPvB9pjDfEEoCVAs9HXvMM8A8RaQ60UNV3I89PBv4eyZPUUVVfAlDVLQCR9f1PVZdHHs8HugCzA6+VcxXwwO+cEWCyql693ZMi1+3wut3lONld901hufsl+P+eSyLv6nHOvAMcLyJ7wLZru/4M+x85PvKa3wOzVTUfWCcih0SePxV4N5I7fbmIjIyso4GINKrJSjgXC291OAeo6uci8ifs6mVpQBFwAbAJ6Cki84B87DwAWErgRyKBfTFwRuT5U4FHReTGyDpG1WA1nIuJZ+d0bjdEpEBVmyS7HM4lknf1OOdcyHiL3znnQsZb/M45FzIe+J1zLmQ88DvnXMh44HfOuZDxwO+ccyHz/329Qt9jOdQrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### 3rd -1 trial #####\n",
    "# draw a graph\n",
    "x_len = np.arange(len(y_loss2))\n",
    "plt.plot(x_len,y_vloss2, marker='.', c='red', label='Testset_loss')\n",
    "plt.plot(x_len,y_loss2, marker='.', c='blue', label='Trainset_loss')\n",
    "#------------------\n",
    "#plt.xlim(-1,3)\n",
    "#------------------\n",
    "# labeling\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "0964e049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 1ms/step - loss: 3.1312 - mse: 3.1312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.1311817169189453, 3.1311817169189453]"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d914d84d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "b5e06622",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3_2 = Sequential()\n",
    "#--------------------------------------------\n",
    "model3_2.add(Dense(128, input_shape=(43,)))\n",
    "model3_2.add(Activation('relu'))\n",
    "#model3_2.add(Dropout(0.5))\n",
    "#----------------------------------\n",
    "model3_2.add(Dense(64))\n",
    "model3_2.add(Activation('relu'))\n",
    "#model3_2.add(Dropout(0.5))\n",
    "#----------------------------------\n",
    "model3_2.add(Dense(16))\n",
    "model3_2.add(Activation('relu'))\n",
    "#model3_2.add(Dropout(0.5))\n",
    "#----------------------------------\n",
    "model3_2.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "368f5379",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3_2.compile(loss='mean_squared_error', metrics=['mse'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "9c91424f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "105/115 [==========================>...] - ETA: 0s - loss: 24.2270 - mse: 24.2270 \n",
      "Epoch 1: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 1s 3ms/step - loss: 22.3932 - mse: 22.3932 - val_loss: 2.1540 - val_mse: 2.1540\n",
      "Epoch 2/200\n",
      "105/115 [==========================>...] - ETA: 0s - loss: 1.5185 - mse: 1.5185\n",
      "Epoch 2: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 1.5106 - mse: 1.5106 - val_loss: 1.5162 - val_mse: 1.5162\n",
      "Epoch 3/200\n",
      "100/115 [=========================>....] - ETA: 0s - loss: 1.0165 - mse: 1.0165\n",
      "Epoch 3: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 1.0035 - mse: 1.0035 - val_loss: 1.2337 - val_mse: 1.2337\n",
      "Epoch 4/200\n",
      " 95/115 [=======================>......] - ETA: 0s - loss: 0.7629 - mse: 0.7629\n",
      "Epoch 4: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.7746 - mse: 0.7746 - val_loss: 1.1821 - val_mse: 1.1821\n",
      "Epoch 5/200\n",
      "101/115 [=========================>....] - ETA: 0s - loss: 0.5726 - mse: 0.5726\n",
      "Epoch 5: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.5738 - mse: 0.5738 - val_loss: 1.0165 - val_mse: 1.0165\n",
      "Epoch 6/200\n",
      "100/115 [=========================>....] - ETA: 0s - loss: 0.4738 - mse: 0.4738\n",
      "Epoch 6: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.4788 - mse: 0.4788 - val_loss: 0.8980 - val_mse: 0.8980\n",
      "Epoch 7/200\n",
      "103/115 [=========================>....] - ETA: 0s - loss: 0.4235 - mse: 0.4235\n",
      "Epoch 7: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.4192 - mse: 0.4192 - val_loss: 0.7874 - val_mse: 0.7874\n",
      "Epoch 8/200\n",
      "103/115 [=========================>....] - ETA: 0s - loss: 0.3326 - mse: 0.3326\n",
      "Epoch 8: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.3329 - mse: 0.3329 - val_loss: 0.7206 - val_mse: 0.7206\n",
      "Epoch 9/200\n",
      "105/115 [==========================>...] - ETA: 0s - loss: 0.2722 - mse: 0.2722\n",
      "Epoch 9: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2746 - mse: 0.2746 - val_loss: 0.6776 - val_mse: 0.6776\n",
      "Epoch 10/200\n",
      "101/115 [=========================>....] - ETA: 0s - loss: 0.2554 - mse: 0.2554\n",
      "Epoch 10: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2564 - mse: 0.2564 - val_loss: 0.6717 - val_mse: 0.6717\n",
      "Epoch 11/200\n",
      "106/115 [==========================>...] - ETA: 0s - loss: 0.2133 - mse: 0.2133\n",
      "Epoch 11: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2145 - mse: 0.2145 - val_loss: 0.6250 - val_mse: 0.6250\n",
      "Epoch 12/200\n",
      " 99/115 [========================>.....] - ETA: 0s - loss: 0.1644 - mse: 0.1644\n",
      "Epoch 12: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1713 - mse: 0.1713 - val_loss: 0.6695 - val_mse: 0.6695\n",
      "Epoch 13/200\n",
      " 87/115 [=====================>........] - ETA: 0s - loss: 0.1520 - mse: 0.1520\n",
      "Epoch 13: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1545 - mse: 0.1545 - val_loss: 0.6158 - val_mse: 0.6158\n",
      "Epoch 14/200\n",
      "102/115 [=========================>....] - ETA: 0s - loss: 0.1280 - mse: 0.1280\n",
      "Epoch 14: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1338 - mse: 0.1338 - val_loss: 0.5561 - val_mse: 0.5561\n",
      "Epoch 15/200\n",
      " 97/115 [========================>.....] - ETA: 0s - loss: 0.1249 - mse: 0.1249\n",
      "Epoch 15: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1284 - mse: 0.1284 - val_loss: 0.5360 - val_mse: 0.5360\n",
      "Epoch 16/200\n",
      "102/115 [=========================>....] - ETA: 0s - loss: 0.0994 - mse: 0.0994\n",
      "Epoch 16: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1022 - mse: 0.1022 - val_loss: 0.5220 - val_mse: 0.5220\n",
      "Epoch 17/200\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.1032 - mse: 0.1032\n",
      "Epoch 17: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1031 - mse: 0.1031 - val_loss: 0.5593 - val_mse: 0.5593\n",
      "Epoch 18/200\n",
      " 95/115 [=======================>......] - ETA: 0s - loss: 0.0928 - mse: 0.0928\n",
      "Epoch 18: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.4835 - val_mse: 0.4835\n",
      "Epoch 19/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0804 - mse: 0.0804\n",
      "Epoch 19: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0808 - mse: 0.0808 - val_loss: 0.5067 - val_mse: 0.5067\n",
      "Epoch 20/200\n",
      " 97/115 [========================>.....] - ETA: 0s - loss: 0.0825 - mse: 0.0825\n",
      "Epoch 20: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0815 - mse: 0.0815 - val_loss: 0.5606 - val_mse: 0.5606\n",
      "Epoch 21/200\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0641 - mse: 0.0641\n",
      "Epoch 21: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0641 - mse: 0.0641 - val_loss: 0.4843 - val_mse: 0.4843\n",
      "Epoch 22/200\n",
      "103/115 [=========================>....] - ETA: 0s - loss: 0.0637 - mse: 0.0637\n",
      "Epoch 22: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0657 - mse: 0.0657 - val_loss: 0.4668 - val_mse: 0.4668\n",
      "Epoch 23/200\n",
      " 94/115 [=======================>......] - ETA: 0s - loss: 0.0584 - mse: 0.0584\n",
      "Epoch 23: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0560 - mse: 0.0560 - val_loss: 0.4574 - val_mse: 0.4574\n",
      "Epoch 24/200\n",
      "101/115 [=========================>....] - ETA: 0s - loss: 0.0460 - mse: 0.0460\n",
      "Epoch 24: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0464 - mse: 0.0464 - val_loss: 0.4571 - val_mse: 0.4571\n",
      "Epoch 25/200\n",
      " 94/115 [=======================>......] - ETA: 0s - loss: 0.0450 - mse: 0.0450\n",
      "Epoch 25: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0443 - mse: 0.0443 - val_loss: 0.4300 - val_mse: 0.4300\n",
      "Epoch 26/200\n",
      "104/115 [==========================>...] - ETA: 0s - loss: 0.0414 - mse: 0.0414\n",
      "Epoch 26: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0422 - mse: 0.0422 - val_loss: 0.4413 - val_mse: 0.4413\n",
      "Epoch 27/200\n",
      " 98/115 [========================>.....] - ETA: 0s - loss: 0.0360 - mse: 0.0360\n",
      "Epoch 27: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0374 - mse: 0.0374 - val_loss: 0.4779 - val_mse: 0.4779\n",
      "Epoch 28/200\n",
      "105/115 [==========================>...] - ETA: 0s - loss: 0.0384 - mse: 0.0384\n",
      "Epoch 28: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.4153 - val_mse: 0.4153\n",
      "Epoch 29/200\n",
      " 99/115 [========================>.....] - ETA: 0s - loss: 0.0494 - mse: 0.0494\n",
      "Epoch 29: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0495 - mse: 0.0495 - val_loss: 0.4550 - val_mse: 0.4550\n",
      "Epoch 30/200\n",
      "103/115 [=========================>....] - ETA: 0s - loss: 0.0470 - mse: 0.0470\n",
      "Epoch 30: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0477 - mse: 0.0477 - val_loss: 0.4360 - val_mse: 0.4360\n",
      "Epoch 31/200\n",
      "103/115 [=========================>....] - ETA: 0s - loss: 0.0483 - mse: 0.0483\n",
      "Epoch 31: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0484 - mse: 0.0484 - val_loss: 0.4540 - val_mse: 0.4540\n",
      "Epoch 32/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/115 [===========================>..] - ETA: 0s - loss: 0.0401 - mse: 0.0401\n",
      "Epoch 32: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0400 - mse: 0.0400 - val_loss: 0.4096 - val_mse: 0.4096\n",
      "Epoch 33/200\n",
      " 98/115 [========================>.....] - ETA: 0s - loss: 0.0387 - mse: 0.0387\n",
      "Epoch 33: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0396 - mse: 0.0396 - val_loss: 0.4195 - val_mse: 0.4195\n",
      "Epoch 34/200\n",
      "102/115 [=========================>....] - ETA: 0s - loss: 0.0425 - mse: 0.0425\n",
      "Epoch 34: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0423 - mse: 0.0423 - val_loss: 0.4310 - val_mse: 0.4310\n",
      "Epoch 35/200\n",
      "104/115 [==========================>...] - ETA: 0s - loss: 0.0401 - mse: 0.0401\n",
      "Epoch 35: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0398 - mse: 0.0398 - val_loss: 0.4257 - val_mse: 0.4257\n",
      "Epoch 36/200\n",
      " 94/115 [=======================>......] - ETA: 0s - loss: 0.0504 - mse: 0.0504\n",
      "Epoch 36: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0508 - mse: 0.0508 - val_loss: 0.4010 - val_mse: 0.4010\n",
      "Epoch 37/200\n",
      " 95/115 [=======================>......] - ETA: 0s - loss: 0.0675 - mse: 0.0675\n",
      "Epoch 37: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0696 - mse: 0.0696 - val_loss: 0.4840 - val_mse: 0.4840\n",
      "Epoch 38/200\n",
      "105/115 [==========================>...] - ETA: 0s - loss: 0.0931 - mse: 0.0931\n",
      "Epoch 38: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0925 - mse: 0.0925 - val_loss: 0.4727 - val_mse: 0.4727\n",
      "Epoch 39/200\n",
      " 85/115 [=====================>........] - ETA: 0s - loss: 0.1058 - mse: 0.1058\n",
      "Epoch 39: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1108 - mse: 0.1108 - val_loss: 0.4671 - val_mse: 0.4671\n",
      "Epoch 40/200\n",
      "105/115 [==========================>...] - ETA: 0s - loss: 0.0701 - mse: 0.0701\n",
      "Epoch 40: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0697 - mse: 0.0697 - val_loss: 0.4926 - val_mse: 0.4926\n",
      "Epoch 41/200\n",
      " 98/115 [========================>.....] - ETA: 0s - loss: 0.0744 - mse: 0.0744\n",
      "Epoch 41: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0718 - mse: 0.0718 - val_loss: 0.3880 - val_mse: 0.3880\n",
      "Epoch 42/200\n",
      " 91/115 [======================>.......] - ETA: 0s - loss: 0.0518 - mse: 0.0518\n",
      "Epoch 42: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0528 - mse: 0.0528 - val_loss: 0.4423 - val_mse: 0.4423\n",
      "Epoch 43/200\n",
      "102/115 [=========================>....] - ETA: 0s - loss: 0.0646 - mse: 0.0646\n",
      "Epoch 43: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0628 - mse: 0.0628 - val_loss: 0.3956 - val_mse: 0.3956\n",
      "Epoch 44/200\n",
      " 92/115 [=======================>......] - ETA: 0s - loss: 0.0432 - mse: 0.0432\n",
      "Epoch 44: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0412 - mse: 0.0412 - val_loss: 0.3551 - val_mse: 0.3551\n",
      "Epoch 45/200\n",
      " 95/115 [=======================>......] - ETA: 0s - loss: 0.0322 - mse: 0.0322\n",
      "Epoch 45: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0359 - mse: 0.0359 - val_loss: 0.4037 - val_mse: 0.4037\n",
      "Epoch 46/200\n",
      " 99/115 [========================>.....] - ETA: 0s - loss: 0.0389 - mse: 0.0389\n",
      "Epoch 46: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0375 - mse: 0.0375 - val_loss: 0.3906 - val_mse: 0.3906\n",
      "Epoch 47/200\n",
      " 96/115 [========================>.....] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
      "Epoch 47: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0307 - mse: 0.0307 - val_loss: 0.3825 - val_mse: 0.3825\n",
      "Epoch 48/200\n",
      "105/115 [==========================>...] - ETA: 0s - loss: 0.0499 - mse: 0.0499\n",
      "Epoch 48: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0496 - mse: 0.0496 - val_loss: 0.3527 - val_mse: 0.3527\n",
      "Epoch 49/200\n",
      " 95/115 [=======================>......] - ETA: 0s - loss: 0.0470 - mse: 0.0470\n",
      "Epoch 49: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0492 - mse: 0.0492 - val_loss: 0.4317 - val_mse: 0.4317\n",
      "Epoch 50/200\n",
      "104/115 [==========================>...] - ETA: 0s - loss: 0.0761 - mse: 0.0761\n",
      "Epoch 50: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0759 - mse: 0.0759 - val_loss: 0.3445 - val_mse: 0.3445\n",
      "Epoch 51/200\n",
      "102/115 [=========================>....] - ETA: 0s - loss: 0.0487 - mse: 0.0487\n",
      "Epoch 51: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0482 - mse: 0.0482 - val_loss: 0.3623 - val_mse: 0.3623\n",
      "Epoch 52/200\n",
      " 94/115 [=======================>......] - ETA: 0s - loss: 0.0494 - mse: 0.0494\n",
      "Epoch 52: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0471 - mse: 0.0471 - val_loss: 0.3665 - val_mse: 0.3665\n",
      "Epoch 53/200\n",
      "104/115 [==========================>...] - ETA: 0s - loss: 0.0497 - mse: 0.0497\n",
      "Epoch 53: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0497 - mse: 0.0497 - val_loss: 0.3223 - val_mse: 0.3223\n",
      "Epoch 54/200\n",
      " 97/115 [========================>.....] - ETA: 0s - loss: 0.0382 - mse: 0.0382\n",
      "Epoch 54: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0403 - mse: 0.0403 - val_loss: 0.3639 - val_mse: 0.3639\n",
      "Epoch 55/200\n",
      " 95/115 [=======================>......] - ETA: 0s - loss: 0.0459 - mse: 0.0459\n",
      "Epoch 55: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0443 - mse: 0.0443 - val_loss: 0.3553 - val_mse: 0.3553\n",
      "Epoch 56/200\n",
      " 98/115 [========================>.....] - ETA: 0s - loss: 0.0494 - mse: 0.0494\n",
      "Epoch 56: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0545 - mse: 0.0545 - val_loss: 0.3992 - val_mse: 0.3992\n",
      "Epoch 57/200\n",
      "103/115 [=========================>....] - ETA: 0s - loss: 0.0673 - mse: 0.0673\n",
      "Epoch 57: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0654 - mse: 0.0654 - val_loss: 0.3640 - val_mse: 0.3640\n",
      "Epoch 58/200\n",
      " 97/115 [========================>.....] - ETA: 0s - loss: 0.0484 - mse: 0.0484\n",
      "Epoch 58: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0466 - mse: 0.0466 - val_loss: 0.3141 - val_mse: 0.3141\n",
      "Epoch 59/200\n",
      "106/115 [==========================>...] - ETA: 0s - loss: 0.0385 - mse: 0.0385\n",
      "Epoch 59: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.3443 - val_mse: 0.3443\n",
      "Epoch 60/200\n",
      "106/115 [==========================>...] - ETA: 0s - loss: 0.0455 - mse: 0.0455\n",
      "Epoch 60: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0457 - mse: 0.0457 - val_loss: 0.3384 - val_mse: 0.3384\n",
      "Epoch 61/200\n",
      " 97/115 [========================>.....] - ETA: 0s - loss: 0.0552 - mse: 0.0552\n",
      "Epoch 61: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0535 - mse: 0.0535 - val_loss: 0.3062 - val_mse: 0.3062\n",
      "Epoch 62/200\n",
      " 98/115 [========================>.....] - ETA: 0s - loss: 0.0485 - mse: 0.0485\n",
      "Epoch 62: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0487 - mse: 0.0487 - val_loss: 0.3527 - val_mse: 0.3527\n",
      "Epoch 63/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 92/115 [=======================>......] - ETA: 0s - loss: 0.0534 - mse: 0.0534\n",
      "Epoch 63: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0501 - mse: 0.0501 - val_loss: 0.3345 - val_mse: 0.3345\n",
      "Epoch 64/200\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0301 - mse: 0.0301\n",
      "Epoch 64: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0301 - mse: 0.0301 - val_loss: 0.3080 - val_mse: 0.3080\n",
      "Epoch 65/200\n",
      "103/115 [=========================>....] - ETA: 0s - loss: 0.0271 - mse: 0.0271\n",
      "Epoch 65: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0308 - mse: 0.0308 - val_loss: 0.3052 - val_mse: 0.3052\n",
      "Epoch 66/200\n",
      "106/115 [==========================>...] - ETA: 0s - loss: 0.0370 - mse: 0.0370\n",
      "Epoch 66: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0375 - mse: 0.0375 - val_loss: 0.3291 - val_mse: 0.3291\n",
      "Epoch 67/200\n",
      "102/115 [=========================>....] - ETA: 0s - loss: 0.0337 - mse: 0.0337\n",
      "Epoch 67: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0330 - mse: 0.0330 - val_loss: 0.3169 - val_mse: 0.3169\n",
      "Epoch 68/200\n",
      "105/115 [==========================>...] - ETA: 0s - loss: 0.0494 - mse: 0.0494\n",
      "Epoch 68: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0482 - mse: 0.0482 - val_loss: 0.2899 - val_mse: 0.2899\n",
      "Epoch 69/200\n",
      " 94/115 [=======================>......] - ETA: 0s - loss: 0.0424 - mse: 0.0424\n",
      "Epoch 69: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0417 - mse: 0.0417 - val_loss: 0.3152 - val_mse: 0.3152\n",
      "Epoch 70/200\n",
      "105/115 [==========================>...] - ETA: 0s - loss: 0.0440 - mse: 0.0440\n",
      "Epoch 70: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0435 - mse: 0.0435 - val_loss: 0.3050 - val_mse: 0.3050\n",
      "Epoch 71/200\n",
      " 96/115 [========================>.....] - ETA: 0s - loss: 0.0457 - mse: 0.0457\n",
      "Epoch 71: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0492 - mse: 0.0492 - val_loss: 0.3341 - val_mse: 0.3341\n",
      "Epoch 72/200\n",
      "105/115 [==========================>...] - ETA: 0s - loss: 0.0477 - mse: 0.0477\n",
      "Epoch 72: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0473 - mse: 0.0473 - val_loss: 0.3593 - val_mse: 0.3593\n",
      "Epoch 73/200\n",
      " 98/115 [========================>.....] - ETA: 0s - loss: 0.0377 - mse: 0.0377\n",
      "Epoch 73: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.3000 - val_mse: 0.3000\n",
      "Epoch 74/200\n",
      " 98/115 [========================>.....] - ETA: 0s - loss: 0.0267 - mse: 0.0267\n",
      "Epoch 74: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0269 - mse: 0.0269 - val_loss: 0.2911 - val_mse: 0.2911\n",
      "Epoch 75/200\n",
      "104/115 [==========================>...] - ETA: 0s - loss: 0.0274 - mse: 0.0274\n",
      "Epoch 75: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0276 - mse: 0.0276 - val_loss: 0.3336 - val_mse: 0.3336\n",
      "Epoch 76/200\n",
      "101/115 [=========================>....] - ETA: 0s - loss: 0.0300 - mse: 0.0300\n",
      "Epoch 76: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0302 - mse: 0.0302 - val_loss: 0.2890 - val_mse: 0.2890\n",
      "Epoch 77/200\n",
      "101/115 [=========================>....] - ETA: 0s - loss: 0.0263 - mse: 0.0263\n",
      "Epoch 77: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0269 - mse: 0.0269 - val_loss: 0.3390 - val_mse: 0.3390\n",
      "Epoch 78/200\n",
      " 95/115 [=======================>......] - ETA: 0s - loss: 0.0419 - mse: 0.0419\n",
      "Epoch 78: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0465 - mse: 0.0465 - val_loss: 0.3245 - val_mse: 0.3245\n",
      "Epoch 79/200\n",
      "106/115 [==========================>...] - ETA: 0s - loss: 0.0614 - mse: 0.0614\n",
      "Epoch 79: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0607 - mse: 0.0607 - val_loss: 0.2587 - val_mse: 0.2587\n",
      "Epoch 80/200\n",
      " 96/115 [========================>.....] - ETA: 0s - loss: 0.0519 - mse: 0.0519\n",
      "Epoch 80: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0517 - mse: 0.0517 - val_loss: 0.3064 - val_mse: 0.3064\n",
      "Epoch 81/200\n",
      "102/115 [=========================>....] - ETA: 0s - loss: 0.0488 - mse: 0.0488\n",
      "Epoch 81: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0489 - mse: 0.0489 - val_loss: 0.3028 - val_mse: 0.3028\n",
      "Epoch 82/200\n",
      " 99/115 [========================>.....] - ETA: 0s - loss: 0.0613 - mse: 0.0613\n",
      "Epoch 82: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0593 - mse: 0.0593 - val_loss: 0.3392 - val_mse: 0.3392\n",
      "Epoch 83/200\n",
      " 94/115 [=======================>......] - ETA: 0s - loss: 0.0715 - mse: 0.0715\n",
      "Epoch 83: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0670 - mse: 0.0670 - val_loss: 0.2828 - val_mse: 0.2828\n",
      "Epoch 84/200\n",
      "103/115 [=========================>....] - ETA: 0s - loss: 0.0394 - mse: 0.0394\n",
      "Epoch 84: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.2865 - val_mse: 0.2865\n",
      "Epoch 85/200\n",
      " 96/115 [========================>.....] - ETA: 0s - loss: 0.0176 - mse: 0.0176\n",
      "Epoch 85: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0191 - mse: 0.0191 - val_loss: 0.2509 - val_mse: 0.2509\n",
      "Epoch 86/200\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 86: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0172 - mse: 0.0172 - val_loss: 0.2559 - val_mse: 0.2559\n",
      "Epoch 87/200\n",
      " 99/115 [========================>.....] - ETA: 0s - loss: 0.0137 - mse: 0.0137\n",
      "Epoch 87: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0135 - mse: 0.0135 - val_loss: 0.2736 - val_mse: 0.2736\n",
      "Epoch 88/200\n",
      "100/115 [=========================>....] - ETA: 0s - loss: 0.0118 - mse: 0.0118\n",
      "Epoch 88: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.2578 - val_mse: 0.2578\n",
      "Epoch 89/200\n",
      " 96/115 [========================>.....] - ETA: 0s - loss: 0.0092 - mse: 0.0092\n",
      "Epoch 89: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.2619 - val_mse: 0.2619\n",
      "Epoch 90/200\n",
      "106/115 [==========================>...] - ETA: 0s - loss: 0.0163 - mse: 0.0163\n",
      "Epoch 90: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0165 - mse: 0.0165 - val_loss: 0.2661 - val_mse: 0.2661\n",
      "Epoch 91/200\n",
      " 93/115 [=======================>......] - ETA: 0s - loss: 0.0306 - mse: 0.0306\n",
      "Epoch 91: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0374 - mse: 0.0374 - val_loss: 0.3097 - val_mse: 0.3097\n",
      "Epoch 92/200\n",
      "102/115 [=========================>....] - ETA: 0s - loss: 0.0516 - mse: 0.0516\n",
      "Epoch 92: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0497 - mse: 0.0497 - val_loss: 0.2944 - val_mse: 0.2944\n",
      "Epoch 93/200\n",
      "104/115 [==========================>...] - ETA: 0s - loss: 0.0364 - mse: 0.0364\n",
      "Epoch 93: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0357 - mse: 0.0357 - val_loss: 0.3042 - val_mse: 0.3042\n",
      "Epoch 94/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 93/115 [=======================>......] - ETA: 0s - loss: 0.0523 - mse: 0.0523\n",
      "Epoch 94: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0550 - mse: 0.0550 - val_loss: 0.2947 - val_mse: 0.2947\n",
      "Epoch 95/200\n",
      "102/115 [=========================>....] - ETA: 0s - loss: 0.0915 - mse: 0.0915\n",
      "Epoch 95: val_loss did not improve from 0.23536\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.2869 - val_mse: 0.2869\n"
     ]
    }
   ],
   "source": [
    "history3_2 = model3_2.fit(X_train,y_train, validation_data=(X_test,y_test), epochs=200, batch_size=10, callbacks=[early_stopping_callback,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "c51aef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss of test set\n",
    "y_vloss3_2 = history3_2.history['val_loss']\n",
    "# loss of training set\n",
    "y_loss3_2 = history3_2.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "95c60a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### 3rd - 2 trial #####\n",
    "# draw a graph\n",
    "x_len = np.arange(len(y_loss3_2))\n",
    "plt.plot(x_len,y_vloss3_2, marker='.', c='red', label='Testset_loss')\n",
    "plt.plot(x_len,y_loss3_2, marker='.', c='blue', label='Trainset_loss')\n",
    "#------------------\n",
    "plt.xlim(-0.5,5)\n",
    "#------------------\n",
    "# labeling\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "f712c81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 2ms/step - loss: 0.2869 - mse: 0.2869\n",
      "Test Accuracy : [0.2869018614292145, 0.2869018614292145]\n"
     ]
    }
   ],
   "source": [
    "print('Test Accuracy : {}'.format(model3_2.evaluate(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "e8eaa801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>287004.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>100139.734375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>210437.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>225651.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>141183.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2915</td>\n",
       "      <td>42633.191406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2916</td>\n",
       "      <td>44655.222656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2917</td>\n",
       "      <td>402929.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2918</td>\n",
       "      <td>123030.039062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2919</td>\n",
       "      <td>414338.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id      SalePrice\n",
       "0     1461  287004.031250\n",
       "1     1462  100139.734375\n",
       "2     1463  210437.250000\n",
       "3     1464  225651.125000\n",
       "4     1465  141183.437500\n",
       "...    ...            ...\n",
       "1454  2915   42633.191406\n",
       "1455  2916   44655.222656\n",
       "1456  2917  402929.312500\n",
       "1457  2918  123030.039062\n",
       "1458  2919  414338.750000\n",
       "\n",
       "[1459 rows x 2 columns]"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_from_model = model3_2.predict(df_house_test_extracted2b_predicted)\n",
    "df_house_test['SalePrice'] = 0\n",
    "df_house_test['SalePrice'] = np.expm1(result_from_model)\n",
    "df_house_test[['Id','SalePrice']].to_csv('C:/Users/thsong/submission15_2.csv',index=False)\n",
    "df_house_test[['Id','SalePrice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bef43d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "332b0b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3_3 = Sequential()\n",
    "#--------------------------------------------\n",
    "model3_3.add(Dense(64, input_shape=(43,)))\n",
    "model3_3.add(Activation('relu'))\n",
    "#----------------------------------\n",
    "model3_3.add(Dense(32))\n",
    "model3_3.add(Activation('relu'))\n",
    "#----------------------------------\n",
    "model3_3.add(Dense(8))\n",
    "model3_3.add(Activation('relu'))\n",
    "#----------------------------------\n",
    "model3_3.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "af7032e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "107/115 [==========================>...] - ETA: 0s - loss: 40.7725 - mse: 40.7725 \n",
      "Epoch 1: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 1s 3ms/step - loss: 38.3935 - mse: 38.3935 - val_loss: 3.3214 - val_mse: 3.3214\n",
      "Epoch 2/200\n",
      "105/115 [==========================>...] - ETA: 0s - loss: 1.9691 - mse: 1.9691\n",
      "Epoch 2: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 1.9499 - mse: 1.9499 - val_loss: 1.7067 - val_mse: 1.7067\n",
      "Epoch 3/200\n",
      "104/115 [==========================>...] - ETA: 0s - loss: 1.2342 - mse: 1.2342\n",
      "Epoch 3: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 1.2559 - mse: 1.2559 - val_loss: 1.3372 - val_mse: 1.3372\n",
      "Epoch 4/200\n",
      "103/115 [=========================>....] - ETA: 0s - loss: 1.0209 - mse: 1.0209\n",
      "Epoch 4: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 1.0197 - mse: 1.0197 - val_loss: 1.1691 - val_mse: 1.1691\n",
      "Epoch 5/200\n",
      "107/115 [==========================>...] - ETA: 0s - loss: 0.8635 - mse: 0.8635\n",
      "Epoch 5: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.8647 - mse: 0.8647 - val_loss: 1.0277 - val_mse: 1.0277\n",
      "Epoch 6/200\n",
      "100/115 [=========================>....] - ETA: 0s - loss: 0.7222 - mse: 0.7222\n",
      "Epoch 6: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.7423 - mse: 0.7423 - val_loss: 0.9723 - val_mse: 0.9723\n",
      "Epoch 7/200\n",
      "103/115 [=========================>....] - ETA: 0s - loss: 0.6379 - mse: 0.6379\n",
      "Epoch 7: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6417 - mse: 0.6417 - val_loss: 0.9038 - val_mse: 0.9038\n",
      "Epoch 8/200\n",
      "106/115 [==========================>...] - ETA: 0s - loss: 0.5632 - mse: 0.5632\n",
      "Epoch 8: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.5647 - mse: 0.5647 - val_loss: 0.8565 - val_mse: 0.8565\n",
      "Epoch 9/200\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.4794 - mse: 0.4794\n",
      "Epoch 9: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.4889 - mse: 0.4889 - val_loss: 0.7759 - val_mse: 0.7759\n",
      "Epoch 10/200\n",
      " 92/115 [=======================>......] - ETA: 0s - loss: 0.4261 - mse: 0.4261\n",
      "Epoch 10: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.4204 - mse: 0.4204 - val_loss: 0.7025 - val_mse: 0.7025\n",
      "Epoch 11/200\n",
      " 90/115 [======================>.......] - ETA: 0s - loss: 0.3695 - mse: 0.3695\n",
      "Epoch 11: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.3684 - mse: 0.3684 - val_loss: 0.7276 - val_mse: 0.7276\n",
      "Epoch 12/200\n",
      " 97/115 [========================>.....] - ETA: 0s - loss: 0.3285 - mse: 0.3285\n",
      "Epoch 12: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.3327 - mse: 0.3327 - val_loss: 0.7194 - val_mse: 0.7194\n",
      "Epoch 13/200\n",
      " 94/115 [=======================>......] - ETA: 0s - loss: 0.3371 - mse: 0.3371\n",
      "Epoch 13: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.3318 - mse: 0.3318 - val_loss: 0.5952 - val_mse: 0.5952\n",
      "Epoch 14/200\n",
      "101/115 [=========================>....] - ETA: 0s - loss: 0.2706 - mse: 0.2706\n",
      "Epoch 14: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2736 - mse: 0.2736 - val_loss: 0.6012 - val_mse: 0.6012\n",
      "Epoch 15/200\n",
      "107/115 [==========================>...] - ETA: 0s - loss: 0.2536 - mse: 0.2536\n",
      "Epoch 15: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2527 - mse: 0.2527 - val_loss: 0.5838 - val_mse: 0.5838\n",
      "Epoch 16/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.2233 - mse: 0.2233\n",
      "Epoch 16: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2203 - mse: 0.2203 - val_loss: 0.5768 - val_mse: 0.5768\n",
      "Epoch 17/200\n",
      "107/115 [==========================>...] - ETA: 0s - loss: 0.2140 - mse: 0.2140\n",
      "Epoch 17: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2146 - mse: 0.2146 - val_loss: 0.5358 - val_mse: 0.5358\n",
      "Epoch 18/200\n",
      "106/115 [==========================>...] - ETA: 0s - loss: 0.2018 - mse: 0.2018\n",
      "Epoch 18: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2046 - mse: 0.2046 - val_loss: 0.5129 - val_mse: 0.5129\n",
      "Epoch 19/200\n",
      "103/115 [=========================>....] - ETA: 0s - loss: 0.1801 - mse: 0.1801\n",
      "Epoch 19: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1770 - mse: 0.1770 - val_loss: 0.4905 - val_mse: 0.4905\n",
      "Epoch 20/200\n",
      "102/115 [=========================>....] - ETA: 0s - loss: 0.1549 - mse: 0.1549\n",
      "Epoch 20: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1551 - mse: 0.1551 - val_loss: 0.4739 - val_mse: 0.4739\n",
      "Epoch 21/200\n",
      "101/115 [=========================>....] - ETA: 0s - loss: 0.1498 - mse: 0.1498\n",
      "Epoch 21: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1460 - mse: 0.1460 - val_loss: 0.4453 - val_mse: 0.4453\n",
      "Epoch 22/200\n",
      "101/115 [=========================>....] - ETA: 0s - loss: 0.1316 - mse: 0.1316\n",
      "Epoch 22: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1360 - mse: 0.1360 - val_loss: 0.4592 - val_mse: 0.4592\n",
      "Epoch 23/200\n",
      " 97/115 [========================>.....] - ETA: 0s - loss: 0.1394 - mse: 0.1394\n",
      "Epoch 23: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1369 - mse: 0.1369 - val_loss: 0.4598 - val_mse: 0.4598\n",
      "Epoch 24/200\n",
      "107/115 [==========================>...] - ETA: 0s - loss: 0.1169 - mse: 0.1169\n",
      "Epoch 24: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1164 - mse: 0.1164 - val_loss: 0.4730 - val_mse: 0.4730\n",
      "Epoch 25/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.1075 - mse: 0.1075\n",
      "Epoch 25: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1086 - mse: 0.1086 - val_loss: 0.4555 - val_mse: 0.4555\n",
      "Epoch 26/200\n",
      "106/115 [==========================>...] - ETA: 0s - loss: 0.1104 - mse: 0.1104\n",
      "Epoch 26: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1099 - mse: 0.1099 - val_loss: 0.4066 - val_mse: 0.4066\n",
      "Epoch 27/200\n",
      " 98/115 [========================>.....] - ETA: 0s - loss: 0.0904 - mse: 0.0904\n",
      "Epoch 27: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.4175 - val_mse: 0.4175\n",
      "Epoch 28/200\n",
      "107/115 [==========================>...] - ETA: 0s - loss: 0.0905 - mse: 0.0905\n",
      "Epoch 28: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0916 - mse: 0.0916 - val_loss: 0.4415 - val_mse: 0.4415\n",
      "Epoch 29/200\n",
      "105/115 [==========================>...] - ETA: 0s - loss: 0.0936 - mse: 0.0936\n",
      "Epoch 29: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0947 - mse: 0.0947 - val_loss: 0.4328 - val_mse: 0.4328\n",
      "Epoch 30/200\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.0778 - mse: 0.0778\n",
      "Epoch 30: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0778 - mse: 0.0778 - val_loss: 0.4031 - val_mse: 0.4031\n",
      "Epoch 31/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0712 - mse: 0.0712\n",
      "Epoch 31: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0725 - mse: 0.0725 - val_loss: 0.3974 - val_mse: 0.3974\n",
      "Epoch 32/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/115 [==========================>...] - ETA: 0s - loss: 0.0690 - mse: 0.0690\n",
      "Epoch 32: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0697 - mse: 0.0697 - val_loss: 0.3952 - val_mse: 0.3952\n",
      "Epoch 33/200\n",
      "107/115 [==========================>...] - ETA: 0s - loss: 0.0653 - mse: 0.0653\n",
      "Epoch 33: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0658 - mse: 0.0658 - val_loss: 0.3717 - val_mse: 0.3717\n",
      "Epoch 34/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0600 - mse: 0.0600\n",
      "Epoch 34: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0599 - mse: 0.0599 - val_loss: 0.3899 - val_mse: 0.3899\n",
      "Epoch 35/200\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.0613 - mse: 0.0613\n",
      "Epoch 35: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0609 - mse: 0.0609 - val_loss: 0.3645 - val_mse: 0.3645\n",
      "Epoch 36/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0583 - mse: 0.0583\n",
      "Epoch 36: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0588 - mse: 0.0588 - val_loss: 0.3544 - val_mse: 0.3544\n",
      "Epoch 37/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0510 - mse: 0.0510\n",
      "Epoch 37: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0518 - mse: 0.0518 - val_loss: 0.3393 - val_mse: 0.3393\n",
      "Epoch 38/200\n",
      "106/115 [==========================>...] - ETA: 0s - loss: 0.0528 - mse: 0.0528\n",
      "Epoch 38: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0531 - mse: 0.0531 - val_loss: 0.3467 - val_mse: 0.3467\n",
      "Epoch 39/200\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.0444 - mse: 0.0444\n",
      "Epoch 39: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0449 - mse: 0.0449 - val_loss: 0.3436 - val_mse: 0.3436\n",
      "Epoch 40/200\n",
      " 92/115 [=======================>......] - ETA: 0s - loss: 0.0432 - mse: 0.0432\n",
      "Epoch 40: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0431 - mse: 0.0431 - val_loss: 0.3559 - val_mse: 0.3559\n",
      "Epoch 41/200\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.0434 - mse: 0.0434\n",
      "Epoch 41: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0436 - mse: 0.0436 - val_loss: 0.3502 - val_mse: 0.3502\n",
      "Epoch 42/200\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0577 - mse: 0.0577\n",
      "Epoch 42: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0577 - mse: 0.0577 - val_loss: 0.3498 - val_mse: 0.3498\n",
      "Epoch 43/200\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.0446 - mse: 0.0446\n",
      "Epoch 43: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0445 - mse: 0.0445 - val_loss: 0.3473 - val_mse: 0.3473\n",
      "Epoch 44/200\n",
      "107/115 [==========================>...] - ETA: 0s - loss: 0.0383 - mse: 0.0383\n",
      "Epoch 44: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.3474 - val_mse: 0.3474\n",
      "Epoch 45/200\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.0380 - mse: 0.0380\n",
      "Epoch 45: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0405 - mse: 0.0405 - val_loss: 0.3493 - val_mse: 0.3493\n",
      "Epoch 46/200\n",
      "105/115 [==========================>...] - ETA: 0s - loss: 0.0449 - mse: 0.0449\n",
      "Epoch 46: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0452 - mse: 0.0452 - val_loss: 0.3450 - val_mse: 0.3450\n",
      "Epoch 47/200\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.0357 - mse: 0.0357\n",
      "Epoch 47: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0357 - mse: 0.0357 - val_loss: 0.3377 - val_mse: 0.3377\n",
      "Epoch 48/200\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
      "Epoch 48: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0314 - mse: 0.0314 - val_loss: 0.3431 - val_mse: 0.3431\n",
      "Epoch 49/200\n",
      "105/115 [==========================>...] - ETA: 0s - loss: 0.0333 - mse: 0.0333\n",
      "Epoch 49: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0348 - mse: 0.0348 - val_loss: 0.3436 - val_mse: 0.3436\n",
      "Epoch 50/200\n",
      "103/115 [=========================>....] - ETA: 0s - loss: 0.0324 - mse: 0.0324\n",
      "Epoch 50: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0333 - mse: 0.0333 - val_loss: 0.3612 - val_mse: 0.3612\n",
      "Epoch 51/200\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.0299 - mse: 0.0299\n",
      "Epoch 51: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0301 - mse: 0.0301 - val_loss: 0.3322 - val_mse: 0.3322\n",
      "Epoch 52/200\n",
      "107/115 [==========================>...] - ETA: 0s - loss: 0.0297 - mse: 0.0297\n",
      "Epoch 52: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0295 - mse: 0.0295 - val_loss: 0.3527 - val_mse: 0.3527\n",
      "Epoch 53/200\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
      "Epoch 53: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0311 - mse: 0.0311 - val_loss: 0.3243 - val_mse: 0.3243\n",
      "Epoch 54/200\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.0290 - mse: 0.0290\n",
      "Epoch 54: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0292 - mse: 0.0292 - val_loss: 0.3348 - val_mse: 0.3348\n",
      "Epoch 55/200\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.0322 - mse: 0.0322\n",
      "Epoch 55: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0321 - mse: 0.0321 - val_loss: 0.3260 - val_mse: 0.3260\n",
      "Epoch 56/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0267 - mse: 0.0267\n",
      "Epoch 56: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0265 - mse: 0.0265 - val_loss: 0.3327 - val_mse: 0.3327\n",
      "Epoch 57/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0283 - mse: 0.0283\n",
      "Epoch 57: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0287 - mse: 0.0287 - val_loss: 0.3271 - val_mse: 0.3271\n",
      "Epoch 58/200\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.0322 - mse: 0.0322\n",
      "Epoch 58: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0321 - mse: 0.0321 - val_loss: 0.3136 - val_mse: 0.3136\n",
      "Epoch 59/200\n",
      "107/115 [==========================>...] - ETA: 0s - loss: 0.0277 - mse: 0.0277\n",
      "Epoch 59: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0279 - mse: 0.0279 - val_loss: 0.3358 - val_mse: 0.3358\n",
      "Epoch 60/200\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.0405 - mse: 0.0405\n",
      "Epoch 60: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0408 - mse: 0.0408 - val_loss: 0.2963 - val_mse: 0.2963\n",
      "Epoch 61/200\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.0309 - mse: 0.0309\n",
      "Epoch 61: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0310 - mse: 0.0310 - val_loss: 0.2966 - val_mse: 0.2966\n",
      "Epoch 62/200\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.0264 - mse: 0.0264\n",
      "Epoch 62: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0265 - mse: 0.0265 - val_loss: 0.3066 - val_mse: 0.3066\n",
      "Epoch 63/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/115 [==========================>...] - ETA: 0s - loss: 0.0344 - mse: 0.0344\n",
      "Epoch 63: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0334 - mse: 0.0334 - val_loss: 0.3035 - val_mse: 0.3035\n",
      "Epoch 64/200\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.0290 - mse: 0.0290\n",
      "Epoch 64: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0291 - mse: 0.0291 - val_loss: 0.3273 - val_mse: 0.3273\n",
      "Epoch 65/200\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.0383 - mse: 0.0383\n",
      "Epoch 65: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.3178 - val_mse: 0.3178\n",
      "Epoch 66/200\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.0290 - mse: 0.0290\n",
      "Epoch 66: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0290 - mse: 0.0290 - val_loss: 0.2968 - val_mse: 0.2968\n",
      "Epoch 67/200\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.0323 - mse: 0.0323\n",
      "Epoch 67: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0323 - mse: 0.0323 - val_loss: 0.3056 - val_mse: 0.3056\n",
      "Epoch 68/200\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.0211 - mse: 0.0211\n",
      "Epoch 68: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0214 - mse: 0.0214 - val_loss: 0.2917 - val_mse: 0.2917\n",
      "Epoch 69/200\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.0268 - mse: 0.0268\n",
      "Epoch 69: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0265 - mse: 0.0265 - val_loss: 0.2782 - val_mse: 0.2782\n",
      "Epoch 70/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0214 - mse: 0.0214\n",
      "Epoch 70: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0213 - mse: 0.0213 - val_loss: 0.3048 - val_mse: 0.3048\n",
      "Epoch 71/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0223 - mse: 0.0223\n",
      "Epoch 71: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0226 - mse: 0.0226 - val_loss: 0.2932 - val_mse: 0.2932\n",
      "Epoch 72/200\n",
      "107/115 [==========================>...] - ETA: 0s - loss: 0.0215 - mse: 0.0215\n",
      "Epoch 72: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 0.3066 - val_mse: 0.3066\n",
      "Epoch 73/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0206 - mse: 0.0206\n",
      "Epoch 73: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0211 - mse: 0.0211 - val_loss: 0.2857 - val_mse: 0.2857\n",
      "Epoch 74/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0337 - mse: 0.0337\n",
      "Epoch 74: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0354 - mse: 0.0354 - val_loss: 0.3221 - val_mse: 0.3221\n",
      "Epoch 75/200\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.0461 - mse: 0.0461\n",
      "Epoch 75: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0457 - mse: 0.0457 - val_loss: 0.3232 - val_mse: 0.3232\n",
      "Epoch 76/200\n",
      " 97/115 [========================>.....] - ETA: 0s - loss: 0.0497 - mse: 0.0497\n",
      "Epoch 76: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0485 - mse: 0.0485 - val_loss: 0.2667 - val_mse: 0.2667\n",
      "Epoch 77/200\n",
      " 94/115 [=======================>......] - ETA: 0s - loss: 0.0271 - mse: 0.0271\n",
      "Epoch 77: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0273 - mse: 0.0273 - val_loss: 0.2622 - val_mse: 0.2622\n",
      "Epoch 78/200\n",
      "104/115 [==========================>...] - ETA: 0s - loss: 0.0191 - mse: 0.0191\n",
      "Epoch 78: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0188 - mse: 0.0188 - val_loss: 0.2653 - val_mse: 0.2653\n",
      "Epoch 79/200\n",
      " 98/115 [========================>.....] - ETA: 0s - loss: 0.0201 - mse: 0.0201\n",
      "Epoch 79: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0202 - mse: 0.0202 - val_loss: 0.2819 - val_mse: 0.2819\n",
      "Epoch 80/200\n",
      "105/115 [==========================>...] - ETA: 0s - loss: 0.0146 - mse: 0.0146\n",
      "Epoch 80: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0146 - mse: 0.0146 - val_loss: 0.2767 - val_mse: 0.2767\n",
      "Epoch 81/200\n",
      "105/115 [==========================>...] - ETA: 0s - loss: 0.0191 - mse: 0.0191\n",
      "Epoch 81: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0190 - mse: 0.0190 - val_loss: 0.2700 - val_mse: 0.2700\n",
      "Epoch 82/200\n",
      " 91/115 [======================>.......] - ETA: 0s - loss: 0.0196 - mse: 0.0196\n",
      "Epoch 82: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0219 - mse: 0.0219 - val_loss: 0.2507 - val_mse: 0.2507\n",
      "Epoch 83/200\n",
      " 92/115 [=======================>......] - ETA: 0s - loss: 0.0280 - mse: 0.0280\n",
      "Epoch 83: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0267 - mse: 0.0267 - val_loss: 0.2509 - val_mse: 0.2509\n",
      "Epoch 84/200\n",
      " 96/115 [========================>.....] - ETA: 0s - loss: 0.0185 - mse: 0.0185\n",
      "Epoch 84: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0178 - mse: 0.0178 - val_loss: 0.2691 - val_mse: 0.2691\n",
      "Epoch 85/200\n",
      " 94/115 [=======================>......] - ETA: 0s - loss: 0.0273 - mse: 0.0273\n",
      "Epoch 85: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0255 - mse: 0.0255 - val_loss: 0.2713 - val_mse: 0.2713\n",
      "Epoch 86/200\n",
      " 92/115 [=======================>......] - ETA: 0s - loss: 0.0187 - mse: 0.0187\n",
      "Epoch 86: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0183 - mse: 0.0183 - val_loss: 0.2813 - val_mse: 0.2813\n",
      "Epoch 87/200\n",
      "100/115 [=========================>....] - ETA: 0s - loss: 0.0211 - mse: 0.0211\n",
      "Epoch 87: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0214 - mse: 0.0214 - val_loss: 0.2740 - val_mse: 0.2740\n",
      "Epoch 88/200\n",
      "103/115 [=========================>....] - ETA: 0s - loss: 0.0232 - mse: 0.0232\n",
      "Epoch 88: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0239 - mse: 0.0239 - val_loss: 0.2794 - val_mse: 0.2794\n",
      "Epoch 89/200\n",
      " 97/115 [========================>.....] - ETA: 0s - loss: 0.0304 - mse: 0.0304\n",
      "Epoch 89: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0299 - mse: 0.0299 - val_loss: 0.2778 - val_mse: 0.2778\n",
      "Epoch 90/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0247 - mse: 0.0247\n",
      "Epoch 90: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.2615 - val_mse: 0.2615\n",
      "Epoch 91/200\n",
      " 97/115 [========================>.....] - ETA: 0s - loss: 0.0246 - mse: 0.0246\n",
      "Epoch 91: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0239 - mse: 0.0239 - val_loss: 0.2716 - val_mse: 0.2716\n",
      "Epoch 92/200\n",
      "101/115 [=========================>....] - ETA: 0s - loss: 0.0232 - mse: 0.0232\n",
      "Epoch 92: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0225 - mse: 0.0225 - val_loss: 0.2643 - val_mse: 0.2643\n"
     ]
    }
   ],
   "source": [
    "model3_3.compile(loss='mean_squared_error', metrics=['mse'], optimizer='adam')\n",
    "history3_3 = model3_3.fit(X_train,y_train, validation_data=(X_test,y_test), epochs=200, batch_size=10, callbacks=[early_stopping_callback,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "e3406a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " 80/115 [===================>..........] - ETA: 0s - loss: 48.9494 - mse: 48.9494\n",
      "Epoch 1: val_loss did not improve from 0.22063\n",
      "115/115 [==============================] - 1s 3ms/step - loss: 35.3759 - mse: 35.3759 - val_loss: 1.8674 - val_mse: 1.8674\n",
      "Epoch 2/200\n",
      " 75/115 [==================>...........] - ETA: 0s - loss: 0.7094 - mse: 0.7094\n",
      "Epoch 2: val_loss improved from 0.22063 to 0.15666, saving model to C:/Users/thsong/dl_model\\2-0.1566554754972458.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.5286 - mse: 0.5286 - val_loss: 0.1567 - val_mse: 0.1567\n",
      "Epoch 3/200\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.1598 - mse: 0.1598\n",
      "Epoch 3: val_loss improved from 0.15666 to 0.13732, saving model to C:/Users/thsong/dl_model\\3-0.1373203545808792.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1593 - mse: 0.1593 - val_loss: 0.1373 - val_mse: 0.1373\n",
      "Epoch 4/200\n",
      " 77/115 [===================>..........] - ETA: 0s - loss: 0.1483 - mse: 0.1483\n",
      "Epoch 4: val_loss improved from 0.13732 to 0.13674, saving model to C:/Users/thsong/dl_model\\4-0.1367437094449997.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1560 - mse: 0.1560 - val_loss: 0.1367 - val_mse: 0.1367\n",
      "Epoch 5/200\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.1548 - mse: 0.1548\n",
      "Epoch 5: val_loss improved from 0.13674 to 0.13658, saving model to C:/Users/thsong/dl_model\\5-0.13657571375370026.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1558 - mse: 0.1558 - val_loss: 0.1366 - val_mse: 0.1366\n",
      "Epoch 6/200\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.1552 - mse: 0.1552\n",
      "Epoch 6: val_loss did not improve from 0.13658\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1557 - mse: 0.1557 - val_loss: 0.1366 - val_mse: 0.1366\n",
      "Epoch 7/200\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.1577 - mse: 0.1577\n",
      "Epoch 7: val_loss did not improve from 0.13658\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1558 - mse: 0.1558 - val_loss: 0.1368 - val_mse: 0.1368\n",
      "Epoch 8/200\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.1560 - mse: 0.1560\n",
      "Epoch 8: val_loss did not improve from 0.13658\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1558 - mse: 0.1558 - val_loss: 0.1369 - val_mse: 0.1369\n",
      "Epoch 9/200\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.1547 - mse: 0.1547\n",
      "Epoch 9: val_loss did not improve from 0.13658\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1559 - mse: 0.1559 - val_loss: 0.1367 - val_mse: 0.1367\n",
      "Epoch 10/200\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.1574 - mse: 0.1574\n",
      "Epoch 10: val_loss did not improve from 0.13658\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1557 - mse: 0.1557 - val_loss: 0.1371 - val_mse: 0.1371\n",
      "Epoch 11/200\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1555 - mse: 0.1555\n",
      "Epoch 11: val_loss improved from 0.13658 to 0.13656, saving model to C:/Users/thsong/dl_model\\11-0.1365565061569214.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1555 - mse: 0.1555 - val_loss: 0.1366 - val_mse: 0.1366\n",
      "Epoch 12/200\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.1568 - mse: 0.1568\n",
      "Epoch 12: val_loss did not improve from 0.13656\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1557 - mse: 0.1557 - val_loss: 0.1366 - val_mse: 0.1366\n",
      "Epoch 13/200\n",
      " 76/115 [==================>...........] - ETA: 0s - loss: 0.1555 - mse: 0.1555\n",
      "Epoch 13: val_loss did not improve from 0.13656\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1555 - mse: 0.1555 - val_loss: 0.1374 - val_mse: 0.1374\n",
      "Epoch 14/200\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.1542 - mse: 0.1542\n",
      "Epoch 14: val_loss did not improve from 0.13656\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1557 - mse: 0.1557 - val_loss: 0.1369 - val_mse: 0.1369\n",
      "Epoch 15/200\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.1555 - mse: 0.1555\n",
      "Epoch 15: val_loss improved from 0.13656 to 0.13654, saving model to C:/Users/thsong/dl_model\\15-0.1365387737751007.hdf5\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1555 - mse: 0.1555 - val_loss: 0.1365 - val_mse: 0.1365\n",
      "Epoch 16/200\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1557 - mse: 0.1557\n",
      "Epoch 16: val_loss did not improve from 0.13654\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1557 - mse: 0.1557 - val_loss: 0.1367 - val_mse: 0.1367\n",
      "Epoch 17/200\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.1534 - mse: 0.1534\n",
      "Epoch 17: val_loss did not improve from 0.13654\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1556 - mse: 0.1556 - val_loss: 0.1366 - val_mse: 0.1366\n",
      "Epoch 18/200\n",
      " 92/115 [=======================>......] - ETA: 0s - loss: 0.1514 - mse: 0.1514\n",
      "Epoch 18: val_loss did not improve from 0.13654\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1555 - mse: 0.1555 - val_loss: 0.1367 - val_mse: 0.1367\n",
      "Epoch 19/200\n",
      " 74/115 [==================>...........] - ETA: 0s - loss: 0.1546 - mse: 0.1546\n",
      "Epoch 19: val_loss did not improve from 0.13654\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1556 - mse: 0.1556 - val_loss: 0.1367 - val_mse: 0.1367\n",
      "Epoch 20/200\n",
      " 98/115 [========================>.....] - ETA: 0s - loss: 0.1524 - mse: 0.1524\n",
      "Epoch 20: val_loss did not improve from 0.13654\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1556 - mse: 0.1556 - val_loss: 0.1366 - val_mse: 0.1366\n",
      "Epoch 21/200\n",
      " 78/115 [===================>..........] - ETA: 0s - loss: 0.1553 - mse: 0.1553\n",
      "Epoch 21: val_loss did not improve from 0.13654\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1553 - mse: 0.1553 - val_loss: 0.1377 - val_mse: 0.1377\n",
      "Epoch 22/200\n",
      " 78/115 [===================>..........] - ETA: 0s - loss: 0.1586 - mse: 0.1586\n",
      "Epoch 22: val_loss did not improve from 0.13654\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1556 - mse: 0.1556 - val_loss: 0.1373 - val_mse: 0.1373\n",
      "Epoch 23/200\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.1555 - mse: 0.1555\n",
      "Epoch 23: val_loss did not improve from 0.13654\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1555 - mse: 0.1555 - val_loss: 0.1369 - val_mse: 0.1369\n",
      "Epoch 24/200\n",
      " 76/115 [==================>...........] - ETA: 0s - loss: 0.1598 - mse: 0.1598\n",
      "Epoch 24: val_loss did not improve from 0.13654\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1554 - mse: 0.1554 - val_loss: 0.1374 - val_mse: 0.1374\n",
      "Epoch 25/200\n",
      " 77/115 [===================>..........] - ETA: 0s - loss: 0.1542 - mse: 0.1542\n",
      "Epoch 25: val_loss did not improve from 0.13654\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1555 - mse: 0.1555 - val_loss: 0.1370 - val_mse: 0.1370\n"
     ]
    }
   ],
   "source": [
    "model3_3.compile(loss='mean_squared_error', metrics=['mse'], optimizer='SGD')\n",
    "history3_3 = model3_3.fit(X_train,y_train, validation_data=(X_test,y_test), epochs=200, batch_size=10, callbacks=[early_stopping_callback,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "6f5d88cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 1ms/step - loss: 0.2643 - mse: 0.2643\n",
      "Test Accuracy : [0.26432883739471436, 0.26432883739471436]\n"
     ]
    }
   ],
   "source": [
    "print('Test Accuracy : {}'.format(model3_3.evaluate(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "fd37366f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 1ms/step - loss: 0.2500 - mse: 0.2500\n",
      "Test Accuracy : [0.249980166554451, 0.249980166554451]\n"
     ]
    }
   ],
   "source": [
    "# RMS Prop\n",
    "print('Test Accuracy : {}'.format(model3_3.evaluate(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "89828ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 1ms/step - loss: 0.1370 - mse: 0.1370\n",
      "Test Accuracy : [0.13696162402629852, 0.13696162402629852]\n"
     ]
    }
   ],
   "source": [
    "# SGD\n",
    "print('Test Accuracy : {}'.format(model3_3.evaluate(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "ebcd72b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loss of test set\n",
    "y_vloss3_3 = history3_3.history['val_loss']\n",
    "# loss of training set\n",
    "y_loss3_3 = history3_3.history['loss']\n",
    "#---------------------------------------------\n",
    "##### 3rd - 3 trial #####\n",
    "# draw a graph\n",
    "x_len = np.arange(len(y_loss3_3))\n",
    "plt.plot(x_len,y_vloss3_3, marker='.', c='red', label='Testset_loss')\n",
    "plt.plot(x_len,y_loss3_3, marker='.', c='blue', label='Trainset_loss')\n",
    "#------------------\n",
    "plt.xlim(-0.5,3)\n",
    "#------------------\n",
    "# labeling\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "f2467cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>93667.515625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>85401.382812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>291318.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>404094.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>201270.484375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2915</td>\n",
       "      <td>42800.667969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2916</td>\n",
       "      <td>61840.179688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2917</td>\n",
       "      <td>234762.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2918</td>\n",
       "      <td>78696.523438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2919</td>\n",
       "      <td>274924.843750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id      SalePrice\n",
       "0     1461   93667.515625\n",
       "1     1462   85401.382812\n",
       "2     1463  291318.312500\n",
       "3     1464  404094.937500\n",
       "4     1465  201270.484375\n",
       "...    ...            ...\n",
       "1454  2915   42800.667969\n",
       "1455  2916   61840.179688\n",
       "1456  2917  234762.062500\n",
       "1457  2918   78696.523438\n",
       "1458  2919  274924.843750\n",
       "\n",
       "[1459 rows x 2 columns]"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_from_model = model3_3.predict(df_house_test_extracted2b_predicted)\n",
    "df_house_test['SalePrice'] = 0\n",
    "df_house_test['SalePrice'] = np.expm1(result_from_model)\n",
    "df_house_test[['Id','SalePrice']].to_csv('C:/Users/thsong/submission15_2.csv',index=False)\n",
    "df_house_test[['Id','SalePrice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd79c80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "f1fbda47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "58/58 [==============================] - ETA: 0s - loss: 14.7541 - mse: 14.7541\n",
      "Epoch 1: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 1s 7ms/step - loss: 14.7541 - mse: 14.7541 - val_loss: 1.4385 - val_mse: 1.4385\n",
      "Epoch 2/300\n",
      "57/58 [============================>.] - ETA: 0s - loss: 1.0241 - mse: 1.0241\n",
      "Epoch 2: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 1.0261 - mse: 1.0261 - val_loss: 1.0981 - val_mse: 1.0981\n",
      "Epoch 3/300\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.6850 - mse: 0.6850\n",
      "Epoch 3: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.6844 - mse: 0.6844 - val_loss: 0.8615 - val_mse: 0.8615\n",
      "Epoch 4/300\n",
      "52/58 [=========================>....] - ETA: 0s - loss: 0.5439 - mse: 0.5439\n",
      "Epoch 4: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 0.5447 - mse: 0.5447 - val_loss: 0.8572 - val_mse: 0.8572\n",
      "Epoch 5/300\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3809 - mse: 0.3809\n",
      "Epoch 5: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.3809 - mse: 0.3809 - val_loss: 0.6969 - val_mse: 0.6969\n",
      "Epoch 6/300\n",
      "45/58 [======================>.......] - ETA: 0s - loss: 0.3065 - mse: 0.3065\n",
      "Epoch 6: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.3067 - mse: 0.3067 - val_loss: 0.6400 - val_mse: 0.6400\n",
      "Epoch 7/300\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.2732 - mse: 0.2732\n",
      "Epoch 7: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2723 - mse: 0.2723 - val_loss: 0.5866 - val_mse: 0.5866\n",
      "Epoch 8/300\n",
      "43/58 [=====================>........] - ETA: 0s - loss: 0.2175 - mse: 0.2175\n",
      "Epoch 8: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2150 - mse: 0.2150 - val_loss: 0.5284 - val_mse: 0.5284\n",
      "Epoch 9/300\n",
      "45/58 [======================>.......] - ETA: 0s - loss: 0.1706 - mse: 0.1706\n",
      "Epoch 9: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.1726 - mse: 0.1726 - val_loss: 0.5195 - val_mse: 0.5195\n",
      "Epoch 10/300\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1663 - mse: 0.1663\n",
      "Epoch 10: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.1663 - mse: 0.1663 - val_loss: 0.4741 - val_mse: 0.4741\n",
      "Epoch 11/300\n",
      "43/58 [=====================>........] - ETA: 0s - loss: 0.1574 - mse: 0.1574\n",
      "Epoch 11: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.1495 - mse: 0.1495 - val_loss: 0.4928 - val_mse: 0.4928\n",
      "Epoch 12/300\n",
      "48/58 [=======================>......] - ETA: 0s - loss: 0.1009 - mse: 0.1009\n",
      "Epoch 12: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.1077 - mse: 0.1077 - val_loss: 0.4795 - val_mse: 0.4795\n",
      "Epoch 13/300\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.1114 - mse: 0.1114\n",
      "Epoch 13: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 0.1109 - mse: 0.1109 - val_loss: 0.4432 - val_mse: 0.4432\n",
      "Epoch 14/300\n",
      "46/58 [======================>.......] - ETA: 0s - loss: 0.1068 - mse: 0.1068\n",
      "Epoch 14: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.1059 - mse: 0.1059 - val_loss: 0.4295 - val_mse: 0.4295\n",
      "Epoch 15/300\n",
      "48/58 [=======================>......] - ETA: 0s - loss: 0.0900 - mse: 0.0900\n",
      "Epoch 15: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0893 - mse: 0.0893 - val_loss: 0.4080 - val_mse: 0.4080\n",
      "Epoch 16/300\n",
      "48/58 [=======================>......] - ETA: 0s - loss: 0.0784 - mse: 0.0784\n",
      "Epoch 16: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0836 - mse: 0.0836 - val_loss: 0.4110 - val_mse: 0.4110\n",
      "Epoch 17/300\n",
      "50/58 [========================>.....] - ETA: 0s - loss: 0.0668 - mse: 0.0668\n",
      "Epoch 17: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0662 - mse: 0.0662 - val_loss: 0.4049 - val_mse: 0.4049\n",
      "Epoch 18/300\n",
      "46/58 [======================>.......] - ETA: 0s - loss: 0.0505 - mse: 0.0505\n",
      "Epoch 18: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0531 - mse: 0.0531 - val_loss: 0.3682 - val_mse: 0.3682\n",
      "Epoch 19/300\n",
      "46/58 [======================>.......] - ETA: 0s - loss: 0.0740 - mse: 0.0740\n",
      "Epoch 19: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0814 - mse: 0.0814 - val_loss: 0.4365 - val_mse: 0.4365\n",
      "Epoch 20/300\n",
      "47/58 [=======================>......] - ETA: 0s - loss: 0.1285 - mse: 0.1285\n",
      "Epoch 20: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.1220 - mse: 0.1220 - val_loss: 0.4494 - val_mse: 0.4494\n",
      "Epoch 21/300\n",
      "48/58 [=======================>......] - ETA: 0s - loss: 0.0994 - mse: 0.0994\n",
      "Epoch 21: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0951 - mse: 0.0951 - val_loss: 0.4004 - val_mse: 0.4004\n",
      "Epoch 22/300\n",
      "49/58 [========================>.....] - ETA: 0s - loss: 0.0648 - mse: 0.0648\n",
      "Epoch 22: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0659 - mse: 0.0659 - val_loss: 0.4026 - val_mse: 0.4026\n",
      "Epoch 23/300\n",
      "48/58 [=======================>......] - ETA: 0s - loss: 0.0557 - mse: 0.0557\n",
      "Epoch 23: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0544 - mse: 0.0544 - val_loss: 0.3559 - val_mse: 0.3559\n",
      "Epoch 24/300\n",
      "50/58 [========================>.....] - ETA: 0s - loss: 0.0485 - mse: 0.0485\n",
      "Epoch 24: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0509 - mse: 0.0509 - val_loss: 0.3671 - val_mse: 0.3671\n",
      "Epoch 25/300\n",
      "48/58 [=======================>......] - ETA: 0s - loss: 0.0511 - mse: 0.0511\n",
      "Epoch 25: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0523 - mse: 0.0523 - val_loss: 0.4226 - val_mse: 0.4226\n",
      "Epoch 26/300\n",
      "48/58 [=======================>......] - ETA: 0s - loss: 0.0830 - mse: 0.0830\n",
      "Epoch 26: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0795 - mse: 0.0795 - val_loss: 0.3798 - val_mse: 0.3798\n",
      "Epoch 27/300\n",
      "51/58 [=========================>....] - ETA: 0s - loss: 0.0643 - mse: 0.0643\n",
      "Epoch 27: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0686 - mse: 0.0686 - val_loss: 0.4298 - val_mse: 0.4298\n",
      "Epoch 28/300\n",
      "51/58 [=========================>....] - ETA: 0s - loss: 0.1030 - mse: 0.1030\n",
      "Epoch 28: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.1019 - mse: 0.1019 - val_loss: 0.3559 - val_mse: 0.3559\n",
      "Epoch 29/300\n",
      "51/58 [=========================>....] - ETA: 0s - loss: 0.0764 - mse: 0.0764\n",
      "Epoch 29: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0743 - mse: 0.0743 - val_loss: 0.3538 - val_mse: 0.3538\n",
      "Epoch 30/300\n",
      "50/58 [========================>.....] - ETA: 0s - loss: 0.0678 - mse: 0.0678\n",
      "Epoch 30: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0718 - mse: 0.0718 - val_loss: 0.4082 - val_mse: 0.4082\n",
      "Epoch 31/300\n",
      "52/58 [=========================>....] - ETA: 0s - loss: 0.0930 - mse: 0.0930\n",
      "Epoch 31: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0889 - mse: 0.0889 - val_loss: 0.3681 - val_mse: 0.3681\n",
      "Epoch 32/300\n",
      "49/58 [========================>.....] - ETA: 0s - loss: 0.0680 - mse: 0.0680\n",
      "Epoch 32: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0672 - mse: 0.0672 - val_loss: 0.3407 - val_mse: 0.3407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/300\n",
      "50/58 [========================>.....] - ETA: 0s - loss: 0.0841 - mse: 0.0841\n",
      "Epoch 33: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0840 - mse: 0.0840 - val_loss: 0.4007 - val_mse: 0.4007\n",
      "Epoch 34/300\n",
      "49/58 [========================>.....] - ETA: 0s - loss: 0.0922 - mse: 0.0922\n",
      "Epoch 34: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0947 - mse: 0.0947 - val_loss: 0.4033 - val_mse: 0.4033\n",
      "Epoch 35/300\n",
      "50/58 [========================>.....] - ETA: 0s - loss: 0.1147 - mse: 0.1147\n",
      "Epoch 35: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.1137 - mse: 0.1137 - val_loss: 0.3709 - val_mse: 0.3709\n",
      "Epoch 36/300\n",
      "44/58 [=====================>........] - ETA: 0s - loss: 0.1339 - mse: 0.1339\n",
      "Epoch 36: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.1324 - mse: 0.1324 - val_loss: 0.4036 - val_mse: 0.4036\n",
      "Epoch 37/300\n",
      "50/58 [========================>.....] - ETA: 0s - loss: 0.1586 - mse: 0.1586\n",
      "Epoch 37: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.1553 - mse: 0.1553 - val_loss: 0.4728 - val_mse: 0.4728\n",
      "Epoch 38/300\n",
      "50/58 [========================>.....] - ETA: 0s - loss: 0.1567 - mse: 0.1567\n",
      "Epoch 38: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.1555 - mse: 0.1555 - val_loss: 0.3601 - val_mse: 0.3601\n",
      "Epoch 39/300\n",
      "50/58 [========================>.....] - ETA: 0s - loss: 0.0942 - mse: 0.0942\n",
      "Epoch 39: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.3403 - val_mse: 0.3403\n",
      "Epoch 40/300\n",
      "49/58 [========================>.....] - ETA: 0s - loss: 0.0539 - mse: 0.0539\n",
      "Epoch 40: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0576 - mse: 0.0576 - val_loss: 0.3821 - val_mse: 0.3821\n",
      "Epoch 41/300\n",
      "47/58 [=======================>......] - ETA: 0s - loss: 0.0663 - mse: 0.0663\n",
      "Epoch 41: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0620 - mse: 0.0620 - val_loss: 0.2803 - val_mse: 0.2803\n",
      "Epoch 42/300\n",
      "48/58 [=======================>......] - ETA: 0s - loss: 0.0439 - mse: 0.0439\n",
      "Epoch 42: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0441 - mse: 0.0441 - val_loss: 0.3294 - val_mse: 0.3294\n",
      "Epoch 43/300\n",
      "46/58 [======================>.......] - ETA: 0s - loss: 0.0387 - mse: 0.0387\n",
      "Epoch 43: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0363 - mse: 0.0363 - val_loss: 0.2944 - val_mse: 0.2944\n",
      "Epoch 44/300\n",
      "46/58 [======================>.......] - ETA: 0s - loss: 0.0437 - mse: 0.0437\n",
      "Epoch 44: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0523 - mse: 0.0523 - val_loss: 0.3392 - val_mse: 0.3392\n",
      "Epoch 45/300\n",
      "46/58 [======================>.......] - ETA: 0s - loss: 0.0779 - mse: 0.0779\n",
      "Epoch 45: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0758 - mse: 0.0758 - val_loss: 0.4016 - val_mse: 0.4016\n",
      "Epoch 46/300\n",
      "47/58 [=======================>......] - ETA: 0s - loss: 0.0878 - mse: 0.0878\n",
      "Epoch 46: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0828 - mse: 0.0828 - val_loss: 0.3444 - val_mse: 0.3444\n",
      "Epoch 47/300\n",
      "48/58 [=======================>......] - ETA: 0s - loss: 0.0787 - mse: 0.0787\n",
      "Epoch 47: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0841 - mse: 0.0841 - val_loss: 0.3035 - val_mse: 0.3035\n",
      "Epoch 48/300\n",
      "48/58 [=======================>......] - ETA: 0s - loss: 0.0756 - mse: 0.0756\n",
      "Epoch 48: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0765 - mse: 0.0765 - val_loss: 0.3563 - val_mse: 0.3563\n",
      "Epoch 49/300\n",
      "51/58 [=========================>....] - ETA: 0s - loss: 0.0776 - mse: 0.0776\n",
      "Epoch 49: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0752 - mse: 0.0752 - val_loss: 0.3000 - val_mse: 0.3000\n",
      "Epoch 50/300\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.0678 - mse: 0.0678\n",
      "Epoch 50: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0678 - mse: 0.0678 - val_loss: 0.3158 - val_mse: 0.3158\n",
      "Epoch 51/300\n",
      "51/58 [=========================>....] - ETA: 0s - loss: 0.0593 - mse: 0.0593\n",
      "Epoch 51: val_loss did not improve from 0.04271\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 0.0621 - mse: 0.0621 - val_loss: 0.3012 - val_mse: 0.3012\n"
     ]
    }
   ],
   "source": [
    "### 3rd - 4 trial ###\n",
    "model3_4 = Sequential()\n",
    "#--------------------------------------------\n",
    "model3_4.add(Dense(1024, input_shape=(43,)))\n",
    "model3_4.add(Activation('relu'))\n",
    "#----------------------------------\n",
    "model3_4.add(Dense(256, input_shape=(43,)))\n",
    "model3_4.add(Activation('relu'))\n",
    "#----------------------------------\n",
    "model3_4.add(Dense(64, input_shape=(43,)))\n",
    "model3_4.add(Activation('relu'))\n",
    "#----------------------------------\n",
    "model3_4.add(Dense(16))\n",
    "model3_4.add(Activation('relu'))\n",
    "#----------------------------------\n",
    "model3_4.add(Dense(4))\n",
    "model3_4.add(Activation('relu'))\n",
    "#----------------------------------\n",
    "model3_4.add(Dense(1))\n",
    "model3_4.add(Activation('elu'))\n",
    "#==================================================================================\n",
    "model3_4.compile(loss='mean_squared_error', metrics=['mse'], optimizer='adam')\n",
    "history3_4 = model3_4.fit(X_train,y_train, validation_data=(X_test,y_test), epochs=300, batch_size=20, callbacks=[early_stopping_callback,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "0569df5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 3ms/step - loss: 0.4191 - mse: 0.4191\n",
      "Test Accuracy : [0.4191492795944214, 0.4191492795944214]\n"
     ]
    }
   ],
   "source": [
    "print('Test Accuracy : {}'.format(model3_4.evaluate(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "14a07676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0605 - mse: 0.0605\n",
      "Test Accuracy : [0.0604710578918457, 0.0604710578918457]\n"
     ]
    }
   ],
   "source": [
    "print('Test Accuracy : {}'.format(model3_4.evaluate(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "92d6aebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 2ms/step - loss: 0.3012 - mse: 0.3012\n",
      "Test Accuracy : [0.30116891860961914, 0.30116891860961914]\n"
     ]
    }
   ],
   "source": [
    "print('Test Accuracy : {}'.format(model3_4.evaluate(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "3a9d0e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>238171.765625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>101936.382812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>175452.203125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>375964.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>173549.390625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2915</td>\n",
       "      <td>44481.507812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2916</td>\n",
       "      <td>47049.640625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2917</td>\n",
       "      <td>217682.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2918</td>\n",
       "      <td>81919.085938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2919</td>\n",
       "      <td>316102.812500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id      SalePrice\n",
       "0     1461  238171.765625\n",
       "1     1462  101936.382812\n",
       "2     1463  175452.203125\n",
       "3     1464  375964.843750\n",
       "4     1465  173549.390625\n",
       "...    ...            ...\n",
       "1454  2915   44481.507812\n",
       "1455  2916   47049.640625\n",
       "1456  2917  217682.031250\n",
       "1457  2918   81919.085938\n",
       "1458  2919  316102.812500\n",
       "\n",
       "[1459 rows x 2 columns]"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_from_model = model3_4.predict(df_house_test_extracted2b_predicted)\n",
    "df_house_test['SalePrice'] = 0\n",
    "df_house_test['SalePrice'] = np.expm1(result_from_model)\n",
    "df_house_test[['Id','SalePrice']].to_csv('C:/Users/thsong/submission15_2.csv',index=False)\n",
    "df_house_test[['Id','SalePrice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b08470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "31911773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 143.1955 - mse: 143.1955\n",
      "Epoch 1: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 7ms/step - loss: 143.0958 - mse: 143.0958 - val_loss: 142.0612 - val_mse: 142.0612\n",
      "Epoch 2/500\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 140.4288 - mse: 140.4288\n",
      "Epoch 2: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 140.3688 - mse: 140.3688 - val_loss: 139.3521 - val_mse: 139.3521\n",
      "Epoch 3/500\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 137.6163 - mse: 137.6163\n",
      "Epoch 3: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 137.6812 - mse: 137.6812 - val_loss: 136.6801 - val_mse: 136.6801\n",
      "Epoch 4/500\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 134.9911 - mse: 134.9911\n",
      "Epoch 4: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 135.0316 - mse: 135.0315 - val_loss: 134.0459 - val_mse: 134.0459\n",
      "Epoch 5/500\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 132.4803 - mse: 132.4803\n",
      "Epoch 5: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 132.4189 - mse: 132.4189 - val_loss: 131.4480 - val_mse: 131.4480\n",
      "Epoch 6/500\n",
      "112/115 [============================>.] - ETA: 0s - loss: 129.8412 - mse: 129.8412\n",
      "Epoch 6: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 129.8423 - mse: 129.8423 - val_loss: 128.8856 - val_mse: 128.8856\n",
      "Epoch 7/500\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 127.3471 - mse: 127.3471\n",
      "Epoch 7: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 127.3015 - mse: 127.3015 - val_loss: 126.3594 - val_mse: 126.3594\n",
      "Epoch 8/500\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 124.8619 - mse: 124.8619\n",
      "Epoch 8: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 124.7959 - mse: 124.7959 - val_loss: 123.8677 - val_mse: 123.8677\n",
      "Epoch 9/500\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 122.3785 - mse: 122.3785\n",
      "Epoch 9: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 122.3245 - mse: 122.3245 - val_loss: 121.4103 - val_mse: 121.4103\n",
      "Epoch 10/500\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 119.9654 - mse: 119.9654\n",
      "Epoch 10: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 119.8867 - mse: 119.8867 - val_loss: 118.9849 - val_mse: 118.9849\n",
      "Epoch 11/500\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 117.4505 - mse: 117.4505\n",
      "Epoch 11: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 117.4817 - mse: 117.4817 - val_loss: 116.5945 - val_mse: 116.5945\n",
      "Epoch 12/500\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 115.0518 - mse: 115.0518\n",
      "Epoch 12: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 115.1091 - mse: 115.1091 - val_loss: 114.2347 - val_mse: 114.2347\n",
      "Epoch 13/500\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 112.8438 - mse: 112.8438\n",
      "Epoch 13: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 7ms/step - loss: 112.7686 - mse: 112.7686 - val_loss: 111.9054 - val_mse: 111.9054\n",
      "Epoch 14/500\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 110.5357 - mse: 110.5357\n",
      "Epoch 14: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 110.4599 - mse: 110.4599 - val_loss: 109.6096 - val_mse: 109.6096\n",
      "Epoch 15/500\n",
      "114/115 [============================>.] - ETA: 0s - loss: 108.1725 - mse: 108.1725\n",
      "Epoch 15: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 7ms/step - loss: 108.1819 - mse: 108.1819 - val_loss: 107.3441 - val_mse: 107.3441\n",
      "Epoch 16/500\n",
      "106/115 [==========================>...] - ETA: 0s - loss: 106.0134 - mse: 106.0134\n",
      "Epoch 16: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 105.9342 - mse: 105.9342 - val_loss: 105.1079 - val_mse: 105.1079\n",
      "Epoch 17/500\n",
      "106/115 [==========================>...] - ETA: 0s - loss: 103.6984 - mse: 103.6984\n",
      "Epoch 17: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 103.7168 - mse: 103.7168 - val_loss: 102.9027 - val_mse: 102.9027\n",
      "Epoch 18/500\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 101.6189 - mse: 101.6189\n",
      "Epoch 18: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 101.5289 - mse: 101.5289 - val_loss: 100.7255 - val_mse: 100.7255\n",
      "Epoch 19/500\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 99.4203 - mse: 99.4203\n",
      "Epoch 19: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 99.3699 - mse: 99.3699 - val_loss: 98.5781 - val_mse: 98.5781\n",
      "Epoch 20/500\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 97.2397 - mse: 97.2397\n",
      "Epoch 20: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 97.2398 - mse: 97.2398 - val_loss: 96.4591 - val_mse: 96.4591\n",
      "Epoch 21/500\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 95.2372 - mse: 95.2372\n",
      "Epoch 21: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 95.1382 - mse: 95.1382 - val_loss: 94.3675 - val_mse: 94.3675\n",
      "Epoch 22/500\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 93.0549 - mse: 93.0549\n",
      "Epoch 22: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 93.0649 - mse: 93.0649 - val_loss: 92.3059 - val_mse: 92.3059\n",
      "Epoch 23/500\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 91.0611 - mse: 91.0611\n",
      "Epoch 23: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 91.0195 - mse: 91.0195 - val_loss: 90.2709 - val_mse: 90.2709\n",
      "Epoch 24/500\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 89.0551 - mse: 89.0551\n",
      "Epoch 24: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 89.0012 - mse: 89.0012 - val_loss: 88.2628 - val_mse: 88.2628\n",
      "Epoch 25/500\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 86.9762 - mse: 86.9762\n",
      "Epoch 25: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 87.0102 - mse: 87.0102 - val_loss: 86.2830 - val_mse: 86.2830\n",
      "Epoch 26/500\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 85.0805 - mse: 85.0805\n",
      "Epoch 26: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 85.0459 - mse: 85.0459 - val_loss: 84.3282 - val_mse: 84.3282\n",
      "Epoch 27/500\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 83.2207 - mse: 83.2207\n",
      "Epoch 27: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 83.1081 - mse: 83.1081 - val_loss: 82.4005 - val_mse: 82.4005\n",
      "Epoch 28/500\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 81.2076 - mse: 81.2076\n",
      "Epoch 28: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 81.1965 - mse: 81.1965 - val_loss: 80.4992 - val_mse: 80.4992\n",
      "Epoch 29/500\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 79.3710 - mse: 79.3710\n",
      "Epoch 29: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 79.3114 - mse: 79.3114 - val_loss: 78.6230 - val_mse: 78.6230\n",
      "Epoch 30/500\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 77.5215 - mse: 77.5215\n",
      "Epoch 30: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 77.4520 - mse: 77.4520 - val_loss: 76.7736 - val_mse: 76.7736\n",
      "Epoch 31/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/115 [============================>.] - ETA: 0s - loss: 75.6479 - mse: 75.6479\n",
      "Epoch 31: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 75.6182 - mse: 75.6182 - val_loss: 74.9495 - val_mse: 74.9495\n",
      "Epoch 32/500\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 73.9249 - mse: 73.9249\n",
      "Epoch 32: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 73.8099 - mse: 73.8099 - val_loss: 73.1510 - val_mse: 73.1511\n",
      "Epoch 33/500\n",
      "113/115 [============================>.] - ETA: 0s - loss: 72.0014 - mse: 72.0014\n",
      "Epoch 33: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 72.0268 - mse: 72.0268 - val_loss: 71.3778 - val_mse: 71.3778\n",
      "Epoch 34/500\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 70.3313 - mse: 70.3313\n",
      "Epoch 34: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 70.2689 - mse: 70.2689 - val_loss: 69.6284 - val_mse: 69.6284\n",
      "Epoch 35/500\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 68.5488 - mse: 68.5488\n",
      "Epoch 35: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 68.5359 - mse: 68.5359 - val_loss: 67.9057 - val_mse: 67.9057\n",
      "Epoch 36/500\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 66.9906 - mse: 66.9906\n",
      "Epoch 36: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 66.8281 - mse: 66.8281 - val_loss: 66.2057 - val_mse: 66.2057\n",
      "Epoch 37/500\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 65.2202 - mse: 65.2202\n",
      "Epoch 37: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 7ms/step - loss: 65.1447 - mse: 65.1447 - val_loss: 64.5324 - val_mse: 64.5324\n",
      "Epoch 38/500\n",
      "107/115 [==========================>...] - ETA: 0s - loss: 63.5586 - mse: 63.5586\n",
      "Epoch 38: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 63.4856 - mse: 63.4856 - val_loss: 62.8826 - val_mse: 62.8826\n",
      "Epoch 39/500\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 61.8794 - mse: 61.8794\n",
      "Epoch 39: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 61.8508 - mse: 61.8508 - val_loss: 61.2568 - val_mse: 61.2568\n",
      "Epoch 40/500\n",
      "115/115 [==============================] - ETA: 0s - loss: 60.2405 - mse: 60.2405\n",
      "Epoch 40: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 60.2405 - mse: 60.2405 - val_loss: 59.6547 - val_mse: 59.6547\n",
      "Epoch 41/500\n",
      "112/115 [============================>.] - ETA: 0s - loss: 58.6577 - mse: 58.6577\n",
      "Epoch 41: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 58.6539 - mse: 58.6539 - val_loss: 58.0781 - val_mse: 58.0781\n",
      "Epoch 42/500\n",
      "106/115 [==========================>...] - ETA: 0s - loss: 57.0418 - mse: 57.0418\n",
      "Epoch 42: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 57.0915 - mse: 57.0915 - val_loss: 56.5247 - val_mse: 56.5247\n",
      "Epoch 43/500\n",
      "112/115 [============================>.] - ETA: 0s - loss: 55.5606 - mse: 55.5606\n",
      "Epoch 43: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 55.5530 - mse: 55.5530 - val_loss: 54.9952 - val_mse: 54.9952\n",
      "Epoch 44/500\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 54.1408 - mse: 54.1408\n",
      "Epoch 44: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 54.0382 - mse: 54.0382 - val_loss: 53.4882 - val_mse: 53.4882\n",
      "Epoch 45/500\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 52.5499 - mse: 52.5499\n",
      "Epoch 45: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 52.5472 - mse: 52.5472 - val_loss: 52.0073 - val_mse: 52.0073\n",
      "Epoch 46/500\n",
      "112/115 [============================>.] - ETA: 0s - loss: 51.0824 - mse: 51.0824\n",
      "Epoch 46: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 51.0795 - mse: 51.0795 - val_loss: 50.5480 - val_mse: 50.5480\n",
      "Epoch 47/500\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 49.5921 - mse: 49.5921\n",
      "Epoch 47: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 49.6353 - mse: 49.6353 - val_loss: 49.1129 - val_mse: 49.1129\n",
      "Epoch 48/500\n",
      "112/115 [============================>.] - ETA: 0s - loss: 48.2657 - mse: 48.2657\n",
      "Epoch 48: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 48.2147 - mse: 48.2147 - val_loss: 47.6997 - val_mse: 47.6997\n",
      "Epoch 49/500\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 46.9021 - mse: 46.9021\n",
      "Epoch 49: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 46.8177 - mse: 46.8177 - val_loss: 46.3114 - val_mse: 46.3114\n",
      "Epoch 50/500\n",
      "112/115 [============================>.] - ETA: 0s - loss: 45.4661 - mse: 45.4661\n",
      "Epoch 50: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 45.4438 - mse: 45.4438 - val_loss: 44.9465 - val_mse: 44.9465\n",
      "Epoch 51/500\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 44.0868 - mse: 44.0868\n",
      "Epoch 51: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 44.0930 - mse: 44.0930 - val_loss: 43.6040 - val_mse: 43.6040\n",
      "Epoch 52/500\n",
      "114/115 [============================>.] - ETA: 0s - loss: 42.7640 - mse: 42.7640\n",
      "Epoch 52: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 42.7652 - mse: 42.7652 - val_loss: 42.2839 - val_mse: 42.2839\n",
      "Epoch 53/500\n",
      "112/115 [============================>.] - ETA: 0s - loss: 41.4644 - mse: 41.4644\n",
      "Epoch 53: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 41.4602 - mse: 41.4602 - val_loss: 40.9884 - val_mse: 40.9884\n",
      "Epoch 54/500\n",
      "114/115 [============================>.] - ETA: 0s - loss: 40.1698 - mse: 40.1698\n",
      "Epoch 54: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 40.1784 - mse: 40.1784 - val_loss: 39.7144 - val_mse: 39.7144\n",
      "Epoch 55/500\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 38.9374 - mse: 38.9374\n",
      "Epoch 55: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 38.9190 - mse: 38.9190 - val_loss: 38.4640 - val_mse: 38.4640\n",
      "Epoch 56/500\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 37.7294 - mse: 37.7294\n",
      "Epoch 56: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 37.6827 - mse: 37.6827 - val_loss: 37.2356 - val_mse: 37.2356\n",
      "Epoch 57/500\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 36.4648 - mse: 36.4648\n",
      "Epoch 57: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 36.4689 - mse: 36.4689 - val_loss: 36.0306 - val_mse: 36.0306\n",
      "Epoch 58/500\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 35.2703 - mse: 35.2703\n",
      "Epoch 58: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 7ms/step - loss: 35.2779 - mse: 35.2779 - val_loss: 34.8476 - val_mse: 34.8476\n",
      "Epoch 59/500\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 34.0770 - mse: 34.0770\n",
      "Epoch 59: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 7ms/step - loss: 34.1089 - mse: 34.1089 - val_loss: 33.6877 - val_mse: 33.6877\n",
      "Epoch 60/500\n",
      "113/115 [============================>.] - ETA: 0s - loss: 32.9731 - mse: 32.9731\n",
      "Epoch 60: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 7ms/step - loss: 32.9626 - mse: 32.9626 - val_loss: 32.5488 - val_mse: 32.5488\n",
      "Epoch 61/500\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 31.8536 - mse: 31.8536\n",
      "Epoch 61: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 31.8386 - mse: 31.8386 - val_loss: 31.4326 - val_mse: 31.4326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/500\n",
      "112/115 [============================>.] - ETA: 0s - loss: 30.7553 - mse: 30.7553\n",
      "Epoch 62: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 30.7369 - mse: 30.7369 - val_loss: 30.3391 - val_mse: 30.3391\n",
      "Epoch 63/500\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 29.6541 - mse: 29.6541\n",
      "Epoch 63: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 29.6572 - mse: 29.6572 - val_loss: 29.2678 - val_mse: 29.2678\n",
      "Epoch 64/500\n",
      "114/115 [============================>.] - ETA: 0s - loss: 28.6125 - mse: 28.6125\n",
      "Epoch 64: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 28.5997 - mse: 28.5997 - val_loss: 28.2179 - val_mse: 28.2179\n",
      "Epoch 65/500\n",
      "112/115 [============================>.] - ETA: 0s - loss: 27.6117 - mse: 27.6117\n",
      "Epoch 65: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 27.5642 - mse: 27.5642 - val_loss: 27.1907 - val_mse: 27.1907\n",
      "Epoch 66/500\n",
      "107/115 [==========================>...] - ETA: 0s - loss: 26.5385 - mse: 26.5385\n",
      "Epoch 66: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 26.5506 - mse: 26.5506 - val_loss: 26.1855 - val_mse: 26.1855\n",
      "Epoch 67/500\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 25.5588 - mse: 25.5588\n",
      "Epoch 67: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 25.5587 - mse: 25.5587 - val_loss: 25.2017 - val_mse: 25.2017\n",
      "Epoch 68/500\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 24.6342 - mse: 24.6342\n",
      "Epoch 68: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 24.5885 - mse: 24.5885 - val_loss: 24.2390 - val_mse: 24.2390\n",
      "Epoch 69/500\n",
      "113/115 [============================>.] - ETA: 0s - loss: 23.6534 - mse: 23.6534\n",
      "Epoch 69: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 23.6399 - mse: 23.6399 - val_loss: 23.2981 - val_mse: 23.2981\n",
      "Epoch 70/500\n",
      "113/115 [============================>.] - ETA: 0s - loss: 22.7229 - mse: 22.7229\n",
      "Epoch 70: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 22.7131 - mse: 22.7131 - val_loss: 22.3789 - val_mse: 22.3789\n",
      "Epoch 71/500\n",
      "106/115 [==========================>...] - ETA: 0s - loss: 21.8509 - mse: 21.8509\n",
      "Epoch 71: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 21.8076 - mse: 21.8076 - val_loss: 21.4816 - val_mse: 21.4816\n",
      "Epoch 72/500\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 20.9435 - mse: 20.9435\n",
      "Epoch 72: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 20.9236 - mse: 20.9236 - val_loss: 20.6060 - val_mse: 20.6060\n",
      "Epoch 73/500\n",
      "112/115 [============================>.] - ETA: 0s - loss: 20.0592 - mse: 20.0592\n",
      "Epoch 73: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 20.0607 - mse: 20.0607 - val_loss: 19.7503 - val_mse: 19.7503\n",
      "Epoch 74/500\n",
      "115/115 [==============================] - ETA: 0s - loss: 19.2190 - mse: 19.2190\n",
      "Epoch 74: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 19.2190 - mse: 19.2190 - val_loss: 18.9162 - val_mse: 18.9162\n",
      "Epoch 75/500\n",
      "115/115 [==============================] - ETA: 0s - loss: 18.3981 - mse: 18.3981\n",
      "Epoch 75: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 18.3981 - mse: 18.3981 - val_loss: 18.1035 - val_mse: 18.1035\n",
      "Epoch 76/500\n",
      "113/115 [============================>.] - ETA: 0s - loss: 17.6159 - mse: 17.6159\n",
      "Epoch 76: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 17.5984 - mse: 17.5984 - val_loss: 17.3111 - val_mse: 17.3111\n",
      "Epoch 77/500\n",
      "113/115 [============================>.] - ETA: 0s - loss: 16.8255 - mse: 16.8255\n",
      "Epoch 77: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 16.8195 - mse: 16.8195 - val_loss: 16.5403 - val_mse: 16.5403\n",
      "Epoch 78/500\n",
      "113/115 [============================>.] - ETA: 0s - loss: 16.0840 - mse: 16.0840\n",
      "Epoch 78: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 16.0612 - mse: 16.0612 - val_loss: 15.7893 - val_mse: 15.7893\n",
      "Epoch 79/500\n",
      "112/115 [============================>.] - ETA: 0s - loss: 15.3199 - mse: 15.3199\n",
      "Epoch 79: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 15.3238 - mse: 15.3238 - val_loss: 15.0591 - val_mse: 15.0591\n",
      "Epoch 80/500\n",
      "112/115 [============================>.] - ETA: 0s - loss: 14.6100 - mse: 14.6100\n",
      "Epoch 80: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 7ms/step - loss: 14.6068 - mse: 14.6068 - val_loss: 14.3492 - val_mse: 14.3492\n",
      "Epoch 81/500\n",
      "114/115 [============================>.] - ETA: 0s - loss: 13.9165 - mse: 13.9165\n",
      "Epoch 81: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 7ms/step - loss: 13.9100 - mse: 13.9100 - val_loss: 13.6604 - val_mse: 13.6604\n",
      "Epoch 82/500\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 13.2723 - mse: 13.2723\n",
      "Epoch 82: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 7ms/step - loss: 13.2338 - mse: 13.2338 - val_loss: 12.9909 - val_mse: 12.9909\n",
      "Epoch 83/500\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 12.6077 - mse: 12.6077\n",
      "Epoch 83: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 12.5774 - mse: 12.5774 - val_loss: 12.3418 - val_mse: 12.3418\n",
      "Epoch 84/500\n",
      "114/115 [============================>.] - ETA: 0s - loss: 11.9481 - mse: 11.9481\n",
      "Epoch 84: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 11.9413 - mse: 11.9413 - val_loss: 11.7133 - val_mse: 11.7133\n",
      "Epoch 85/500\n",
      "106/115 [==========================>...] - ETA: 0s - loss: 11.3353 - mse: 11.3353\n",
      "Epoch 85: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 11.3249 - mse: 11.3249 - val_loss: 11.1042 - val_mse: 11.1042\n",
      "Epoch 86/500\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 10.7270 - mse: 10.7270\n",
      "Epoch 86: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 10.7282 - mse: 10.7282 - val_loss: 10.5146 - val_mse: 10.5146\n",
      "Epoch 87/500\n",
      "107/115 [==========================>...] - ETA: 0s - loss: 10.1383 - mse: 10.1383\n",
      "Epoch 87: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 10.1508 - mse: 10.1508 - val_loss: 9.9444 - val_mse: 9.9444\n",
      "Epoch 88/500\n",
      "112/115 [============================>.] - ETA: 0s - loss: 9.5869 - mse: 9.5869\n",
      "Epoch 88: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 9.5926 - mse: 9.5926 - val_loss: 9.3932 - val_mse: 9.3932\n",
      "Epoch 89/500\n",
      "112/115 [============================>.] - ETA: 0s - loss: 9.0571 - mse: 9.0571\n",
      "Epoch 89: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 9.0539 - mse: 9.0539 - val_loss: 8.8612 - val_mse: 8.8612\n",
      "Epoch 90/500\n",
      "112/115 [============================>.] - ETA: 0s - loss: 8.5350 - mse: 8.5350\n",
      "Epoch 90: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 8.5343 - mse: 8.5343 - val_loss: 8.3481 - val_mse: 8.3481\n",
      "Epoch 91/500\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 8.0224 - mse: 8.0224\n",
      "Epoch 91: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 8.0336 - mse: 8.0336 - val_loss: 7.8546 - val_mse: 7.8546\n",
      "Epoch 92/500\n",
      "113/115 [============================>.] - ETA: 0s - loss: 7.5505 - mse: 7.5505\n",
      "Epoch 92: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 7.5516 - mse: 7.5516 - val_loss: 7.3790 - val_mse: 7.3790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/500\n",
      "115/115 [==============================] - ETA: 0s - loss: 7.0881 - mse: 7.0881\n",
      "Epoch 93: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 7.0881 - mse: 7.0881 - val_loss: 6.9214 - val_mse: 6.9214\n",
      "Epoch 94/500\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 6.6466 - mse: 6.6466\n",
      "Epoch 94: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 6.6428 - mse: 6.6428 - val_loss: 6.4833 - val_mse: 6.4833\n",
      "Epoch 95/500\n",
      "114/115 [============================>.] - ETA: 0s - loss: 6.2090 - mse: 6.2090\n",
      "Epoch 95: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 6.2156 - mse: 6.2156 - val_loss: 6.0621 - val_mse: 6.0621\n",
      "Epoch 96/500\n",
      "115/115 [==============================] - ETA: 0s - loss: 5.8060 - mse: 5.8060\n",
      "Epoch 96: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 5.8060 - mse: 5.8060 - val_loss: 5.6588 - val_mse: 5.6588\n",
      "Epoch 97/500\n",
      "112/115 [============================>.] - ETA: 0s - loss: 5.4158 - mse: 5.4158\n",
      "Epoch 97: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 5.4145 - mse: 5.4145 - val_loss: 5.2735 - val_mse: 5.2735\n",
      "Epoch 98/500\n",
      "113/115 [============================>.] - ETA: 0s - loss: 5.0316 - mse: 5.0316\n",
      "Epoch 98: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 5.0401 - mse: 5.0401 - val_loss: 4.9059 - val_mse: 4.9059\n",
      "Epoch 99/500\n",
      "112/115 [============================>.] - ETA: 0s - loss: 4.6842 - mse: 4.6842\n",
      "Epoch 99: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 4.6829 - mse: 4.6829 - val_loss: 4.5545 - val_mse: 4.5545\n",
      "Epoch 100/500\n",
      "113/115 [============================>.] - ETA: 0s - loss: 4.3488 - mse: 4.3488\n",
      "Epoch 100: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 4.3426 - mse: 4.3426 - val_loss: 4.2203 - val_mse: 4.2203\n",
      "Epoch 101/500\n",
      "113/115 [============================>.] - ETA: 0s - loss: 4.0196 - mse: 4.0196\n",
      "Epoch 101: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 4.0189 - mse: 4.0189 - val_loss: 3.9023 - val_mse: 3.9023\n",
      "Epoch 102/500\n",
      "112/115 [============================>.] - ETA: 0s - loss: 3.7192 - mse: 3.7192\n",
      "Epoch 102: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 7ms/step - loss: 3.7118 - mse: 3.7118 - val_loss: 3.6008 - val_mse: 3.6008\n",
      "Epoch 103/500\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 3.4229 - mse: 3.4229\n",
      "Epoch 103: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 7ms/step - loss: 3.4208 - mse: 3.4208 - val_loss: 3.3154 - val_mse: 3.3154\n",
      "Epoch 104/500\n",
      "112/115 [============================>.] - ETA: 0s - loss: 3.1481 - mse: 3.1481\n",
      "Epoch 104: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 7ms/step - loss: 3.1455 - mse: 3.1455 - val_loss: 3.0454 - val_mse: 3.0454\n",
      "Epoch 105/500\n",
      "113/115 [============================>.] - ETA: 0s - loss: 2.8906 - mse: 2.8906\n",
      "Epoch 105: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 7ms/step - loss: 2.8857 - mse: 2.8857 - val_loss: 2.7910 - val_mse: 2.7910\n",
      "Epoch 106/500\n",
      "113/115 [============================>.] - ETA: 0s - loss: 2.6455 - mse: 2.6455\n",
      "Epoch 106: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 2.6410 - mse: 2.6410 - val_loss: 2.5516 - val_mse: 2.5516\n",
      "Epoch 107/500\n",
      "113/115 [============================>.] - ETA: 0s - loss: 2.4157 - mse: 2.4157\n",
      "Epoch 107: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 2.4111 - mse: 2.4111 - val_loss: 2.3268 - val_mse: 2.3268\n",
      "Epoch 108/500\n",
      "113/115 [============================>.] - ETA: 0s - loss: 2.2013 - mse: 2.2013\n",
      "Epoch 108: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 2.1957 - mse: 2.1957 - val_loss: 2.1161 - val_mse: 2.1161\n",
      "Epoch 109/500\n",
      "114/115 [============================>.] - ETA: 0s - loss: 1.9971 - mse: 1.9971\n",
      "Epoch 109: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 1.9944 - mse: 1.9944 - val_loss: 1.9197 - val_mse: 1.9197\n",
      "Epoch 110/500\n",
      "113/115 [============================>.] - ETA: 0s - loss: 1.8065 - mse: 1.8065\n",
      "Epoch 110: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 1.8068 - mse: 1.8068 - val_loss: 1.7367 - val_mse: 1.7367\n",
      "Epoch 111/500\n",
      "114/115 [============================>.] - ETA: 0s - loss: 1.6339 - mse: 1.6339\n",
      "Epoch 111: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 1.6322 - mse: 1.6322 - val_loss: 1.5668 - val_mse: 1.5668\n",
      "Epoch 112/500\n",
      "113/115 [============================>.] - ETA: 0s - loss: 1.4712 - mse: 1.4712\n",
      "Epoch 112: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 1.4707 - mse: 1.4707 - val_loss: 1.4093 - val_mse: 1.4093\n",
      "Epoch 113/500\n",
      "107/115 [==========================>...] - ETA: 0s - loss: 1.3216 - mse: 1.3216\n",
      "Epoch 113: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 1.3216 - mse: 1.3216 - val_loss: 1.2644 - val_mse: 1.2644\n",
      "Epoch 114/500\n",
      "115/115 [==============================] - ETA: 0s - loss: 1.1845 - mse: 1.1845\n",
      "Epoch 114: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 1.1845 - mse: 1.1845 - val_loss: 1.1314 - val_mse: 1.1314\n",
      "Epoch 115/500\n",
      "115/115 [==============================] - ETA: 0s - loss: 1.0590 - mse: 1.0590\n",
      "Epoch 115: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 1.0590 - mse: 1.0590 - val_loss: 1.0095 - val_mse: 1.0095\n",
      "Epoch 116/500\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.9512 - mse: 0.9512\n",
      "Epoch 116: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.9445 - mse: 0.9445 - val_loss: 0.8984 - val_mse: 0.8984\n",
      "Epoch 117/500\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.8387 - mse: 0.8387\n",
      "Epoch 117: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 7ms/step - loss: 0.8406 - mse: 0.8406 - val_loss: 0.7981 - val_mse: 0.7981\n",
      "Epoch 118/500\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.7423 - mse: 0.7423\n",
      "Epoch 118: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 7ms/step - loss: 0.7467 - mse: 0.7467 - val_loss: 0.7072 - val_mse: 0.7072\n",
      "Epoch 119/500\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.6633 - mse: 0.6633\n",
      "Epoch 119: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 7ms/step - loss: 0.6626 - mse: 0.6626 - val_loss: 0.6257 - val_mse: 0.6257\n",
      "Epoch 120/500\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.5920 - mse: 0.5920\n",
      "Epoch 120: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.5873 - mse: 0.5873 - val_loss: 0.5534 - val_mse: 0.5534\n",
      "Epoch 121/500\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.5218 - mse: 0.5218\n",
      "Epoch 121: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 7ms/step - loss: 0.5206 - mse: 0.5206 - val_loss: 0.4889 - val_mse: 0.4889\n",
      "Epoch 122/500\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.4628 - mse: 0.4628\n",
      "Epoch 122: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.4617 - mse: 0.4617 - val_loss: 0.4322 - val_mse: 0.4322\n",
      "Epoch 123/500\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.4149 - mse: 0.4149\n",
      "Epoch 123: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.4102 - mse: 0.4102 - val_loss: 0.3827 - val_mse: 0.3827\n",
      "Epoch 124/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/115 [============================>.] - ETA: 0s - loss: 0.3645 - mse: 0.3645\n",
      "Epoch 124: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 7ms/step - loss: 0.3655 - mse: 0.3655 - val_loss: 0.3401 - val_mse: 0.3401\n",
      "Epoch 125/500\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.3306 - mse: 0.3306\n",
      "Epoch 125: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 7ms/step - loss: 0.3270 - mse: 0.3270 - val_loss: 0.3028 - val_mse: 0.3028\n",
      "Epoch 126/500\n",
      "107/115 [==========================>...] - ETA: 0s - loss: 0.2972 - mse: 0.2972\n",
      "Epoch 126: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 7ms/step - loss: 0.2941 - mse: 0.2941 - val_loss: 0.2713 - val_mse: 0.2713\n",
      "Epoch 127/500\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.2655 - mse: 0.2655\n",
      "Epoch 127: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.2663 - mse: 0.2663 - val_loss: 0.2450 - val_mse: 0.2450\n",
      "Epoch 128/500\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.2418 - mse: 0.2418\n",
      "Epoch 128: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 7ms/step - loss: 0.2430 - mse: 0.2430 - val_loss: 0.2226 - val_mse: 0.2226\n",
      "Epoch 129/500\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.2242 - mse: 0.2242\n",
      "Epoch 129: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 7ms/step - loss: 0.2237 - mse: 0.2237 - val_loss: 0.2041 - val_mse: 0.2041\n",
      "Epoch 130/500\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2081 - mse: 0.2081\n",
      "Epoch 130: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 7ms/step - loss: 0.2081 - mse: 0.2081 - val_loss: 0.1891 - val_mse: 0.1891\n",
      "Epoch 131/500\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.1939 - mse: 0.1939\n",
      "Epoch 131: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 7ms/step - loss: 0.1954 - mse: 0.1954 - val_loss: 0.1771 - val_mse: 0.1771\n",
      "Epoch 132/500\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.1873 - mse: 0.1873\n",
      "Epoch 132: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.1853 - mse: 0.1853 - val_loss: 0.1672 - val_mse: 0.1672\n",
      "Epoch 133/500\n",
      "114/115 [============================>.] - ETA: 0s - loss: 0.1773 - mse: 0.1773\n",
      "Epoch 133: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 7ms/step - loss: 0.1774 - mse: 0.1774 - val_loss: 0.1597 - val_mse: 0.1597\n",
      "Epoch 134/500\n",
      "107/115 [==========================>...] - ETA: 0s - loss: 0.1736 - mse: 0.1736\n",
      "Epoch 134: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 7ms/step - loss: 0.1713 - mse: 0.1713 - val_loss: 0.1536 - val_mse: 0.1536\n",
      "Epoch 135/500\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.1668 - mse: 0.1668\n",
      "Epoch 135: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 7ms/step - loss: 0.1667 - mse: 0.1667 - val_loss: 0.1490 - val_mse: 0.1490\n",
      "Epoch 136/500\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.1614 - mse: 0.1614\n",
      "Epoch 136: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.1633 - mse: 0.1633 - val_loss: 0.1457 - val_mse: 0.1457\n",
      "Epoch 137/500\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.1624 - mse: 0.1624\n",
      "Epoch 137: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 7ms/step - loss: 0.1608 - mse: 0.1608 - val_loss: 0.1430 - val_mse: 0.1430\n",
      "Epoch 138/500\n",
      "107/115 [==========================>...] - ETA: 0s - loss: 0.1601 - mse: 0.1601\n",
      "Epoch 138: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 7ms/step - loss: 0.1590 - mse: 0.1590 - val_loss: 0.1413 - val_mse: 0.1413\n",
      "Epoch 139/500\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.1544 - mse: 0.1544\n",
      "Epoch 139: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 7ms/step - loss: 0.1578 - mse: 0.1578 - val_loss: 0.1398 - val_mse: 0.1398\n",
      "Epoch 140/500\n",
      "106/115 [==========================>...] - ETA: 0s - loss: 0.1580 - mse: 0.1580\n",
      "Epoch 140: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 7ms/step - loss: 0.1570 - mse: 0.1570 - val_loss: 0.1389 - val_mse: 0.1389\n",
      "Epoch 141/500\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.1569 - mse: 0.1569\n",
      "Epoch 141: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 7ms/step - loss: 0.1565 - mse: 0.1565 - val_loss: 0.1383 - val_mse: 0.1383\n",
      "Epoch 142/500\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.1554 - mse: 0.1554\n",
      "Epoch 142: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 8ms/step - loss: 0.1561 - mse: 0.1561 - val_loss: 0.1378 - val_mse: 0.1378\n",
      "Epoch 143/500\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.1586 - mse: 0.1586\n",
      "Epoch 143: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 9ms/step - loss: 0.1559 - mse: 0.1559 - val_loss: 0.1374 - val_mse: 0.1374\n",
      "Epoch 144/500\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.1562 - mse: 0.1562\n",
      "Epoch 144: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 9ms/step - loss: 0.1557 - mse: 0.1557 - val_loss: 0.1372 - val_mse: 0.1372\n",
      "Epoch 145/500\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.1548 - mse: 0.1548\n",
      "Epoch 145: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 8ms/step - loss: 0.1556 - mse: 0.1556 - val_loss: 0.1371 - val_mse: 0.1371\n",
      "Epoch 146/500\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.1573 - mse: 0.1573\n",
      "Epoch 146: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 8ms/step - loss: 0.1556 - mse: 0.1556 - val_loss: 0.1370 - val_mse: 0.1370\n",
      "Epoch 147/500\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.1509 - mse: 0.1509\n",
      "Epoch 147: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.1556 - mse: 0.1556 - val_loss: 0.1369 - val_mse: 0.1369\n",
      "Epoch 148/500\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.1566 - mse: 0.1566\n",
      "Epoch 148: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.1555 - mse: 0.1555 - val_loss: 0.1368 - val_mse: 0.1368\n",
      "Epoch 149/500\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.1552 - mse: 0.1552\n",
      "Epoch 149: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.1555 - mse: 0.1555 - val_loss: 0.1368 - val_mse: 0.1368\n",
      "Epoch 150/500\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.1539 - mse: 0.1539\n",
      "Epoch 150: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.1555 - mse: 0.1555 - val_loss: 0.1368 - val_mse: 0.1368\n",
      "Epoch 151/500\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.1566 - mse: 0.1566\n",
      "Epoch 151: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.1555 - mse: 0.1555 - val_loss: 0.1367 - val_mse: 0.1367\n",
      "Epoch 152/500\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.1566 - mse: 0.1566\n",
      "Epoch 152: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.1555 - mse: 0.1555 - val_loss: 0.1368 - val_mse: 0.1368\n",
      "Epoch 153/500\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.1558 - mse: 0.1558\n",
      "Epoch 153: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.1555 - mse: 0.1555 - val_loss: 0.1367 - val_mse: 0.1367\n",
      "Epoch 154/500\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.1552 - mse: 0.1552\n",
      "Epoch 154: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.1555 - mse: 0.1555 - val_loss: 0.1368 - val_mse: 0.1368\n",
      "Epoch 155/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/115 [==========================>...] - ETA: 0s - loss: 0.1538 - mse: 0.1538\n",
      "Epoch 155: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.1555 - mse: 0.1555 - val_loss: 0.1368 - val_mse: 0.1368\n",
      "Epoch 156/500\n",
      "106/115 [==========================>...] - ETA: 0s - loss: 0.1548 - mse: 0.1548\n",
      "Epoch 156: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.1556 - mse: 0.1556 - val_loss: 0.1368 - val_mse: 0.1368\n",
      "Epoch 157/500\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.1558 - mse: 0.1558\n",
      "Epoch 157: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.1556 - mse: 0.1556 - val_loss: 0.1368 - val_mse: 0.1368\n",
      "Epoch 158/500\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.1561 - mse: 0.1561\n",
      "Epoch 158: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.1555 - mse: 0.1555 - val_loss: 0.1367 - val_mse: 0.1367\n",
      "Epoch 159/500\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.1514 - mse: 0.1514\n",
      "Epoch 159: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.1556 - mse: 0.1556 - val_loss: 0.1368 - val_mse: 0.1368\n",
      "Epoch 160/500\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.1558 - mse: 0.1558\n",
      "Epoch 160: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.1555 - mse: 0.1555 - val_loss: 0.1368 - val_mse: 0.1368\n",
      "Epoch 161/500\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.1550 - mse: 0.1550\n",
      "Epoch 161: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.1556 - mse: 0.1556 - val_loss: 0.1368 - val_mse: 0.1368\n"
     ]
    }
   ],
   "source": [
    "### 3rd - 5 trial ###\n",
    "model3_5 = Sequential()\n",
    "#--------------------------------------------\n",
    "model3_5.add(Dense(1024, input_shape=(43,)))\n",
    "model3_5.add(Activation('relu'))\n",
    "#----------------------------------\n",
    "model3_5.add(Dense(512, input_shape=(43,)))\n",
    "model3_5.add(Activation('relu'))\n",
    "#----------------------------------\n",
    "model3_5.add(Dense(256, input_shape=(43,)))\n",
    "model3_5.add(Activation('relu'))\n",
    "#----------------------------------\n",
    "model3_5.add(Dense(128, input_shape=(43,)))\n",
    "model3_5.add(Activation('relu'))\n",
    "#----------------------------------\n",
    "model3_5.add(Dense(64, input_shape=(43,)))\n",
    "model3_5.add(Activation('relu'))\n",
    "#----------------------------------\n",
    "model3_5.add(Dense(32, input_shape=(43,)))\n",
    "model3_5.add(Activation('relu'))\n",
    "#----------------------------------\n",
    "model3_5.add(Dense(16))\n",
    "model3_5.add(Activation('relu'))\n",
    "#----------------------------------\n",
    "model3_5.add(Dense(8, input_shape=(43,)))\n",
    "model3_5.add(Activation('relu'))\n",
    "#----------------------------------\n",
    "model3_5.add(Dense(4))\n",
    "model3_5.add(Activation('relu'))\n",
    "#----------------------------------\n",
    "model3_5.add(Dense(2))\n",
    "model3_5.add(Activation('relu'))\n",
    "#----------------------------------\n",
    "model3_5.add(Dense(1))\n",
    "model3_5.add(Activation('elu'))\n",
    "#==================================================================================\n",
    "model3_5.compile(loss='mean_squared_error', metrics=['mse'], optimizer='adam')\n",
    "history3_5 = model3_5.fit(X_train,y_train, validation_data=(X_test,y_test), epochs=500, batch_size=10, callbacks=[early_stopping_callback,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "a0e396ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1368 - mse: 0.1368\n",
      "Test Accuracy : [0.13679854571819305, 0.13679854571819305]\n"
     ]
    }
   ],
   "source": [
    "print('Test Accuracy : {}'.format(model3_5.evaluate(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "fc3e567d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>164423.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>164423.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>164423.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>164423.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>164423.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2915</td>\n",
       "      <td>164423.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2916</td>\n",
       "      <td>164423.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2917</td>\n",
       "      <td>164423.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2918</td>\n",
       "      <td>164423.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2919</td>\n",
       "      <td>164423.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  SalePrice\n",
       "0     1461  164423.25\n",
       "1     1462  164423.25\n",
       "2     1463  164423.25\n",
       "3     1464  164423.25\n",
       "4     1465  164423.25\n",
       "...    ...        ...\n",
       "1454  2915  164423.25\n",
       "1455  2916  164423.25\n",
       "1456  2917  164423.25\n",
       "1457  2918  164423.25\n",
       "1458  2919  164423.25\n",
       "\n",
       "[1459 rows x 2 columns]"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_from_model = model3_5.predict(df_house_test_extracted2b_predicted)\n",
    "df_house_test['SalePrice'] = 0\n",
    "df_house_test['SalePrice'] = np.expm1(result_from_model)\n",
    "df_house_test[['Id','SalePrice']].to_csv('C:/Users/thsong/submission15_2.csv',index=False)\n",
    "df_house_test[['Id','SalePrice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9852e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "06367b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 11.5434 - mse: 11.5434\n",
      "Epoch 1: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 11.0532 - mse: 11.0532 - val_loss: 1.2786 - val_mse: 1.2786\n",
      "Epoch 2/300\n",
      "105/115 [==========================>...] - ETA: 0s - loss: 0.8907 - mse: 0.8907\n",
      "Epoch 2: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.8975 - mse: 0.8975 - val_loss: 1.1460 - val_mse: 1.1460\n",
      "Epoch 3/300\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.5650 - mse: 0.5650\n",
      "Epoch 3: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.5749 - mse: 0.5749 - val_loss: 0.6399 - val_mse: 0.6399\n",
      "Epoch 4/300\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.4381 - mse: 0.4381\n",
      "Epoch 4: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.4462 - mse: 0.4462 - val_loss: 0.7182 - val_mse: 0.7182\n",
      "Epoch 5/300\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.3204 - mse: 0.3204\n",
      "Epoch 5: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.3223 - mse: 0.3223 - val_loss: 0.6314 - val_mse: 0.6314\n",
      "Epoch 6/300\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.3045 - mse: 0.3045\n",
      "Epoch 6: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.3065 - mse: 0.3065 - val_loss: 0.6868 - val_mse: 0.6868\n",
      "Epoch 7/300\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.3012 - mse: 0.3012\n",
      "Epoch 7: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.3013 - mse: 0.3013 - val_loss: 0.5730 - val_mse: 0.5730\n",
      "Epoch 8/300\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.2902 - mse: 0.2902\n",
      "Epoch 8: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.2872 - mse: 0.2872 - val_loss: 0.5265 - val_mse: 0.5265\n",
      "Epoch 9/300\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.2623 - mse: 0.2623\n",
      "Epoch 9: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.2636 - mse: 0.2636 - val_loss: 0.4284 - val_mse: 0.4284\n",
      "Epoch 10/300\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2060 - mse: 0.2060\n",
      "Epoch 10: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.2060 - mse: 0.2060 - val_loss: 0.4440 - val_mse: 0.4440\n",
      "Epoch 11/300\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.1839 - mse: 0.1839\n",
      "Epoch 11: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.1862 - mse: 0.1862 - val_loss: 0.3423 - val_mse: 0.3423\n",
      "Epoch 12/300\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.1417 - mse: 0.1417\n",
      "Epoch 12: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.1400 - mse: 0.1400 - val_loss: 0.3739 - val_mse: 0.3739\n",
      "Epoch 13/300\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.1377 - mse: 0.1377\n",
      "Epoch 13: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.1373 - mse: 0.1373 - val_loss: 0.3461 - val_mse: 0.3461\n",
      "Epoch 14/300\n",
      "107/115 [==========================>...] - ETA: 0s - loss: 0.1943 - mse: 0.1943\n",
      "Epoch 14: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.1935 - mse: 0.1935 - val_loss: 0.4442 - val_mse: 0.4442\n",
      "Epoch 15/300\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.2328 - mse: 0.2328\n",
      "Epoch 15: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.2329 - mse: 0.2329 - val_loss: 0.4476 - val_mse: 0.4476\n",
      "Epoch 16/300\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.2662 - mse: 0.2662\n",
      "Epoch 16: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.2629 - mse: 0.2629 - val_loss: 0.3609 - val_mse: 0.3609\n",
      "Epoch 17/300\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.2201 - mse: 0.2201\n",
      "Epoch 17: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.2265 - mse: 0.2265 - val_loss: 0.3756 - val_mse: 0.3756\n",
      "Epoch 18/300\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.2948 - mse: 0.2948\n",
      "Epoch 18: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.2904 - mse: 0.2904 - val_loss: 0.5561 - val_mse: 0.5561\n",
      "Epoch 19/300\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.4443 - mse: 0.4443\n",
      "Epoch 19: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.4588 - mse: 0.4588 - val_loss: 0.9739 - val_mse: 0.9739\n",
      "Epoch 20/300\n",
      "106/115 [==========================>...] - ETA: 0s - loss: 0.5235 - mse: 0.5235\n",
      "Epoch 20: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.5037 - mse: 0.5037 - val_loss: 0.4269 - val_mse: 0.4269\n",
      "Epoch 21/300\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.1832 - mse: 0.1832\n",
      "Epoch 21: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.1801 - mse: 0.1801 - val_loss: 0.2973 - val_mse: 0.2973\n",
      "Epoch 22/300\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.1359 - mse: 0.1359\n",
      "Epoch 22: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.1401 - mse: 0.1401 - val_loss: 0.3470 - val_mse: 0.3470\n",
      "Epoch 23/300\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.1310 - mse: 0.1310\n",
      "Epoch 23: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.1287 - mse: 0.1287 - val_loss: 0.2702 - val_mse: 0.2702\n",
      "Epoch 24/300\n",
      "105/115 [==========================>...] - ETA: 0s - loss: 0.1033 - mse: 0.1033\n",
      "Epoch 24: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.1012 - mse: 0.1012 - val_loss: 0.2539 - val_mse: 0.2539\n",
      "Epoch 25/300\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.0605 - mse: 0.0605\n",
      "Epoch 25: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0595 - mse: 0.0595 - val_loss: 0.2579 - val_mse: 0.2579\n",
      "Epoch 26/300\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.0782 - mse: 0.0782\n",
      "Epoch 26: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0776 - mse: 0.0776 - val_loss: 0.2422 - val_mse: 0.2422\n",
      "Epoch 27/300\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.0715 - mse: 0.0715\n",
      "Epoch 27: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0721 - mse: 0.0721 - val_loss: 0.2507 - val_mse: 0.2507\n",
      "Epoch 28/300\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.0939 - mse: 0.0939\n",
      "Epoch 28: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.0930 - mse: 0.0930 - val_loss: 0.2701 - val_mse: 0.2701\n",
      "Epoch 29/300\n",
      "109/115 [===========================>..] - ETA: 0s - loss: 0.1086 - mse: 0.1086\n",
      "Epoch 29: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.1082 - mse: 0.1082 - val_loss: 0.2598 - val_mse: 0.2598\n",
      "Epoch 30/300\n",
      "113/115 [============================>.] - ETA: 0s - loss: 0.1411 - mse: 0.1411\n",
      "Epoch 30: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.1416 - mse: 0.1416 - val_loss: 0.3744 - val_mse: 0.3744\n",
      "Epoch 31/300\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.1594 - mse: 0.1594\n",
      "Epoch 31: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.1542 - mse: 0.1542 - val_loss: 0.2535 - val_mse: 0.2535\n",
      "Epoch 32/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/115 [===========================>..] - ETA: 0s - loss: 0.0721 - mse: 0.0721\n",
      "Epoch 32: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0715 - mse: 0.0715 - val_loss: 0.2307 - val_mse: 0.2307\n",
      "Epoch 33/300\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0712 - mse: 0.0712\n",
      "Epoch 33: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0731 - mse: 0.0731 - val_loss: 0.2478 - val_mse: 0.2478\n",
      "Epoch 34/300\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.1328 - mse: 0.1328\n",
      "Epoch 34: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.1346 - mse: 0.1346 - val_loss: 0.6433 - val_mse: 0.6433\n",
      "Epoch 35/300\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.3211 - mse: 0.3211\n",
      "Epoch 35: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.3162 - mse: 0.3162 - val_loss: 0.4496 - val_mse: 0.4496\n",
      "Epoch 36/300\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.2642 - mse: 0.2642\n",
      "Epoch 36: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.2633 - mse: 0.2633 - val_loss: 0.3453 - val_mse: 0.3453\n",
      "Epoch 37/300\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.2513 - mse: 0.2513\n",
      "Epoch 37: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.2503 - mse: 0.2503 - val_loss: 0.3840 - val_mse: 0.3840\n",
      "Epoch 38/300\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.1298 - mse: 0.1298\n",
      "Epoch 38: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.1283 - mse: 0.1283 - val_loss: 0.2133 - val_mse: 0.2133\n",
      "Epoch 39/300\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.0980 - mse: 0.0980\n",
      "Epoch 39: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0971 - mse: 0.0971 - val_loss: 0.2298 - val_mse: 0.2298\n",
      "Epoch 40/300\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.0781 - mse: 0.0781\n",
      "Epoch 40: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0774 - mse: 0.0774 - val_loss: 0.2244 - val_mse: 0.2244\n",
      "Epoch 41/300\n",
      "112/115 [============================>.] - ETA: 0s - loss: 0.0712 - mse: 0.0712\n",
      "Epoch 41: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0703 - mse: 0.0703 - val_loss: 0.1943 - val_mse: 0.1943\n",
      "Epoch 42/300\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.0812 - mse: 0.0812\n",
      "Epoch 42: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0847 - mse: 0.0847 - val_loss: 0.5861 - val_mse: 0.5861\n",
      "Epoch 43/300\n",
      "106/115 [==========================>...] - ETA: 0s - loss: 0.1871 - mse: 0.1871\n",
      "Epoch 43: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.1809 - mse: 0.1809 - val_loss: 0.2445 - val_mse: 0.2445\n",
      "Epoch 44/300\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.1148 - mse: 0.1148\n",
      "Epoch 44: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.1137 - mse: 0.1137 - val_loss: 0.2160 - val_mse: 0.2160\n",
      "Epoch 45/300\n",
      "108/115 [===========================>..] - ETA: 0s - loss: 0.1045 - mse: 0.1045\n",
      "Epoch 45: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.1083 - mse: 0.1083 - val_loss: 0.3002 - val_mse: 0.3002\n",
      "Epoch 46/300\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.1311 - mse: 0.1311\n",
      "Epoch 46: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.1298 - mse: 0.1298 - val_loss: 0.2155 - val_mse: 0.2155\n",
      "Epoch 47/300\n",
      "106/115 [==========================>...] - ETA: 0s - loss: 0.1394 - mse: 0.1394\n",
      "Epoch 47: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.1451 - mse: 0.1451 - val_loss: 0.4983 - val_mse: 0.4983\n",
      "Epoch 48/300\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.1451 - mse: 0.1451\n",
      "Epoch 48: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.1431 - mse: 0.1431 - val_loss: 0.2436 - val_mse: 0.2436\n",
      "Epoch 49/300\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.1043 - mse: 0.1043\n",
      "Epoch 49: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.1050 - mse: 0.1050 - val_loss: 0.1957 - val_mse: 0.1957\n",
      "Epoch 50/300\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.0964 - mse: 0.0964\n",
      "Epoch 50: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0965 - mse: 0.0965 - val_loss: 0.3050 - val_mse: 0.3050\n",
      "Epoch 51/300\n",
      "110/115 [===========================>..] - ETA: 0s - loss: 0.0796 - mse: 0.0796\n",
      "Epoch 51: val_loss did not improve from 0.04271\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0831 - mse: 0.0831 - val_loss: 0.2755 - val_mse: 0.2755\n"
     ]
    }
   ],
   "source": [
    "### 3rd - 6 trial ###\n",
    "model3_6 = Sequential()\n",
    "#--------------------------------------------\n",
    "model3_6.add(Dense(2048, input_shape=(43,)))\n",
    "model3_6.add(Activation('relu'))\n",
    "#----------------------------------\n",
    "model3_6.add(Dense(256, input_shape=(43,)))\n",
    "model3_6.add(Activation('relu'))\n",
    "#----------------------------------\n",
    "model3_6.add(Dense(32, input_shape=(43,)))\n",
    "model3_6.add(Activation('relu'))\n",
    "#----------------------------------\n",
    "model3_6.add(Dense(4))\n",
    "model3_6.add(Activation('relu'))\n",
    "#----------------------------------\n",
    "model3_6.add(Dense(1))\n",
    "#model3_6.add(Activation('elu'))\n",
    "#==================================================================================\n",
    "model3_6.compile(loss='mean_squared_error', metrics=['mse'], optimizer='adam')\n",
    "history3_6 = model3_6.fit(X_train,y_train, validation_data=(X_test,y_test), epochs=300, batch_size=10, callbacks=[early_stopping_callback,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "872326ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 2ms/step - loss: 0.2755 - mse: 0.2755\n",
      "Test Accuracy : [0.2755107581615448, 0.2755107581615448]\n"
     ]
    }
   ],
   "source": [
    "print('Test Accuracy : {}'.format(model3_6.evaluate(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "950db675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>185056.484375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>89802.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>161848.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>229686.890625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>157986.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2915</td>\n",
       "      <td>35227.820312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2916</td>\n",
       "      <td>56020.710938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2917</td>\n",
       "      <td>221536.203125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2918</td>\n",
       "      <td>74549.679688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2919</td>\n",
       "      <td>303028.625000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id      SalePrice\n",
       "0     1461  185056.484375\n",
       "1     1462   89802.250000\n",
       "2     1463  161848.625000\n",
       "3     1464  229686.890625\n",
       "4     1465  157986.031250\n",
       "...    ...            ...\n",
       "1454  2915   35227.820312\n",
       "1455  2916   56020.710938\n",
       "1456  2917  221536.203125\n",
       "1457  2918   74549.679688\n",
       "1458  2919  303028.625000\n",
       "\n",
       "[1459 rows x 2 columns]"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_from_model = model3_6.predict(df_house_test_extracted2b_predicted)\n",
    "df_house_test['SalePrice'] = 0\n",
    "df_house_test['SalePrice'] = np.expm1(result_from_model)\n",
    "df_house_test[['Id','SalePrice']].to_csv('C:/Users/thsong/submission15_2.csv',index=False)\n",
    "df_house_test[['Id','SalePrice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02aab59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02789677",
   "metadata": {},
   "source": [
    "### trial - backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "e99778f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from random import randrange\n",
    "from random import random\n",
    "from csv import reader\n",
    "from math import exp\n",
    "#-----------------------------------------------\n",
    "# # Load a CSV file\n",
    "# def load_csv(filename):\n",
    "# \tdataset = list()\n",
    "# \twith open(filename, 'r') as file:\n",
    "# \t\tcsv_reader = reader(file)\n",
    "# \t\tfor row in csv_reader:\n",
    "# \t\t\tif not row:\n",
    "# \t\t\t\tcontinue\n",
    "# \t\t\tdataset.append(row)\n",
    "# \treturn dataset\n",
    " \n",
    "# # Convert string column to float\n",
    "# def str_column_to_float(dataset, column):\n",
    "# \tfor row in dataset:\n",
    "# \t\trow[column] = float(row[column].strip())\n",
    " \n",
    "# # Convert string column to integer\n",
    "# def str_column_to_int(dataset, column):\n",
    "# \tclass_values = [row[column] for row in dataset]\n",
    "# \tunique = set(class_values)\n",
    "# \tlookup = dict()\n",
    "# \tfor i, value in enumerate(unique):\n",
    "# \t\tlookup[value] = i\n",
    "# \tfor row in dataset:\n",
    "# \t\trow[column] = lookup[row[column]]\n",
    "# \treturn lookup\n",
    " \n",
    "# # Find the min and max values for each column\n",
    "# def dataset_minmax(dataset):\n",
    "# \tminmax = list()\n",
    "# \tstats = [[min(column), max(column)] for column in zip(*dataset)]\n",
    "# \treturn stats\n",
    " \n",
    "# # Rescale dataset columns to the range 0-1\n",
    "# def normalize_dataset(dataset, minmax):\n",
    "# \tfor row in dataset:\n",
    "# \t\tfor i in range(len(row)-1):\n",
    "# \t\t\trow[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for i in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    " \n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    " \n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores\n",
    " \n",
    "# Calculate neuron activation for an input\n",
    "def activate(weights, inputs):\n",
    "    activation = weights[-1]\n",
    "    for i in range(len(weights)-1):\n",
    "        activation += weights[i] * inputs[i]\n",
    "    return activation\n",
    " \n",
    "# Transfer neuron activation\n",
    "def transfer(activation):\n",
    "    return 1.0 / (1.0 + exp(-activation))\n",
    " \n",
    "# Forward propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            neuron['output'] = transfer(activation)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "    return inputs\n",
    " \n",
    "# Calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "    return output * (1.0 - output)\n",
    " \n",
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network)-1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(neuron['output'] - expected[j])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    " \n",
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] -= l_rate * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] -= l_rate * neuron['delta']\n",
    " \n",
    "# Train a network for a fixed number of epochs\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "    for epoch in range(n_epoch):\n",
    "        for row in train:\n",
    "            outputs = forward_propagate(network, row)\n",
    "            expected = [0 for i in range(n_outputs)]\n",
    "            expected[row[-1]] = 1\n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, row, l_rate)\n",
    " \n",
    "# Initialize a network\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    " \n",
    "# Make a prediction with a network\n",
    "def predict(network, row):\n",
    "    outputs = forward_propagate(network, row)\n",
    "    return outputs.index(max(outputs))\n",
    " \n",
    "# Backpropagation Algorithm With Stochastic Gradient Descent\n",
    "def back_propagation(train, test, l_rate, n_epoch, n_hidden):\n",
    "    n_inputs = len(train[0]) - 1\n",
    "    n_outputs = len(set([row[-1] for row in train]))\n",
    "    network = initialize_network(n_inputs, n_hidden, n_outputs)\n",
    "    train_network(network, train, l_rate, n_epoch, n_outputs)\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        prediction = predict(network, row)\n",
    "        predictions.append(prediction)\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2dd7c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ecb0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Backprop on Seeds dataset\n",
    "seed(1)\n",
    "#-------------------------------------------\n",
    "# # load and prepare data\n",
    "# filename = 'seeds_dataset.csv'\n",
    "# dataset = load_csv(filename)\n",
    "# for i in range(len(dataset[0])-1):\n",
    "#     str_column_to_float(dataset, i)\n",
    "# # convert class column to integers\n",
    "# str_column_to_int(dataset, len(dataset[0])-1)\n",
    "#---------------------------------------------------\n",
    "# normalize input variables\n",
    "minmax = dataset_minmax(dataset)\n",
    "normalize_dataset(dataset, minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "4d505de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MSSubClass(scaled)', 'MSZoning_(scaled)', 'LotFrontage(scaled)',\n",
       "       'LotArea(scaled)', 'LotShape_(scaled)', 'LotConfig_(scaled)',\n",
       "       'Neighborhood_(scaled)', 'HouseStyle_(scaled)', 'OverallQual(scaled)',\n",
       "       'OverallCond(scaled)', 'RoofStyle_(scaled)', 'MasVnrType_(scaled)',\n",
       "       'MasVnrArea(scaled)', 'ExterQual_(scaled)', 'Foundation_(scaled)',\n",
       "       'BsmtQual_(scaled)', 'BsmtExposure_(scaled)', 'BsmtFinType1_(scaled)',\n",
       "       'BsmtFinSF1(scaled)', 'BsmtUnfSF(scaled)', 'TotalBsmtSF(scaled)',\n",
       "       'HeatingQC_(scaled)', '1stFlrSF(scaled)', '2ndFlrSF(scaled)',\n",
       "       'GrLivArea(scaled)', 'BsmtFullBath(scaled)', 'FullBath(scaled)',\n",
       "       'HalfBath(scaled)', 'BedroomAbvGr(scaled)', 'KitchenQual_(scaled)',\n",
       "       'TotRmsAbvGrd(scaled)', 'Fireplaces(scaled)', 'FireplaceQu_(scaled)',\n",
       "       'GarageType_(scaled)', 'House_Age(scaled)', 'Garage_Age(scaled)',\n",
       "       'GarageFinish_(scaled)', 'GarageCars(scaled)', 'GarageArea(scaled)',\n",
       "       'WoodDeckSF(scaled)', 'OpenPorchSF(scaled)', 'Exterior_Avg(scaled)',\n",
       "       'SaleCondition_(scaled)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeffe3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "l_rate = 0.3\n",
    "n_epoch = 500\n",
    "n_hidden = 5\n",
    "scores = evaluate_algorithm(dataset, back_propagation, n_folds, l_rate, n_epoch, n_hidden)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6470e73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3571f6d4",
   "metadata": {},
   "source": [
    "### Back propagation - Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "d13550b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing as pre\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "bfd9a3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "f10235f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ccip_dataset_tst = pd.read_excel('C:/Users/thsong/CCPP/CCPP/Folds5x2_pp.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "43571165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>V</th>\n",
       "      <th>AP</th>\n",
       "      <th>RH</th>\n",
       "      <th>PE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.96</td>\n",
       "      <td>41.76</td>\n",
       "      <td>1024.07</td>\n",
       "      <td>73.17</td>\n",
       "      <td>463.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25.18</td>\n",
       "      <td>62.96</td>\n",
       "      <td>1020.04</td>\n",
       "      <td>59.08</td>\n",
       "      <td>444.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.11</td>\n",
       "      <td>39.40</td>\n",
       "      <td>1012.16</td>\n",
       "      <td>92.14</td>\n",
       "      <td>488.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.86</td>\n",
       "      <td>57.32</td>\n",
       "      <td>1010.24</td>\n",
       "      <td>76.64</td>\n",
       "      <td>446.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.82</td>\n",
       "      <td>37.50</td>\n",
       "      <td>1009.23</td>\n",
       "      <td>96.62</td>\n",
       "      <td>473.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      AT      V       AP     RH      PE\n",
       "0  14.96  41.76  1024.07  73.17  463.26\n",
       "1  25.18  62.96  1020.04  59.08  444.37\n",
       "2   5.11  39.40  1012.16  92.14  488.56\n",
       "3  20.86  57.32  1010.24  76.64  446.48\n",
       "4  10.82  37.50  1009.23  96.62  473.90"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ccip_dataset_tst.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "9204f005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............Reading the Dataset  and Dataset Pre-Processing ................\n",
      "Time Cost for Pre-processing and Reading the Dataset: 0.010970 seconds \n",
      " \n",
      "............... Initializing hyperparameters ................\n",
      "............... Setting 4 weights for hidden layers ................\n",
      "Time Cost for Setting HyperParameters: 0.001994 seconds \n",
      " \n",
      "............... Training Backpropagation Algorithm ................\n",
      "Training RMSE: 4.55\n",
      "Time Cost for Training algorithm: 12.539735 seconds \n",
      " \n",
      "............... Plotting RMSE Curve ................\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "............... Testing Backpropagation Algorithm ................\n",
      "Testing RMSE: 4.54\n",
      "Time Cost for Testing algorithm: 0.003990 seconds \n",
      " \n",
      "[[-0.99997098]\n",
      " [-0.99997287]\n",
      " [ 0.99997352]\n",
      " ...\n",
      " [ 0.99997239]\n",
      " [ 0.99997194]\n",
      " [ 0.99997229]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This Back-propagation algorithm is implemented on Combined Cycle Power Plant UCI Dataset. Link is given below\n",
    "Dataset Link: https://archive.ics.uci.edu/ml/datasets/combined+cycle+power+plant\n",
    "'''\n",
    "print(\"...............Reading the Dataset  and Dataset Pre-Processing ................\")\n",
    "start_time = time.time()\n",
    "#dataset = shuffle(pd.read_csv(\"ccip_dataset.csv\"))\n",
    "dataset = shuffle(ccip_dataset_tst)\n",
    "\n",
    "\n",
    "x = dataset[[\"AT\",\"V\",\"AP\",\"RH\"]]\n",
    "y = dataset[['PE']]\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.2)\n",
    "\n",
    "\n",
    "# Normalizing data using Standard Scaler Fit Transform\n",
    "x_train = pre.StandardScaler().fit_transform(x_train)\n",
    "x_test = pre.StandardScaler().fit_transform(x_test)\n",
    "\n",
    "# Converting pd dataframe to numpy array to match compatibility\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(\"Time Cost for Pre-processing and Reading the Dataset: %f seconds \\n \" % total_time)\n",
    "\n",
    "\n",
    "# Hyperbolic Tangent Activation function\n",
    "def hyperbolic_tanh(x):\n",
    "  return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "\n",
    "# Hyperbolic derivative\n",
    "def derivative_hyperbolic(x):\n",
    "  return 1 - hyperbolic_tanh(x) * hyperbolic_tanh(x)\n",
    "\n",
    "print(\"............... Initializing hyperparameters ................\")\n",
    "start_time = time.time()\n",
    "# Setting Hyperparameters\n",
    "actual_out_size = y_train.size\n",
    "np.random.seed(10)\n",
    "inp = 4\n",
    "hd = 6\n",
    "out = 1\n",
    "\n",
    "print(\"............... Setting 4 weights for hidden layers ................\")\n",
    "w1_l1 = np.random.randn(inp, hd)\n",
    "w2_l2 = np.random.randn(hd, hd)\n",
    "w3_l3 = np.random.randn(hd, hd)\n",
    "w4_l4 = np.random.randn(hd, hd)\n",
    "out_w = np.random.randn(hd, out)\n",
    "\n",
    "rmse_list = []\n",
    "\n",
    "epochs = 500\n",
    "eta = 0.0001\n",
    "alpha = 0.7\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(\"Time Cost for Setting HyperParameters: %f seconds \\n \" %total_time)\n",
    "\n",
    "print(\"............... Training Backpropagation Algorithm ................\")\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    # Feedforward for 4 hidden layers by calling activation function\n",
    "    l1 = np.dot(x_train, w1_l1)\n",
    "    l1_out = hyperbolic_tanh(l1)\n",
    "\n",
    "    l2 = np.dot(l1_out, w2_l2)\n",
    "    l2_out = hyperbolic_tanh(l2)\n",
    "\n",
    "    l3 = np.dot(l2_out, w3_l3)\n",
    "    l3_out = hyperbolic_tanh(l3)\n",
    "\n",
    "    l4 = np.dot(l3_out, w4_l4)\n",
    "    l4_out = hyperbolic_tanh(l4)\n",
    "\n",
    "    output = np.dot(l4_out, out_w)\n",
    "    final_out = hyperbolic_tanh(output)\n",
    "\n",
    "    rmse = np.sqrt(np.mean(np.square(final_out - y_train))) /100\n",
    "    rmse_list.append(rmse)\n",
    "\n",
    "    # Backpropagation for 4 hidden layers\n",
    "    final_err = final_out - y_train\n",
    "    final_tanh_derivative = final_err * derivative_hyperbolic(final_out)\n",
    "\n",
    "    l4_err = np.dot(final_tanh_derivative, out_w.T)\n",
    "    l4_derivative = l4_err * derivative_hyperbolic(l4_out)\n",
    "\n",
    "    l3_err = np.dot(l4_derivative, w4_l4.T)\n",
    "    l3_derivative = l3_err * derivative_hyperbolic(l3_out)\n",
    "\n",
    "    l2_err = np.dot(l3_derivative, w3_l3.T)\n",
    "    l2_derivative = l2_err * derivative_hyperbolic(l2_out)\n",
    "\n",
    "    l1_err = np.dot(l2_derivative, w2_l2.T)\n",
    "    l1_derivative = l1_err * derivative_hyperbolic(l1_out)\n",
    "\n",
    "    # Divide weights as per size of output\n",
    "    output_weights = np.dot(l4_out.T, final_tanh_derivative) / actual_out_size\n",
    "    weights4 = np.dot(l3_out.T, l4_derivative) / actual_out_size\n",
    "    weights3 = np.dot(l2_out.T, l3_derivative) / actual_out_size\n",
    "    weights2 = np.dot(l1_out.T, l2_derivative) / actual_out_size\n",
    "    weights1 = np.dot(x_train.T, l1_derivative) / actual_out_size\n",
    "\n",
    "    out_w -= eta * alpha * output_weights\n",
    "    w4_l4 -= eta * alpha * weights4\n",
    "    w3_l3 -= eta * alpha * weights3\n",
    "    w2_l2 -= eta * alpha * weights2\n",
    "    w1_l1 -= eta * alpha * weights1\n",
    "\n",
    "print(\"Training RMSE: \"+str(round(rmse_list[-1],2)))\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(\"Time Cost for Training algorithm: %f seconds \\n \" %total_time)\n",
    "\n",
    "print(\"............... Plotting RMSE Curve ................\")\n",
    "plt.title(\"RMSE Curve\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.plot(rmse_list)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"............... Testing Backpropagation Algorithm ................\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Feedforward for 4 hidden layers by calling activation function\n",
    "l1 = np.dot(x_test, w1_l1)\n",
    "l1_out = hyperbolic_tanh(l1)\n",
    "\n",
    "l2 = np.dot(l1_out, w2_l2)\n",
    "l2_out = hyperbolic_tanh(l2)\n",
    "\n",
    "l3 = np.dot(l2_out, w3_l3)\n",
    "l3_out = hyperbolic_tanh(l3)\n",
    "\n",
    "l4 = np.dot(l3_out, w4_l4)\n",
    "l4_out = hyperbolic_tanh(l4)\n",
    "\n",
    "output = np.dot(l4_out, out_w)\n",
    "final_out = hyperbolic_tanh(output)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(np.mean(np.square(final_out - y_test))) /100\n",
    "\n",
    "print(\"Testing RMSE: \"+str(round(rmse,2)))\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(\"Time Cost for Testing algorithm: %f seconds \\n \" %total_time)\n",
    "\n",
    "print(final_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "6f556175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2943</th>\n",
       "      <td>450.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>464.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3911</th>\n",
       "      <td>440.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9213</th>\n",
       "      <td>466.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>436.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2854</th>\n",
       "      <td>472.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6116</th>\n",
       "      <td>441.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7931</th>\n",
       "      <td>486.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3628</th>\n",
       "      <td>444.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5321</th>\n",
       "      <td>437.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9568 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          PE\n",
       "2943  450.60\n",
       "1983  464.36\n",
       "3911  440.15\n",
       "9213  466.27\n",
       "805   436.60\n",
       "...      ...\n",
       "2854  472.77\n",
       "6116  441.08\n",
       "7931  486.18\n",
       "3628  444.28\n",
       "5321  437.52\n",
       "\n",
       "[9568 rows x 1 columns]"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[['PE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "d531886b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............Reading the Dataset  and Dataset Pre-Processing ................\n",
      "Time Cost for Pre-processing and Reading the Dataset: 0.008992 seconds \n",
      " \n",
      "............... Initializing hyperparameters ................\n",
      "............... Setting 4 weights for hidden layers ................\n",
      "Time Cost for Setting HyperParameters: 0.000982 seconds \n",
      " \n",
      "............... Training Backpropagation Algorithm ................\n",
      "Training RMSE: 4.55\n",
      "Time Cost for Training algorithm: 13.263560 seconds \n",
      " \n",
      "............... Plotting RMSE Curve ................\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "............... Testing Backpropagation Algorithm ................\n",
      "Testing RMSE: 4.54\n",
      "Time Cost for Testing algorithm: 0.004987 seconds \n",
      " \n",
      "[[ 0.99996254]\n",
      " [ 0.9999576 ]\n",
      " [-0.99995744]\n",
      " ...\n",
      " [ 0.99996096]\n",
      " [-0.99993833]\n",
      " [-0.99996373]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This Back-propagation algorithm is implemented on Combined Cycle Power Plant UCI Dataset. Link is given below\n",
    "Dataset Link: https://archive.ics.uci.edu/ml/datasets/combined+cycle+power+plant\n",
    "'''\n",
    "print(\"...............Reading the Dataset  and Dataset Pre-Processing ................\")\n",
    "start_time = time.time()\n",
    "#dataset = shuffle(pd.read_csv(\"ccip_dataset.csv\"))\n",
    "dataset = shuffle(ccip_dataset_tst)\n",
    "\n",
    "\n",
    "x = dataset[[\"AT\",\"V\",\"AP\",\"RH\"]]\n",
    "y = dataset[['PE']]\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.2)\n",
    "\n",
    "\n",
    "# Normalizing data using Standard Scaler Fit Transform\n",
    "x_train = pre.StandardScaler().fit_transform(x_train)\n",
    "x_test = pre.StandardScaler().fit_transform(x_test)\n",
    "\n",
    "# Converting pd dataframe to numpy array to match compatibility\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(\"Time Cost for Pre-processing and Reading the Dataset: %f seconds \\n \" % total_time)\n",
    "\n",
    "\n",
    "# Hyperbolic Tangent Activation function\n",
    "def hyperbolic_tanh(x):\n",
    "  return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "\n",
    "# Hyperbolic derivative\n",
    "def derivative_hyperbolic(x):\n",
    "  return 1 - hyperbolic_tanh(x) * hyperbolic_tanh(x)\n",
    "\n",
    "print(\"............... Initializing hyperparameters ................\")\n",
    "start_time = time.time()\n",
    "# Setting Hyperparameters\n",
    "actual_out_size = y_train.size\n",
    "np.random.seed(10)\n",
    "inp = 4\n",
    "hd = 6\n",
    "out = 1\n",
    "\n",
    "print(\"............... Setting 4 weights for hidden layers ................\")\n",
    "w1_l1 = np.random.randn(inp, hd)\n",
    "w2_l2 = np.random.randn(hd, hd)\n",
    "w3_l3 = np.random.randn(hd, hd)\n",
    "w4_l4 = np.random.randn(hd, hd)\n",
    "out_w = np.random.randn(hd, out)\n",
    "\n",
    "rmse_list = []\n",
    "\n",
    "epochs = 500\n",
    "eta = 0.0001\n",
    "alpha = 0.7\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(\"Time Cost for Setting HyperParameters: %f seconds \\n \" %total_time)\n",
    "\n",
    "print(\"............... Training Backpropagation Algorithm ................\")\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    # Feedforward for 4 hidden layers by calling activation function\n",
    "    l1 = np.dot(x_train, w1_l1)\n",
    "    l1_out = hyperbolic_tanh(l1)\n",
    "\n",
    "    l2 = np.dot(l1_out, w2_l2)\n",
    "    l2_out = hyperbolic_tanh(l2)\n",
    "\n",
    "    l3 = np.dot(l2_out, w3_l3)\n",
    "    l3_out = hyperbolic_tanh(l3)\n",
    "\n",
    "    l4 = np.dot(l3_out, w4_l4)\n",
    "    l4_out = hyperbolic_tanh(l4)\n",
    "\n",
    "    output = np.dot(l4_out, out_w)\n",
    "    final_out = hyperbolic_tanh(output)\n",
    "\n",
    "    rmse = np.sqrt(np.mean(np.square(final_out - y_train))) /100\n",
    "    rmse_list.append(rmse)\n",
    "\n",
    "    # Backpropagation for 4 hidden layers\n",
    "    final_err = final_out - y_train\n",
    "    final_tanh_derivative = final_err * derivative_hyperbolic(final_out)\n",
    "\n",
    "    l4_err = np.dot(final_tanh_derivative, out_w.T)\n",
    "    l4_derivative = l4_err * derivative_hyperbolic(l4_out)\n",
    "\n",
    "    l3_err = np.dot(l4_derivative, w4_l4.T)\n",
    "    l3_derivative = l3_err * derivative_hyperbolic(l3_out)\n",
    "\n",
    "    l2_err = np.dot(l3_derivative, w3_l3.T)\n",
    "    l2_derivative = l2_err * derivative_hyperbolic(l2_out)\n",
    "\n",
    "    l1_err = np.dot(l2_derivative, w2_l2.T)\n",
    "    l1_derivative = l1_err * derivative_hyperbolic(l1_out)\n",
    "\n",
    "    # Divide weights as per size of output\n",
    "    output_weights = np.dot(l4_out.T, final_tanh_derivative) / actual_out_size\n",
    "    weights4 = np.dot(l3_out.T, l4_derivative) / actual_out_size\n",
    "    weights3 = np.dot(l2_out.T, l3_derivative) / actual_out_size\n",
    "    weights2 = np.dot(l1_out.T, l2_derivative) / actual_out_size\n",
    "    weights1 = np.dot(x_train.T, l1_derivative) / actual_out_size\n",
    "\n",
    "    out_w -= eta * alpha * output_weights\n",
    "    w4_l4 -= eta * alpha * weights4\n",
    "    w3_l3 -= eta * alpha * weights3\n",
    "    w2_l2 -= eta * alpha * weights2\n",
    "    w1_l1 -= eta * alpha * weights1\n",
    "\n",
    "print(\"Training RMSE: \"+str(round(rmse_list[-1],2)))\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(\"Time Cost for Training algorithm: %f seconds \\n \" %total_time)\n",
    "\n",
    "print(\"............... Plotting RMSE Curve ................\")\n",
    "plt.title(\"RMSE Curve\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.plot(rmse_list)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"............... Testing Backpropagation Algorithm ................\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Feedforward for 4 hidden layers by calling activation function\n",
    "l1 = np.dot(x_test, w1_l1)\n",
    "l1_out = hyperbolic_tanh(l1)\n",
    "\n",
    "l2 = np.dot(l1_out, w2_l2)\n",
    "l2_out = hyperbolic_tanh(l2)\n",
    "\n",
    "l3 = np.dot(l2_out, w3_l3)\n",
    "l3_out = hyperbolic_tanh(l3)\n",
    "\n",
    "l4 = np.dot(l3_out, w4_l4)\n",
    "l4_out = hyperbolic_tanh(l4)\n",
    "\n",
    "output = np.dot(l4_out, out_w)\n",
    "final_out = hyperbolic_tanh(output)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(np.mean(np.square(final_out - y_test))) /100\n",
    "\n",
    "print(\"Testing RMSE: \"+str(round(rmse,2)))\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(\"Time Cost for Testing algorithm: %f seconds \\n \" %total_time)\n",
    "\n",
    "print(final_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "1f18b065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MSSubClass(scaled)', 'MSZoning_(scaled)', 'LotFrontage(scaled)',\n",
       "       'LotArea(scaled)', 'LotShape_(scaled)', 'LotConfig_(scaled)',\n",
       "       'Neighborhood_(scaled)', 'HouseStyle_(scaled)', 'OverallQual(scaled)',\n",
       "       'OverallCond(scaled)', 'RoofStyle_(scaled)', 'MasVnrType_(scaled)',\n",
       "       'MasVnrArea(scaled)', 'ExterQual_(scaled)', 'Foundation_(scaled)',\n",
       "       'BsmtQual_(scaled)', 'BsmtExposure_(scaled)', 'BsmtFinType1_(scaled)',\n",
       "       'BsmtFinSF1(scaled)', 'BsmtUnfSF(scaled)', 'TotalBsmtSF(scaled)',\n",
       "       'HeatingQC_(scaled)', '1stFlrSF(scaled)', '2ndFlrSF(scaled)',\n",
       "       'GrLivArea(scaled)', 'BsmtFullBath(scaled)', 'FullBath(scaled)',\n",
       "       'HalfBath(scaled)', 'BedroomAbvGr(scaled)', 'KitchenQual_(scaled)',\n",
       "       'TotRmsAbvGrd(scaled)', 'Fireplaces(scaled)', 'FireplaceQu_(scaled)',\n",
       "       'GarageType_(scaled)', 'House_Age(scaled)', 'Garage_Age(scaled)',\n",
       "       'GarageFinish_(scaled)', 'GarageCars(scaled)', 'GarageArea(scaled)',\n",
       "       'WoodDeckSF(scaled)', 'OpenPorchSF(scaled)', 'Exterior_Avg(scaled)',\n",
       "       'SaleCondition_(scaled)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "d58d8039",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_house4['SalePrice(log)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "f1970a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       12.247699\n",
       "1       12.109016\n",
       "2       12.317171\n",
       "3       11.849405\n",
       "4       12.429220\n",
       "          ...    \n",
       "1427    12.128117\n",
       "1428    12.072547\n",
       "1429    12.254868\n",
       "1430    12.493133\n",
       "1431    11.864469\n",
       "Name: SalePrice(log), Length: 1432, dtype: float64"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "a2b78312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5464476 ,  0.51496564,  1.63919878, ...,  0.88437415,\n",
       "        -0.41321925,  0.46627249],\n",
       "       [ 0.5464476 ,  0.51496564, -0.22803278, ..., -0.73833325,\n",
       "        -0.42780365,  0.46627249],\n",
       "       [ 1.49308357,  0.51496564, -0.98501855, ..., -0.73833325,\n",
       "        -0.94992519,  0.46627249],\n",
       "       ...,\n",
       "       [ 0.07312961,  0.51496564, -0.22803278, ...,  0.05646221,\n",
       "         1.29315561,  0.46627249],\n",
       "       [ 0.4281181 , -1.80664394, -0.48036137, ..., -0.73833325,\n",
       "        -1.56247001,  0.46627249],\n",
       "       [ 2.43971955, -1.80664394,  0.02681699, ...,  0.85125767,\n",
       "        -1.32620272,  0.46627249]])"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "37dad383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1145, 43)"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "a5aee853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 558,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "244e9a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............Reading the Dataset  and Dataset Pre-Processing ................\n",
      "Time Cost for Pre-processing and Reading the Dataset: 0.000997 seconds \n",
      " \n",
      "............... Initializing hyperparameters ................\n",
      "............... Setting 43 weights for hidden layers ................\n",
      "Time Cost for Setting HyperParameters: 0.000998 seconds \n",
      " \n",
      "............... Training Backpropagation Algorithm ................\n",
      "Training RMSE: 0.12\n",
      "Time Cost for Training algorithm: 161.238249 seconds \n",
      " \n",
      "............... Plotting RMSE Curve ................\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAxS0lEQVR4nO3deXxdVbn/8c83UzM0aZO2aZsOtIVCaQsUGkot4AXKzJWCisIVqaIXvepF0HsVhyt6J1FRlJ84FFFBEVQEAUWgtyIVwUIKpbR0HuiQNE3plM4Znt8fe6WchpPhpDk5yTnP+/U6r7332nvt/axQznP2WnuQmeGcc851VlaqA3DOOde3eOJwzjmXEE8czjnnEuKJwznnXEI8cTjnnEuIJw7nnHMJ8cThnHMuIZ44XMaRtF7Sfkl7JG2R9HNJ/WPW/1ySSbq8Vb3vhvIPheU8Sd+WtCnsa52kO9o4Tsvn++3Edbyk30raJmmXpMWSPiMpOwl/Bue6zBOHy1TvMrP+wBTgVOALrdavBGa3LEjKAa4C1sRs8wWgEpgGFAPnAq/EO07M51PxgpF0LLAA2AicZGYDwvEqw74TEuJ1Lik8cbiMZmZbgKeIEkisx4EzJZWG5YuBxcCWmG1OBx4xs2qLrDez+7oYyteA583sM2ZWE2JbYWb/ZGY7JZ0jaVNshXBGc36Y/6qkhyT9UtJu4IvhbKcsZvtTw9lMbli+XtIySTskPSXpmC7G7jKMJw6X0SSNBC4BVrdadQB4DLg6LF8HtE4Kfwc+I+kTkk6SpKMI5XzgoaOoDzAr7GMg8C3gBeA9Mev/CXjIzBokXQF8EXg3MAT4K/DAUR7fZYiMSRySfippq6Ql3bS/0ZKeDr/YXpc0ppP1Jkh6QdJBSf/Wznb3S1ohaUmIveVXoiTdKWl16AM/LaZO3DZKKpM0V9KqMC2NWfeFsK8Vki6KKZ8q6bWw7s6WL0VJ/ST9OpQviG23pNnhGKskxXbzjA3brgp18zpqS1dJeqeklyU1SnpvO5v+XlI9UdfQVuDWONvcB1wnaQDwD8DvW63/OvAN4ANAFbA5tt0xx9kZ8/nnNuIZBNS027iOvWBmvzezZjPbD/wKuAaivzVREvxV2PZjwNfNbJmZNQL/C0zxsw7XGRmTOICfE3U3dJf7gG+Z2YlEfdxbW28gaX2cetuBG4HbO9j//cAE4CSgAPhoKL8EGB8+NwA/jKnzc+K38RZgnpmNB+aFZSRNJPoymRTq/SBmIPaHYf8tx2rZ70eAHWZ2HHAH0RcnoUvkVuAMor/HrTEJ6hvAHeH4O8I+OmpLV20APsRbX5BtucLMioFziP7Og1tvYGbPEf0a/zLwh/BlHLu+yczuMrMziX7l/w/wU0kntjrOwJjP3W3E8yYwvKPGdWBjq+WHgHdIqgDeCRjRmQXAMcD3WhIa0b9LASOOMgaXATImcZjZfKL/OQ6TdKykJyUtlPRXSRM6s6/whZtjZnPDvveY2b5OxrHVzF4CGjrY7onQb27Ai8DIsGoWcF9Y9XdgoKThbbUxps69Yf5e4IqY8gfN7KCZrSPqrpkW9ldiZi+E49/Xqk7Lvh4CZoZfsxcBc81su5ntAOYCF4d15/FWN0zr48dti6RrJb0oaZGkH6uTVxaFcYbFQHMnt3+WKOG2lch/CXyWt3dTtd7PfjO7iygxTuzMsVv5P47sVmptL1DYshD+HkNah9Eqpp3A08D7iLqpHrC3Hoe9EfhYq6RWYGbPdyF2l2EyJnG0YQ7wr2Y2Ffg34AedrHc8sFPSw5JekfStzn6xJSp0UX0QeDIUjeDIX5ab6PhX4tCYAdcaoLyDfY0I8/GOcbhO6OLYRdTN0ta+BgE7w7Zt7it2XfjF/n7gTDObAjQRdQcly3eBCyRNibPuTuACYH7rFZJuUjRoXSApJ3RTFfP2K6s641ZgRvi3NCzs/zhFg90Dia7yypd0Wfg38WWgXyf2+yui8Zn3cORZ2I+AL0iaFI41QNJVXYjbZaCMvWRP0XX7M4Df6q0xzX5h3buB/4xTbbOZXUT0dzub6DLODcCvibpH7pF0F3Bm2L5C0qIw/1sz+58uhPoDYL6ZtXQxxBuA7epLVdraV3vHSLROV/Y1E5gKvBT+2xQQugIl3QfEGwv5gZl1NvEfeUCzurDf/6DVr34z207UvRfPfuDbwHEh7pXAe8xsbcw2j0tqilmea2ZXxolhjaR3AP8NLFV0Oe164GdAvZk1SfoE8BMgG/gmRyb3tjwW6mwws1djjvdI+H/gwTCusYvoLPG3ndiny3AZmziIzrZ2hl+0RzCzh4GH26m7CXil5QtC0u+B6cA9ZvbJlo0krY+3/86SdCtRd8THWh17VMzySKC6g13VShpuZjWhK6hlPKatfW3ira6x1sdoqbMpfLkNIOoe20Q0XhBb5y/ANqIuqJxw1hFvX62PI+BeM2t9bwVmdl0Hbe2QmY2JU/YvMfMfaqfuWTHzPwZ+nMhxOohrBdG9G22t/zlRt1qL22PWfbWNOvtp4z4QM/sF8ItEYnQOMrirysx2A+taTs/DFT6ndLL6S0CppJY+5vOA17szPkkfJRo3uMbMYvvrHyO60keSpgO7Wrqh2vEYb93MNht4NKb86nCl1FiiQeoXw/7qJU0PYxTXtarTsq/3An8O/eZPARdKKg2D4hcCT4V1z4Rt4x0/XlvmAe+VVB7+FmV+tY9zvYiZZcSH6Br1GqJB6U1EV/aMJRo7eJXoi/8rCezvAqIbwl4j+hWYF2eb9XHKhoXj7wZ2hvmSsO4JoCLMNxLdpbwofL4SygXcFda9BlS218ZQPojoy3hVmJbF1PlS2NcK4JKY8kpgSVj3fUChPJ+oO2M10aD9uJg614fy1cCHY8rHhW1Xh7r9OtGW94d2LwYWAtM7+d/l9ND2vURXKi1N9b89//gn3T4tXwbOOedcp2RsV5VzzrmuyYjB8cGDB9uYMWNSHYZzzvUpCxcu3GZmre8XSm7ikHQz0R3PRtSH/WHgXcBXgROBaWZWFafeKKIbroYR3cg1x8y+F9Z9FfhnoC5s/kUze6K9OMaMGUNV1dsO45xzrh2S3ohXnrSuKkkjiB6tUWlmk4muPb+aaMD13cS5oSpGI/BZix7nMR34ZLhbu8UdZjYlfNpNGs4557pXsruqcoACSQ1Ej0uoNrNlAGrnQaIWXZLZcqdzvaRlRHcZd+slr8455xKXtDMOM9tMdIPSBqIksMvMnk50P4qevnoq0UtuWnwqPE31pzEP0mtd7wZJVZKq6urq4m3inHOuC5LZVVVK9BC7sUAFUCTp2gT30R/4HXCTRTfsQfQE1WOJXrxTQ/TIh7cxszlmVmlmlUOGvG1sxznnXBcl83Lc84F1ZlZnZg1Ej/CY0dnK4UFuvwPut+gRIACYWa1Fj7NuBu4meoS3c865HpLMxLEBmC6pMDy2YiawrDMVw/b3AMvM7Dut1sW+s+BKosF255xzPSSZYxwLiN7B8DLRpbhZwBxJVyp6d/I7gD9KegpAUoWkliukziR6lPh54X0MiyRdGtZ9U9Gb6RYD5wI3J6sNzjnn3i4jHjlSWVlpXbmP45nlW1m2ZTefOOe4JETlnHO9m6SFZlbZutwfOdKO51Zv43v/t4qm5vRPrs4511meONoxYVgxBxubWf/m3lSH4pxzvYYnjnZMGFYCwPKa+hRH4pxzvYcnjnaMH9qfLMGKLbs73tg55zKEJ4525OdmM2ZwEcu3+BmHc8618MTRgROHlXjicM65GJ44OnDCsGI2bN/H3oONqQ7FOed6BU8cHZgwrBiAlbV+1uGcc+CJo0OHr6zy7irnnAM8cXRoZGkBRXnZrPDE4ZxzgCeODmVlieOHFbOsxi/Jdc458MTRKROGlbCitp5MeK6Xc851xBNHJ0wYVszOfQ1srT+Y6lCccy7lPHF0QsuVVd5d5Zxznjg6ZcLw6Mqq1z1xOOecJ47OGFCQy6iyApZWe+JwzrmkJg5JN0taKmmJpAck5Uu6KpQ1S3rbC0Ji6l4saYWk1ZJuiSkvkzRX0qowLU1mG1pMrhjA0s27euJQzjnXqyUtcUgaAdwIVJrZZCAbuJroHeHvBua3UzcbuAu4BJgIXCNpYlh9CzDPzMYD88Jy0k2qKGH9m/uoP9DQE4dzzrleK9ldVTlAgaQcoBCoNrNlZraig3rTgNVmttbMDgEPArPCulnAvWH+XuCK7g/77SaNGADA695d5ZzLcElLHGa2Gbgd2ADUALvM7OlOVh8BbIxZ3hTKAIaaWU04Rg1QHm8Hkm6QVCWpqq6uritNOMLkiihxLPHE4ZzLcMnsqiolOjsYC1QARZKu7Wz1OGUJ3X1nZnPMrNLMKocMGZJI1biGFPejvLgfS6t9nMM5l9mS2VV1PrDOzOrMrAF4GJjRybqbgFExyyOB6jBfK2k4QJhu7aZ4OzR5xACWbvYzDudcZktm4tgATJdUKEnATGBZJ+u+BIyXNFZSHtGg+mNh3WPA7DA/G3i0G2Nu16SKElbX7eFAQ1NPHdI553qdZI5xLAAeAl4GXgvHmiPpSkmbgHcAf5T0FICkCklPhLqNwKeAp4iSzW/MbGnY9W3ABZJWAReE5R4xqWIATc3mj1h3zmW0nGTu3MxuBW5tVfxI+LTethq4NGb5CeCJONu9SXT20uMmj4juIF+yeRdTRg1MRQjOOZdyfud4AkYMLGBAQa7fQe6cy2ieOBIgickjSljid5A75zKYJ44EnTxyIMu37PYBcudcxvLEkaBTRg6kocn8EevOuYzliSNBLYPir27cmdI4nHMuVTxxJGjYgHyGlvTj1U0+zuGcy0yeOLrglJED/YzDOZexPHF0wSmjBrJ221527fNHrDvnMo8nji5oGedYvHlnSuNwzrlU8MTRBSeNjB6x7t1VzrlM5ImjC0ryczl2SBGLNvoAuXMu83ji6KJTRg1k0cadmCX0mhDnnOvzPHF00ZRRA9m25yDVuw6kOhTnnOtRnji6qGWA/JUNO1IbiHPO9TBPHF104vASCnKzqVrvicM5l1k8cXRRbnYWU0YNZOEbnjicc5klqYlD0s2SlkpaIukBSfmSyiTNlbQqTEvj1DtB0qKYz25JN4V1X5W0OWbdpW87cA+Zekwpr9fsZu/BxlSF4JxzPS5piUPSCOBGoNLMJgPZRO8OvwWYZ2bjgXlh+QhmtsLMppjZFGAqsI8j3xp4R8v68KbAlJg6ppSmZvP7OZxzGSXZXVU5QIGkHKAQqAZmAfeG9fcCV3Swj5nAGjN7I1lBdtVpo0uRoMq7q5xzGSRpicPMNgO3AxuAGmCXmT0NDDWzmrBNDVDewa6uBh5oVfYpSYsl/TReVxeApBskVUmqqqurO6q2tGVAQS7Hlxd74nDOZZRkdlWVEp1djAUqgCJJ1ya4jzzgcuC3McU/BI4FphAlpG/Hq2tmc8ys0swqhwwZkngDOmnqmFJeeWMHzc1+I6BzLjMks6vqfGCdmdWZWQPwMDADqJU0HCBMt7azj0uAl82stqXAzGrNrMnMmoG7gWlJa0EnVB5TSv3BRlZurU9lGM4512OSmTg2ANMlFUoS0VjFMuAxYHbYZjbwaDv7uIZW3VQtSSe4EljSbRF3wdRjop4yv5/DOZcpkjnGsQB4CHgZeC0caw5wG3CBpFXABWEZSRWSDl8hJakwrH+41a6/Kek1SYuBc4Gbk9WGzhhdVsiQ4n5Urd+eyjCcc67H5CRz52Z2K3Brq+KDRGcfrbetBi6NWd4HDIqz3Qe7OcyjIolpY8p4cd12zIzo5Mo559KX3zneDaaPK6N61wE2bt+f6lCccy7pPHF0g+njohOjv699M8WROOdc8nni6AbHlfdnUFGeJw7nXEbwxNENJDF93CAWhHEO55xLZ544uskZ48rYvHM/m3b4OIdzLr154ugmLeMcL3h3lXMuzXni6Cbjy/tT5uMczrkM4Imjm0TjHGUsWOvjHM659OaJoxtNHzeIzTv3s2H7vlSH4pxzSeOJoxvNOHYwAM+t3pbiSJxzLnk8cXSjY4cUUTEgn+dWeeJwzqUvTxzdSBJnjR/M31Zvo8nfz+GcS1OeOLrZWeOHsPtAI4s37Ux1KM45lxSeOLrZWccNRsK7q5xzacsTRzcrK8pjUkUJf/UBcudcmvLEkQRnHTeEVzbsYO/BxlSH4pxz3S6piUPSzZKWSloi6QFJ+ZLKJM2VtCpMS9uouz686W+RpKqY8k7VT6Wzxw+moclYsM7vInfOpZ+kJQ5JI4AbgUozmwxkA1cDtwDzzGw8MC8st+VcM5tiZpUxZYnUT4mpx5SSn5vF/JXeXeWcSz/J7qrKAQok5QCFQDUwC7g3rL8XuCLBfR5t/aTLz81mxrGD+fPyrf74Eedc2kla4jCzzcDtwAagBthlZk8DQ82sJmxTA5S3tQvgaUkLJd0QU96p+pJukFQlqaqurq57GpWAcyeUs2H7PtbU7e3xYzvnXDIls6uqlOjsYCxQARRJujaBXZxpZqcBlwCflPTORI5vZnPMrNLMKocMGZJI1W5x3oQonz2zfGuPH9s555IpmV1V5wPrzKzOzBqAh4EZQK2k4QBhGveb1cyqw3Qr8AgwLazqVP1UGzGwgBOGFvNnTxzOuTSTzMSxAZguqVCSgJnAMuAxYHbYZjbwaOuKkookFbfMAxcCS8LqDuv3FudOKOel9dvZfaAh1aE451y3SeYYxwLgIeBl4LVwrDnAbcAFklYBF4RlJFVIeiJUHwo8J+lV4EXgj2b2ZFgXt35vdN6Echqbze8id86llZxk7tzMbgVubVV8kOjso/W21cClYX4tcEob+3wzXv3e6LTRAxlQkMufl2/l0pOGpzoc55zrFn7neBLlZGfxzuOH8JcVW2n2p+U659KEJ44kO//EcrbtOcQrG3ekOhTnnOsWnjiS7NwJ5eRmiyeXbEl1KM451y08cSRZSX4uZx43mCeXbvG7yJ1zacETRw+4eNIwNm7fz7Ka+lSH4pxzR80TRw84f+JQsgRPLvXuKudc3+eJowcM7t+P08eU8ZSPczjn0oAnjh5y0aRhrKitZ902f+ihc65v88TRQy6aPAyAp7y7yjnXx3ni6CEjBhZw8sgBflmuc67P88TRgy6aNIxFG3eyeef+VIfinHNd5omjB73r5AoAHn+1OsWROOdc13ni6EGjBxVy6uiBPLrIE4dzru9qN3FIOi9mfmyrde9OVlDpbNYpFSyr2c3KWr8Z0DnXN3V0xnF7zPzvWq37cjfHkhEuO7mCLMFjftbhnOujOkocamM+3rLrhCHF/TjzuME8+upmf3aVc65P6ihxWBvz8ZbfRtLNkpZKWiLpAUn5ksokzZW0KkxL49QbJekZSctC/U/HrPuqpM2SFoXPpR3F0dvMmjKCjdv388rGnakOxTnnEtZR4hgn6TFJj8fMtyyPba+ipBHAjUClmU0GsoGrgVuAeWY2HpgXlltrBD5rZicC04FPSpoYs/4OM5sSPk/Eqd+rXTRpKHk5Wd5d5Zzrkzp6deysmPnbW61rvdzW/gskNQCFQDXwBeCcsP5e4C/A52MrmVkNUBPm6yUtA0YAr3fimL1ecX4uMyeU84fF1Xz5shPJyfaL25xzfUe731hm9mzsB3ge2A0sC8vt1d1MlFw2ECWBXWb2NDA0JIaWBFHe3n4kjQFOBRbEFH9K0mJJP43X1RXq3SCpSlJVXV1de4dIiStOHcG2PYd4dmXvi80559rT0eW4P5I0KcwPAF4F7gNekXRNB3VLic5YxgIVQJGkaxMJTlJ/oqu5bjKz3aH4h8CxwBSihPTteHXNbI6ZVZpZ5ZAhQxI5bI84b0I5g/vn8euXNqY6FOecS0hHfSRnm9nSMP9hYKWZnQRMBT7XQd3zgXVmVmdmDcDDwAygVtJwgDDdGq+ypFyipHG/mT3cUm5mtWbWZGbNwN3AtA7i6JVys7N492kj+fPyrdTVH0x1OM4512kdJY5DMfMXAL8HMLPOPKlvAzBdUqEkATOBZcBjwOywzWzg0dYVw/b3EHWJfafVuuExi1cCSzoRS6/0vsqRNDYbj7yyKdWhOOdcp3WUOHZK+kdJpwJnAk8CSMoBCtqraGYLgIeAl4HXwrHmALcBF0haRZSMbgv7rJDUcoXUmcAHgfPiXHb7TUmvSVoMnAvcnFCLe5Hjyos5bfRAfv3SRr+nwznXZ3R0VdXHgDuBYUTjDC1nGjOBP3a0czO7Fbi1VfHBUL/1ttXApWH+Odq4wdDMPtjRcfuS958+is//7jVe3rCTqcfEHed3zrlepaOrqlaa2cXhfomfx5Q/ZWafTXp0GeCykysozMvmNz5I7pzrI9o945B0Z3vrzezG7g0n8/Tvl8NlJw3nD4ur+cq7JlLUr6OTQOecS62Oxjg+DpxFdONeFbCw1cd1g6unjWbvoSYeeWVzqkNxzrkOdfTzdjhwFfB+oseA/Br4nZntSHZgmeS00QOZPKKE+15YzwfOGE10UZlzzvVOHY1xvGlmPzKzc4EPAQOBpZLSaoA61SRx3TvGsLJ2Dy+sfTPV4TjnXLs69ZAkSacBNwHXAn/Cu6m63eWnVFBamMt9z7+R6lCcc65dHT1y5GuSFgKfAZ4letLtR8wsLR422Jvk52bz/tNH8/TrW9i8c3+qw3HOuTZ1dMbxH8AA4BTg68DL4eGCLTfguW70gTNGA3D/3/2swznXe3U0ON7uOzdc9xpVVsjME4fy4EsbuXHmePJzs1MdknPOvU1Hg+NvxPsAm4gu03Xd7EMzxrB97yF+75fmOud6qY7GOEokfUHS9yVdqMi/AmuB9/VMiJllxrGDmFRRwpz5a2lu9udXOed6n47GOH4BnED0kMKPAk8D7wVmmdms9iq6rpHEx//hWNZu28vTr9emOhznnHubjsY4xoX3byDpJ8A2YLSZ1Sc9sgx2yeRhjCor4EfPruGiSUP9hkDnXK/S0RlHQ8uMmTURvZjJk0aS5WRn8c9nj2PRxp28tN5v0nfO9S4dJY5TJO0On3rg5JZ5Sbs7qOuOwlVTR1FWlMePnl2T6lCcc+4IHV1VlW1mJeFTbGY5MfMlPRVkJirIy2b2O8bw5+VbWb7Fc7Rzrvfo1CNHukrSzZKWSloi6QFJ+ZLKJM2VtCpM4769SNLFklZIWi3plpjyTtVPB7NnHEP/fjncOW9VqkNxzrnDkpY4JI0AbiR6TMlkIBu4GrgFmGdm44F5Ybl13WzgLuASYCJwjaSJYXWH9dPFwMI8PnzmGJ54bQvLavyswznXOyT1jIPoqq2C8I7yQqL3eswC7g3r7wWuiFNvGrDazNaa2SHgwVCPTtZPGx89axzF/XL43v/5WYdzrndIWuIws83A7cAGoAbYZWZPA0PNrCZsUwOUx6k+Aoh9l+qmUEYn6yPpBklVkqrq6uq6o0kpMaAwl+vPGsuTS7ewtHpXqsNxzrmkdlWVEp0djAUqgCJJ13a2epyyhG6jNrM5ZlZpZpVDhgxJpGqvc/1ZYynOz+G7ftbhnOsFktlVdT7RfR91ZtYAPAzMAGolDQcI061x6m4CRsUsjyTq5qKT9dPKgIJcPnrWOOa+XsviTTtTHY5zLsMlM3FsAKZLKlR06/NMYBnwGDA7bDMbeDRO3ZeA8ZLGSsojGlR/LKzrTP20c/1ZYygryuPrTyzHzJ9h5ZxLnWSOcSwAHgJeJnrWVRYwB7gNuEDSKuCCsIykCklPhLqNwKeAp4iSzW/MbGnYddz66a44P5cbzzuOF9a+yV9W9N0xG+dc36dM+PVaWVlpVVVVqQ7jqB1qbObCO54lLyeLJ248m5zsZF8U55zLZJIWmlll63L/5ulD8nKy+PzFE1hZu4eHFm5KdTjOuQzliaOPuXjyMKYeU8p35q5k36HGVIfjnMtAnjj6GEl88dIT2Vp/kB884w9AdM71PE8cfdDUY0q58tQRzJm/lnXb9qY6HOdchvHE0Ud94ZIJ5OVk8bXHl/rluc65HuWJo48qL8nnpvPH85cVdf6KWedcj/LE0YfNnjGG44f25z8ff539h5pSHY5zLkN44ujDcrOz+M9Zk9m8cz/f83d2OOd6iCeOPm76uEG8r3Ikd/91La9t8qfnOueSzxNHGvjSZRMZVJTH5363mIam5lSH45xLc5440sCAglz+64rJLKvZzY/+4vd2OOeSyxNHmrho0jAuO3k4/+/Pq1lZW5/qcJxzacwTRxr52uWT6J+fw82/XsShRu+ycs4lhyeONDK4fz9ue/dJLK3ezbfnrkh1OM65NOWJI81cOGkY10wbzZz5a3lhzZupDsc5l4Y8caSh//jHExk7qIjP/GYRu/Y1pDoc51yaSVrikHSCpEUxn92SbpJ0iqQXJL0m6XFJJZ2tG9Z9VdLmmHWXJqsNfVVhXg7fvXoKdfUH+dzvXvVnWTnnulUyXx27wsymmNkUYCqwD3gE+Alwi5mdFJb/PYG6Le5oWW9mTySrDX3ZySMHcsslE3hqaS33PLcu1eE459JIT3VVzQTWmNkbwAnA/FA+F3hPAnVdAj5y1lgunjSMr/9pOS+t357qcJxzaaKnEsfVwANhfglweZi/ChiVQN0Wn5K0WNJPJZXGqyTpBklVkqrq6uq6GnefJolvXnUyo0oL+OT9L1NXfzDVITnn0kDSE4ekPKJE8dtQdD3wSUkLgWLgUAJ1AX4IHAtMAWqAb8era2ZzzKzSzCqHDBlytM3os0ryc/nBB6aya38Dn7h/IQcb/Sm6zrmj0xNnHJcAL5tZLYCZLTezC81sKtGZRHvPyDiibqhfa2ZNZtYM3A1MS2LsaWFiRQm3X3UKL63fwZceWeKD5c65o9ITieMaYrqaJJWHaRbwZeBHna0b6g2PWbySqOvLdeBdp1Tw6ZnjeWjhJubMX5vqcJxzfVhSE4ekQuAC4OGY4mskrQSWA9XAz8K2FZKe6KAuwDfDpbyLgXOBm5PYhLRy0/nj+ceTh3Pbk8uZ628NdM51kTKh26KystKqqqpSHUavcKChiff/+AVWbd3Dr294ByeNHJDqkJxzvZSkhWZW2brc7xzPMPm52dx9XSVlRXnM/tmLrKnbk+qQnHN9jCeODFReks8vPnIGWYIP/mQB1Tv3pzok51wf4okjQ40dXMTPPzyN+gONfPCeBWzf2+ZV0c45dwRPHBls8ogB/GR2JZt27OdDP3uRXfv9gYjOuY554shwZ4wbxA8+cBrLanbzwXsW+NN0nXMd8sThmHniUH78waksr6nnA/f8nZ37vNvKOdc2TxwOgPMmDOXH101lZe0e/uluH/NwzrXNE4c77NwTyrn7ukrW1O3hfT9+gc1+tZVzLg5PHO4I/3D8EO69fhq1uw7wnh88z4ot9akOyTnXy3jicG8zfdwgfvPxd9BsxlU/ep4X1/m7PJxzb/HE4eI6cXgJD39iBoOL+3HtPQt4dNHmVIfknOslPHG4No0sLeShj89gysiBfPrBRXzrqeU0N6f/s82cc+3zxOHaVVaUxy8/egbXTBvFXc+s4YZfLGTPwcZUh+WcSyFPHK5DeTlZ/O+VJ/HVd03kmRVbufKuv7Gq1gfNnctUnjhcp0jiQ2eO5RfXT2PHvkNc/v2/8fDLm1IdlnMuBTxxuITMOG4wf7zxbE4eOYDP/OZVPvfQq+w/5O8xdy6TJC1xSDpB0qKYz25JN0k6RdIL4S1+j0sqaaP++rDNIklVMeVlkuZKWhWmpclqg4tvaEk+93/0DP71vOP47cJNXHbnX3llw45Uh+Wc6yFJSxxmtsLMppjZFGAqsA94BPgJcIuZnRSW/72d3Zwb9hH7BqpbgHlmNh6YF5ZdD8vJzuKzF57A/R85g4ONzbznh89z+1MrONTYnOrQnHNJ1lNdVTOBNWb2BnACMD+UzwXek+C+ZgH3hvl7gSu6I0DXNTOOG8yfbjqb95w2ku8/s5pZd/2NZTW7Ux2Wcy6JeipxXA08EOaXAJeH+auAUW3UMeBpSQsl3RBTPtTMagDCtDxeZUk3SKqSVFVXV3fUDXBtK8nP5VtXncLd11VSV3+Ad/2/5/j6n5ax75BftutcOpJZcm/okpQHVAOTzKxW0gTgTmAQ8Bhwo5kNilOvwsyqJZUTnZn8q5nNl7TTzAbGbLfDzNod56isrLSqqqr2NnHdZMfeQ9z2p+X8umojIwYW8LXLJ3H+xKGpDss51wWSFrYaKgB65ozjEuBlM6sFMLPlZnahmU0lOgtZE6+SmVWH6VaisZBpYVWtpOEAYbo1yfG7BJQW5fGN957Mbz/+Dor6ZfPR+6r46L1VrNu2N9WhOee6SU8kjmt4q5uKcAaBpCzgy8CPWleQVCSpuGUeuJCoiwuis5TZYX428GjSIndddvqYMv5449l8/uIJPL9mGxfe8Sxfe3ypvyTKuTSQ1MQhqRC4AHg4pvgaSSuB5URdWD8L21ZIeiJsMxR4TtKrwIvAH83sybDuNuACSavCvm9LZhtc1+VmZ/Ev5xzLX/79HN47dST3Pr+ed37zGX7y17UcaPB7P5zrq5I+xtEb+BhH77BiSz3/+8Qynl1Zx9CSfnzinON4/+mjyM/NTnVozrk42hrj8MThetzza7bx3bmreHH9dk8gzvVinjg8cfQqZsYLa988IoF8+MyxXHP6aAYU5qY6POccnjg8cfRSLQnkrmdW87fVb1KYl837Kkdx/ZljGT2oMNXhOZfRPHF44uj1llbv4p7n1vH4q9U0NRsXThzGB6aP5sxjB5OVpVSH51zG8cThiaPP2LLrAPe9sJ5fvbiBnfsaGF1WyNXTRvHeqSMpL85PdXjOZQxPHJ44+pwDDU08tXQLD7y4gb+v3U5Olph5YjlXnjqCc04o98F055KsrcSRk4pgnOuM/NxsZk0ZwawpI1hTt4cHX9zAI69s5qmltRTn53Dp5OHMOrWC6WMHeVeWcz3Izzhcn9LY1Mzza97k94s289SSLew91MSwknwunjyMCycO5fSxZeRm+/vJnOsO3lXliSPt7D/UxLzltTy6qJr5K+s42NjMgIJcZk4o58JJQ3nn8UMozPOTaue6yhOHJ460tu9QI/NXbuPp17cwb9lWdu1vIC87i9PHlnL2+CGcPX4wE4eXIHmXlnOd5YnDE0fGaGhq5qV123lmxVbmr9zGitp6AAb378fZ4wdz1nGDmTa2jJGlBZ5InGuHJw5PHBmrdvcB/rpqG/NX1vHc6m1s3xs9oXf4gHxOH1PG6WPLmDamjPHl/X2Q3bkYnjg8cTigudlYvqWel9Zv58X123lp3Xa21h8EYEBBLqeOHsjJIwdy8ogBnDxyAOUlft+Iy1x+Oa5zQFaWmFhRwsSKEmbPGIOZsWH7Pl5av4OX1m1n0cadzF+5iubwe2pYST4njRzAySMGMHnkACYMK2ZYSb53cbmM5onDZTRJHDOoiGMGFfHeqSOBaKD99erdLN60i8WbdrJ48y7mvl57uE5xfg4nDC3m+GHFTBhWzPFDizlhaDGlRXmpaoZzPcoTh3OtFOblUDmmjMoxZYfL6g808Hr1blZu3cOKLbtZuWUPf3i1ml8taDy8zeD+eYwZVMSYwUWMHVwU5gsZM6iIon7+v5pLH0n71yzpBODXMUXjgK8AzxC9LrY/sB74gJntblV3FHAfMAxoBuaY2ffCuq8C/wzUhc2/aGZP4FwSFefncsa4QZwxbtDhMjNja/1BVmypZ2VtPatq97Duzb3MX1nHQws3HVG/vLgfYwYVMbKsgJEDC6gInxGlBVQMKKAgzx+f4vqOHhkcl5QNbAbOAB4C/s3MnpV0PTDWzP6j1fbDgeFm9nJ49/hC4Aozez0kjj1mdntnj++D466n7TvUyPpt+1j/5l7WbdvL+m17eePNfWzasY8tuw8cHkNpUVaUx4iBBVQMzGdYST7lJfkMKe7HkOJ+lBf3o7w4n7KiPLL9qi/Xg1I9OD4TWGNmb4QzkfmhfC7wFHBE4jCzGqAmzNdLWgaMAF7voXidOyqFeTmHB+Fba2xqZsvuA1TvPMDmnfvCdD+bd+xnTd1enl/zJvUHGt9WLztLDCrKO5xMhhT3o7Qwj9KiPEoLc2Pmo+WBhZ5oXHL0VOK4GnggzC8BLgceBa4CRrVXUdIY4FRgQUzxpyRdB1QBnzWzHXHq3QDcADB69OijDN+57pOTncXI0kJGlhYCZXG3OdDQRF39QbbWHwjTg9F090Hq9kTlr9fsZse+Bg41NsfdhwQl+blRUinKoyQ/l+L8HIrzcynJzzk8f+Q05/B2/fvlkOPP/XJxJL2rSlIeUA1MMrNaSROAO4FBwGPAjWY2qI26/YFngf8xs4dD2VBgG2DAfxF1aV3fXgzeVeXSlZmxv6GJHfsa2LH3EDv2HWL73kPs3NcQpofYHtbtPtBA/YFG6g80sPtAY5sJJ1a/nCwK87IpzMuhIC+bwrxsCnKz316Wl01RXs7h+Zbt8nKy6JeTTb8wjZaz6Jd75HJOlvwS514olV1VlwAvm1ktgJktBy4MQR0PXBavkqRc4HfA/S1JI9SvjdnmbuAPyQvdud5NEoV5ORTm5TBiYEFCdQ82NoVEEiWT2KTSMr//UBP7Dn8a2Xeoif2Hmqjbc5B9h/YdXr//UBOHmjpORG3JEvETS3YWeTlZ5GaLnKwscrJFbnZYzs4iNytMW63PyYq3/u3bZmeJbImsMM3Oems+K4sjyrIUU55AvcPzElmCLAmJPp0oeyJxXMNb3VRIKjezrZKygC8TXWF1BEV/0XuAZWb2nVbrhocxEIAribq+nHMJ6peTTb/+2Qzu369b9tfY1My+hqbDyeRAQxMHG5s52BAllYMNzdFyYxOHGt+abymPtgl1GpvDNtFyY5PR0NTMvgajsSksN0fTxqZmGprjlLe+AqEXap1IBIcTjEJ5VuyUqDxa37JtlICyst7aJkuCsP7r7z6J08fE7xLtqqQmDkmFwAXAx2KKr5H0yTD/MPCzsG0F8BMzuxQ4E/gg8JqkRWHblstuvylpClFX1fpW+3bOpUhOdhYl2VmU5OemOhQg6sZrbLa4SaahsZnGZqPZjKbm6NMyH005oqzJDItT/vb60GRGc6t9tpQ1G5hBc9ifEc23lB8ua9kWi9k+TEPbmpuj9c2hHIvZF2+VFSbhUm9/VpVzzrm42hrj8EsmnHPOJcQTh3POuYR44nDOOZcQTxzOOecS4onDOedcQjxxOOecS4gnDueccwnxxOGccy4hGXEDoKQ64I0uVh9M9FDFTOJtzgze5sxwNG0+xsyGtC7MiMRxNCRVxbtzMp15mzODtzkzJKPN3lXlnHMuIZ44nHPOJcQTR8fmpDqAFPA2ZwZvc2bo9jb7GIdzzrmE+BmHc865hHjicM45lxBPHO2QdLGkFZJWS7ol1fF0F0k/lbRV0pKYsjJJcyWtCtPSmHVfCH+DFZIuSk3UXSdplKRnJC2TtFTSp0N5Orc5X9KLkl4Nbf5aKE/bNreQlC3pFUl/CMtp3WZJ6yW9JmmRpKpQltw2W8srDP1zxAfIBtYA44A84FVgYqrj6qa2vRM4DVgSU/ZN4JYwfwvwjTA/MbS9HzA2/E2yU92GBNs7HDgtzBcDK0O70rnNAvqH+VxgATA9ndsc0/bPAL8C/hCW07rNRK/QHtyqLKlt9jOOtk0DVpvZWjM7BDwIzEpxTN3CzOYD21sVzwLuDfP3AlfElD9oZgfNbB2wmuhv02eYWY2ZvRzm64FlwAjSu81mZnvCYm74GGncZgBJI4HLgJ/EFKd1m9uQ1DZ74mjbCGBjzPKmUJauhppZDURftEB5KE+rv4OkMcCpRL/A07rNoctmEbAVmGtmad9m4LvA54DmmLJ0b7MBT0taKOmGUJbUNuccRbDpTnHKMvHa5bT5O0jqD/wOuMnMdkvxmhZtGqesz7XZzJqAKZIGAo9ImtzO5n2+zZL+EdhqZgslndOZKnHK+lSbgzPNrFpSOTBX0vJ2tu2WNvsZR9s2AaNilkcC1SmKpSfUShoOEKZbQ3la/B0k5RIljfvN7OFQnNZtbmFmO4G/ABeT3m0+E7hc0nqiruXzJP2S9G4zZlYdpluBR4i6npLaZk8cbXsJGC9prKQ84GrgsRTHlEyPAbPD/Gzg0ZjyqyX1kzQWGA+8mIL4ukzRqcU9wDIz+07MqnRu85BwpoGkAuB8YDlp3GYz+4KZjTSzMUT/v/7ZzK4ljdssqUhSccs8cCGwhGS3OdVXBPTmD3Ap0RU4a4AvpTqebmzXA0AN0ED0C+QjwCBgHrAqTMtitv9S+BusAC5JdfxdaO9ZRKfji4FF4XNpmrf5ZOCV0OYlwFdCedq2uVX7z+Gtq6rSts1EV32+Gj5LW76nkt1mf+SIc865hHhXlXPOuYR44nDOOZcQTxzOOecS4onDOedcQjxxOOecS4gnDueOgqSm8FTSlk+3PUVZ0pjYJxg711v4I0ecOzr7zWxKqoNwrif5GYdzSRDekfCN8E6MFyUdF8qPkTRP0uIwHR3Kh0p6JLw/41VJM8KusiXdHd6p8XS4CxxJN0p6PeznwRQ102UoTxzOHZ2CVl1V749Zt9vMpgHfJ3pqK2H+PjM7GbgfuDOU3wk8a2anEL0rZWkoHw/cZWaTgJ3Ae0L5LcCpYT8fT07TnIvP7xx37ihI2mNm/eOUrwfOM7O14QGLW8xskKRtwHAzawjlNWY2WFIdMNLMDsbsYwzR49DHh+XPA7lm9t+SngT2AL8Hfm9vvXvDuaTzMw7nksfamG9rm3gOxsw38da45GXAXcBUYKEkH690PcYTh3PJ8/6Y6Qth/nmiJ7cCfAB4LszPA/4FDr+AqaStnUrKAkaZ2TNELy0aCLztrMe5ZPFfKc4dnYLwlr0WT5pZyyW5/SQtIPqBdk0ouxH4qaR/B+qAD4fyTwNzJH2E6MziX4ieYBxPNvBLSQOIXsxzh0Xv3HCuR/gYh3NJEMY4Ks1sW6pjca67eVeVc865hPgZh3POuYT4GYdzzrmEeOJwzjmXEE8czjnnEuKJwznnXEI8cTjnnEvI/wfANTdVc7pQmgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "............... Testing Backpropagation Algorithm ................\n",
      "Time Cost for Testing algorithm: 0.030917 seconds \n",
      " \n",
      "[[-6.74075271e-05  6.21142682e-05  1.12421430e-04 ...  3.12236118e-04\n",
      "   1.56531847e-05  7.77426505e-06]\n",
      " [-6.76072410e-05  6.22982994e-05  1.12754510e-04 ...  3.13161206e-04\n",
      "   1.56995617e-05  7.79729852e-06]\n",
      " [-3.89309840e-05  3.58738806e-05  6.49286076e-05 ...  1.80330893e-04\n",
      "   9.04044265e-06  4.48999987e-06]\n",
      " ...\n",
      " [ 6.76072755e-05 -6.22983311e-05 -1.12754567e-04 ... -3.13161366e-04\n",
      "  -1.56995697e-05 -7.79730250e-06]\n",
      " [ 6.76072573e-05 -6.22983144e-05 -1.12754537e-04 ... -3.13161281e-04\n",
      "  -1.56995655e-05 -7.79730040e-06]\n",
      " [ 6.75993317e-05 -6.22910112e-05 -1.12741319e-04 ... -3.13124569e-04\n",
      "  -1.56977251e-05 -7.79638633e-06]]\n"
     ]
    }
   ],
   "source": [
    "y = df_house4['SalePrice(log)']\n",
    "'''\n",
    "This Back-propagation algorithm is implemented on Combined Cycle Power Plant UCI Dataset. Link is given below\n",
    "Dataset Link: https://archive.ics.uci.edu/ml/datasets/combined+cycle+power+plant\n",
    "'''\n",
    "print(\"...............Reading the Dataset  and Dataset Pre-Processing ................\")\n",
    "start_time = time.time()\n",
    "\n",
    "#------------------------------------------------------\n",
    "# #dataset = shuffle(pd.read_csv(\"ccip_dataset.csv\"))\n",
    "# dataset = shuffle(ccip_dataset_tst)\n",
    "# dataset = shuffle()\n",
    "\n",
    "# x = dataset[[\"AT\",\"V\",\"AP\",\"RH\"]]\n",
    "# y = dataset[['PE']]\n",
    "#-------------------------------------------------------\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_, y, test_size= 0.2)\n",
    "\n",
    "#--------------------------------------------------------\n",
    "# Normalizing data using Standard Scaler Fit Transform\n",
    "# x_train = pre.StandardScaler().fit_transform(x_train)\n",
    "# x_test = pre.StandardScaler().fit_transform(x_test)\n",
    "#--------------------------------------------------------\n",
    "\n",
    "# Converting pd dataframe to numpy array to match compatibility\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(\"Time Cost for Pre-processing and Reading the Dataset: %f seconds \\n \" % total_time)\n",
    "\n",
    "\n",
    "# Hyperbolic Tangent Activation function\n",
    "def hyperbolic_tanh(x):\n",
    "  return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "\n",
    "# Hyperbolic derivative\n",
    "def derivative_hyperbolic(x):\n",
    "  return 1 - hyperbolic_tanh(x) * hyperbolic_tanh(x)\n",
    "\n",
    "print(\"............... Initializing hyperparameters ................\")\n",
    "start_time = time.time()\n",
    "# Setting Hyperparameters\n",
    "actual_out_size = y_train.size\n",
    "np.random.seed(10)\n",
    "# inp_org : 4\n",
    "inp = 43\n",
    "# hd_org : 6\n",
    "hd = 1\n",
    "\n",
    "out = 1145\n",
    "\n",
    "# 1145,43\n",
    "\n",
    "\n",
    "#print(\"............... Setting 4 weights for hidden layers ................\")\n",
    "print(\"............... Setting 43 weights for hidden layers ................\")\n",
    "w1_l1 = np.random.randn(inp, hd)\n",
    "w2_l2 = np.random.randn(hd, hd)\n",
    "w3_l3 = np.random.randn(hd, hd)\n",
    "w4_l4 = np.random.randn(hd, hd)\n",
    "\n",
    "# more weights should be added ... ?\n",
    "\n",
    "out_w = np.random.randn(hd, out)\n",
    "\n",
    "rmse_list = []\n",
    "\n",
    "epochs = 500\n",
    "eta = 0.0001\n",
    "alpha = 0.7\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(\"Time Cost for Setting HyperParameters: %f seconds \\n \" %total_time)\n",
    "\n",
    "print(\"............... Training Backpropagation Algorithm ................\")\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    # Feedforward for 4 hidden layers by calling activation function\n",
    "    l1 = np.dot(x_train, w1_l1)\n",
    "    l1_out = hyperbolic_tanh(l1)\n",
    "\n",
    "    l2 = np.dot(l1_out, w2_l2)\n",
    "    l2_out = hyperbolic_tanh(l2)\n",
    "\n",
    "    l3 = np.dot(l2_out, w3_l3)\n",
    "    l3_out = hyperbolic_tanh(l3)\n",
    "\n",
    "    l4 = np.dot(l3_out, w4_l4)\n",
    "    l4_out = hyperbolic_tanh(l4)\n",
    "\n",
    "    output = np.dot(l4_out, out_w)\n",
    "    final_out = hyperbolic_tanh(output)\n",
    "\n",
    "    rmse = np.sqrt(np.mean(np.square(final_out - y_train))) /100\n",
    "    rmse_list.append(rmse)\n",
    "\n",
    "    # Backpropagation for 4 hidden layers\n",
    "    final_err = final_out - y_train\n",
    "    final_tanh_derivative = final_err * derivative_hyperbolic(final_out)\n",
    "\n",
    "    l4_err = np.dot(final_tanh_derivative, out_w.T)\n",
    "    l4_derivative = l4_err * derivative_hyperbolic(l4_out)\n",
    "\n",
    "    l3_err = np.dot(l4_derivative, w4_l4.T)\n",
    "    l3_derivative = l3_err * derivative_hyperbolic(l3_out)\n",
    "\n",
    "    l2_err = np.dot(l3_derivative, w3_l3.T)\n",
    "    l2_derivative = l2_err * derivative_hyperbolic(l2_out)\n",
    "\n",
    "    l1_err = np.dot(l2_derivative, w2_l2.T)\n",
    "    l1_derivative = l1_err * derivative_hyperbolic(l1_out)\n",
    "\n",
    "    # Divide weights as per size of output\n",
    "    output_weights = np.dot(l4_out.T, final_tanh_derivative) / actual_out_size\n",
    "    weights4 = np.dot(l3_out.T, l4_derivative) / actual_out_size\n",
    "    weights3 = np.dot(l2_out.T, l3_derivative) / actual_out_size\n",
    "    weights2 = np.dot(l1_out.T, l2_derivative) / actual_out_size\n",
    "    weights1 = np.dot(x_train.T, l1_derivative) / actual_out_size\n",
    "\n",
    "    out_w -= eta * alpha * output_weights\n",
    "    w4_l4 -= eta * alpha * weights4\n",
    "    w3_l3 -= eta * alpha * weights3\n",
    "    w2_l2 -= eta * alpha * weights2\n",
    "    w1_l1 -= eta * alpha * weights1\n",
    "\n",
    "print(\"Training RMSE: \"+str(round(rmse_list[-1],2)))\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(\"Time Cost for Training algorithm: %f seconds \\n \" %total_time)\n",
    "\n",
    "print(\"............... Plotting RMSE Curve ................\")\n",
    "plt.title(\"RMSE Curve\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.plot(rmse_list)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"............... Testing Backpropagation Algorithm ................\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Feedforward for 4 hidden layers by calling activation function\n",
    "l1 = np.dot(x_test, w1_l1)\n",
    "l1_out = hyperbolic_tanh(l1)\n",
    "\n",
    "l2 = np.dot(l1_out, w2_l2)\n",
    "l2_out = hyperbolic_tanh(l2)\n",
    "\n",
    "l3 = np.dot(l2_out, w3_l3)\n",
    "l3_out = hyperbolic_tanh(l3)\n",
    "\n",
    "l4 = np.dot(l3_out, w4_l4)\n",
    "l4_out = hyperbolic_tanh(l4)\n",
    "\n",
    "output = np.dot(l4_out, out_w)\n",
    "final_out = hyperbolic_tanh(output)\n",
    "\n",
    "# Calculate RMSE\n",
    "#rmse = np.sqrt(np.mean(np.square(final_out - y_test))) /100\n",
    "# operands could not be broadcast together with shapes (287,1145) (287,) \n",
    "#print(\"Testing RMSE: \"+str(round(rmse,2)))\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(\"Time Cost for Testing algorithm: %f seconds \\n \" %total_time)\n",
    "\n",
    "print(final_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "c1b25359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6.74075272e-05,  6.21142683e-05,  1.12421430e-04, ...,\n",
       "         3.12236128e-04,  1.56531847e-05,  7.77426505e-06],\n",
       "       [-6.76072411e-05,  6.22982994e-05,  1.12754510e-04, ...,\n",
       "         3.13161216e-04,  1.56995617e-05,  7.79729852e-06],\n",
       "       [-3.89309840e-05,  3.58738806e-05,  6.49286077e-05, ...,\n",
       "         1.80330895e-04,  9.04044265e-06,  4.48999987e-06],\n",
       "       ...,\n",
       "       [ 6.76072756e-05, -6.22983312e-05, -1.12754568e-04, ...,\n",
       "        -3.13161376e-04, -1.56995697e-05, -7.79730250e-06],\n",
       "       [ 6.76072574e-05, -6.22983145e-05, -1.12754538e-04, ...,\n",
       "        -3.13161292e-04, -1.56995655e-05, -7.79730041e-06],\n",
       "       [ 6.75993318e-05, -6.22910113e-05, -1.12741319e-04, ...,\n",
       "        -3.13124580e-04, -1.56977251e-05, -7.79638633e-06]])"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "8fc5de5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6.74075271e-05,  6.21142682e-05,  1.12421430e-04, ...,\n",
       "         3.12236118e-04,  1.56531847e-05,  7.77426505e-06],\n",
       "       [-6.76072410e-05,  6.22982994e-05,  1.12754510e-04, ...,\n",
       "         3.13161206e-04,  1.56995617e-05,  7.79729852e-06],\n",
       "       [-3.89309840e-05,  3.58738806e-05,  6.49286076e-05, ...,\n",
       "         1.80330893e-04,  9.04044265e-06,  4.48999987e-06],\n",
       "       ...,\n",
       "       [ 6.76072755e-05, -6.22983311e-05, -1.12754567e-04, ...,\n",
       "        -3.13161366e-04, -1.56995697e-05, -7.79730250e-06],\n",
       "       [ 6.76072573e-05, -6.22983144e-05, -1.12754537e-04, ...,\n",
       "        -3.13161281e-04, -1.56995655e-05, -7.79730040e-06],\n",
       "       [ 6.75993317e-05, -6.22910112e-05, -1.12741319e-04, ...,\n",
       "        -3.13124569e-04, -1.56977251e-05, -7.79638633e-06]])"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "3f8858f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_out_tst = np.expm1(final_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "6cbbb2d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00674053,  0.00621162,  0.01124277, ...,  0.03122849,\n",
       "         0.00156533,  0.00077743],\n",
       "       [-0.0067605 ,  0.00623002,  0.01127609, ...,  0.03132102,\n",
       "         0.00156997,  0.00077973],\n",
       "       [-0.00389302,  0.00358745,  0.00649307, ...,  0.01803472,\n",
       "         0.00090405,  0.000449  ],\n",
       "       ...,\n",
       "       [ 0.00676096, -0.00622964, -0.01127482, ..., -0.03131123,\n",
       "        -0.00156994, -0.00077973],\n",
       "       [ 0.00676095, -0.00622964, -0.01127482, ..., -0.03131123,\n",
       "        -0.00156994, -0.00077973],\n",
       "       [ 0.00676016, -0.00622891, -0.0112735 , ..., -0.03130756,\n",
       "        -0.00156976, -0.00077964]])"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_out_tst * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "ff0d1fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(287, 1145)"
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_out_tst.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a7b69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_from_model = cat_fitter.predict(df_house_test_extracted2b_predicted)\n",
    "# df_house_test['SalePrice'] = 0\n",
    "# df_house_test['SalePrice'] = np.expm1(result_from_model)\n",
    "# df_house_test[['Id','SalePrice']].to_csv('C:/Users/thsong/submission14_5.csv',index=False)\n",
    "# df_house_test[['Id','SalePrice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "c562891a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.3315865 ,  0.71527897, -1.54540029, -0.00838385,  0.62133597,\n",
       "        -0.72008556],\n",
       "       [ 0.26551159,  0.10854853,  0.00429143, -0.17460021,  0.43302619,\n",
       "         1.20303737],\n",
       "       [-0.96506567,  1.02827408,  0.22863013,  0.44513761, -1.13660221,\n",
       "         0.13513688],\n",
       "       [ 1.484537  , -1.07980489, -1.97772828, -1.7433723 ,  0.26607016,\n",
       "         2.38496733]])"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "21f7cb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 6)"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1_l1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "405219fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.64599805, -1.06620658,  0.89826577, -0.03662299],\n",
       "       [ 0.14944118,  1.1306458 , -0.33490044,  0.93007065],\n",
       "       [ 1.02294382,  1.09206308, -0.04563923, -1.29195353],\n",
       "       ...,\n",
       "       [ 0.05118895, -1.05990736, -1.70508511,  0.96092258],\n",
       "       [ 0.48322955, -0.38982802, -1.66279546,  0.42752708],\n",
       "       [ 0.88296805,  0.36292857,  0.21824819, -1.52574256]])"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "e90e1416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7654, 4)"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "e78ffdb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.99058303,  0.99167614, -1.60253819,  0.54042557,  0.45175727,\n",
       "        -1.56532083],\n",
       "       [-0.04019649,  0.28554214, -0.16983316,  0.12809066,  0.29508916,\n",
       "         0.66445406],\n",
       "       [-0.48355589,  0.58814112,  0.14172206, -0.2307429 , -0.8961773 ,\n",
       "         0.69029846],\n",
       "       [ 1.53744184, -1.22586982, -1.85533497, -2.08060056,  0.30204044,\n",
       "         3.03023219]])"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "956e83d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 6)"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1_l1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd90b11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7214808b",
   "metadata": {},
   "source": [
    "### nov10.2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "2d4222e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "115/115 [==============================] - 1s 2ms/step - loss: 31.5118\n",
      "Epoch 2/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 2.2381\n",
      "Epoch 3/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 1.4547\n",
      "Epoch 4/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 1.1548\n",
      "Epoch 5/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.9663\n",
      "Epoch 6/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.8318\n",
      "Epoch 7/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.7231\n",
      "Epoch 8/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.6327\n",
      "Epoch 9/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.5776\n",
      "Epoch 10/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.5162\n",
      "Epoch 11/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.4591\n",
      "Epoch 12/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.4106\n",
      "Epoch 13/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.4021\n",
      "Epoch 14/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.3413\n",
      "Epoch 15/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.3242\n",
      "Epoch 16/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.2995\n",
      "Epoch 17/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.2796\n",
      "Epoch 18/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.2702\n",
      "Epoch 19/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.2474\n",
      "Epoch 20/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2260\n",
      "Epoch 21/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.2120\n",
      "Epoch 22/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1945\n",
      "Epoch 23/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1809\n",
      "Epoch 24/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1634\n",
      "Epoch 25/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1638\n",
      "Epoch 26/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1401\n",
      "Epoch 27/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1376\n",
      "Epoch 28/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1318\n",
      "Epoch 29/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1206\n",
      "Epoch 30/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1202\n",
      "Epoch 31/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1189\n",
      "Epoch 32/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1076\n",
      "Epoch 33/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1044\n",
      "Epoch 34/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0942\n",
      "Epoch 35/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0939\n",
      "Epoch 36/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0950\n",
      "Epoch 37/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0950\n",
      "Epoch 38/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0882\n",
      "Epoch 39/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0810\n",
      "Epoch 40/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0768\n",
      "Epoch 41/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0765\n",
      "Epoch 42/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0773\n",
      "Epoch 43/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0654\n",
      "Epoch 44/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0783\n",
      "Epoch 45/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0776\n",
      "Epoch 46/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0651\n",
      "Epoch 47/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0605\n",
      "Epoch 48/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0604\n",
      "Epoch 49/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0702\n",
      "Epoch 50/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0624\n",
      "Epoch 51/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0575\n",
      "Epoch 52/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0527\n",
      "Epoch 53/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0484\n",
      "Epoch 54/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0533\n",
      "Epoch 55/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0462\n",
      "Epoch 56/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0550\n",
      "Epoch 57/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0571\n",
      "Epoch 58/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0672\n",
      "Epoch 59/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0552\n",
      "Epoch 60/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0560\n",
      "Epoch 61/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0470\n",
      "Epoch 62/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0618\n",
      "Epoch 63/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0711\n",
      "Epoch 64/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0625\n",
      "Epoch 65/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0490\n",
      "Epoch 66/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0493\n",
      "Epoch 67/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0439\n",
      "Epoch 68/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0509\n",
      "Epoch 69/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0411\n",
      "Epoch 70/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0406\n",
      "Epoch 71/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0477\n",
      "Epoch 72/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0383\n",
      "Epoch 73/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0376\n",
      "Epoch 74/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0586\n",
      "Epoch 75/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0689\n",
      "Epoch 76/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0608\n",
      "Epoch 77/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0359\n",
      "Epoch 78/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0462\n",
      "Epoch 79/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0477\n",
      "Epoch 80/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0585\n",
      "Epoch 81/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0458\n",
      "Epoch 82/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0427\n",
      "Epoch 83/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0395\n",
      "Epoch 84/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0387\n",
      "Epoch 85/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0478\n",
      "Epoch 86/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0415\n",
      "Epoch 87/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0489\n",
      "Epoch 88/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0413\n",
      "Epoch 89/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0320\n",
      "Epoch 90/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0340\n",
      "Epoch 91/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0414\n",
      "Epoch 92/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0329\n",
      "Epoch 93/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0346\n",
      "Epoch 94/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0462\n",
      "Epoch 95/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0427\n",
      "Epoch 96/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0398\n",
      "Epoch 97/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0367\n",
      "Epoch 98/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0311\n",
      "Epoch 99/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0465\n",
      "Epoch 100/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0719\n",
      "Epoch 101/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0538\n",
      "Epoch 102/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0326\n",
      "Epoch 103/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0359\n",
      "Epoch 104/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0270\n",
      "Epoch 105/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0315\n",
      "Epoch 106/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0294\n",
      "Epoch 107/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0380\n",
      "Epoch 108/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0394\n",
      "Epoch 109/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0362\n",
      "Epoch 110/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0315\n",
      "Epoch 111/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0723\n",
      "Epoch 112/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0619\n",
      "Epoch 113/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0444\n",
      "Epoch 114/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0300\n",
      "Epoch 115/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0248\n",
      "Epoch 116/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0274\n",
      "Epoch 117/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0264\n",
      "Epoch 118/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0251\n",
      "Epoch 119/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0239\n",
      "Epoch 120/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0410\n",
      "Epoch 121/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0344\n",
      "Epoch 122/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0425\n",
      "Epoch 123/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0345\n",
      "Epoch 124/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0442\n",
      "Epoch 125/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0838\n",
      "Epoch 126/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0529\n",
      "Epoch 127/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0297\n",
      "Epoch 128/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0257\n",
      "Epoch 129/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0234\n",
      "Epoch 130/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0284\n",
      "Epoch 131/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0248\n",
      "Epoch 132/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0323\n",
      "Epoch 133/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0298\n",
      "Epoch 134/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0289\n",
      "Epoch 135/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0241\n",
      "Epoch 136/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0221\n",
      "Epoch 137/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0482\n",
      "Epoch 138/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0334\n",
      "Epoch 139/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0319\n",
      "Epoch 140/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0516\n",
      "Epoch 141/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0450\n",
      "Epoch 142/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0309\n",
      "Epoch 143/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0245\n",
      "Epoch 144/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0289\n",
      "Epoch 145/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0246\n",
      "Epoch 146/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0212\n",
      "Epoch 147/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0208\n",
      "Epoch 148/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0224\n",
      "Epoch 149/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0233\n",
      "Epoch 150/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0395\n",
      "Epoch 151/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0307\n",
      "Epoch 152/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0466\n",
      "Epoch 153/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0380\n",
      "Epoch 154/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0268\n",
      "Epoch 155/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0230\n",
      "Epoch 156/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0289\n",
      "Epoch 157/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0256\n",
      "Epoch 158/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0237\n",
      "Epoch 159/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0198\n",
      "Epoch 160/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0403\n",
      "Epoch 161/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0506\n",
      "Epoch 162/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0547\n",
      "Epoch 163/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0452\n",
      "Epoch 164/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0288\n",
      "Epoch 165/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0243\n",
      "Epoch 166/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0212\n",
      "Epoch 167/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0210\n",
      "Epoch 168/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0167\n",
      "Epoch 169/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0203\n",
      "Epoch 170/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0255\n",
      "Epoch 171/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0168\n",
      "Epoch 172/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0189\n",
      "Epoch 173/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0219\n",
      "Epoch 174/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0221\n",
      "Epoch 175/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0271\n",
      "Epoch 176/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0271\n",
      "Epoch 177/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0261\n",
      "Epoch 178/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0307\n",
      "Epoch 179/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0288\n",
      "Epoch 180/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0299\n",
      "Epoch 181/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0307\n",
      "Epoch 182/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0216\n",
      "Epoch 183/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0211\n",
      "Epoch 184/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0246\n",
      "Epoch 185/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0294\n",
      "Epoch 186/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0328\n",
      "Epoch 187/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0368\n",
      "Epoch 188/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0256\n",
      "Epoch 189/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0295\n",
      "Epoch 190/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0292\n",
      "Epoch 191/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0296\n",
      "Epoch 192/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0264\n",
      "Epoch 193/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0205\n",
      "Epoch 194/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0194\n",
      "Epoch 195/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0200\n",
      "Epoch 196/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0287\n",
      "Epoch 197/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0237\n",
      "Epoch 198/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0170\n",
      "Epoch 199/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0256\n",
      "Epoch 200/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24765fa1070>"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ = Sequential()\n",
    "#----------------------\n",
    "# 30, 13, 6\n",
    "model_.add(Dense(90, input_dim=43, activation='relu'))\n",
    "model_.add(Dense(21, activation='relu'))\n",
    "model_.add(Dense(1))\n",
    "#-----------------------------------------------------------\n",
    "model_.compile(loss=tf.keras.losses.MeanSquaredError(),optimizer='adam')\n",
    "#---------------------------------------------------------\n",
    "model_.fit(X_train,y_train, epochs=200, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "72f34bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 1ms/step - loss: 0.5497\n",
      "Test Accuracy : 0.549706220626831\n"
     ]
    }
   ],
   "source": [
    "print('Test Accuracy : {}'.format(model_.evaluate(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "449f1e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>171756.968750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>170096.406250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>131561.234375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>234386.015625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>196595.859375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2915</td>\n",
       "      <td>168828.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2916</td>\n",
       "      <td>106568.679688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2917</td>\n",
       "      <td>243363.859375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2918</td>\n",
       "      <td>213592.421875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2919</td>\n",
       "      <td>313649.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id      SalePrice\n",
       "0     1461  171756.968750\n",
       "1     1462  170096.406250\n",
       "2     1463  131561.234375\n",
       "3     1464  234386.015625\n",
       "4     1465  196595.859375\n",
       "...    ...            ...\n",
       "1454  2915  168828.562500\n",
       "1455  2916  106568.679688\n",
       "1456  2917  243363.859375\n",
       "1457  2918  213592.421875\n",
       "1458  2919  313649.750000\n",
       "\n",
       "[1459 rows x 2 columns]"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_from_model = model_.predict(df_house_test_extracted2b_predicted)\n",
    "df_house_test['SalePrice'] = 0\n",
    "df_house_test['SalePrice'] = np.expm1(result_from_model)\n",
    "df_house_test[['Id','SalePrice']].to_csv('C:/Users/thsong/submission15_3.csv',index=False)\n",
    "df_house_test[['Id','SalePrice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "2a7d2be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "115/115 [==============================] - 1s 1ms/step - loss: 35.6898 - rmse: 5.9741\n",
      "Epoch 2/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 2.5811 - rmse: 1.6066\n",
      "Epoch 3/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 1.6675 - rmse: 1.2913\n",
      "Epoch 4/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 1.2578 - rmse: 1.1215\n",
      "Epoch 5/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 1.0084 - rmse: 1.0042\n",
      "Epoch 6/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.8479 - rmse: 0.9208\n",
      "Epoch 7/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.7168 - rmse: 0.8467\n",
      "Epoch 8/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.6164 - rmse: 0.7851\n",
      "Epoch 9/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.5437 - rmse: 0.7374\n",
      "Epoch 10/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.4904 - rmse: 0.7003\n",
      "Epoch 11/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.4342 - rmse: 0.6590\n",
      "Epoch 12/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.3891 - rmse: 0.6237\n",
      "Epoch 13/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.3651 - rmse: 0.6043\n",
      "Epoch 14/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.3238 - rmse: 0.5691\n",
      "Epoch 15/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2951 - rmse: 0.5433\n",
      "Epoch 16/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2757 - rmse: 0.5250\n",
      "Epoch 17/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2570 - rmse: 0.5069\n",
      "Epoch 18/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2372 - rmse: 0.4870\n",
      "Epoch 19/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2303 - rmse: 0.4799\n",
      "Epoch 20/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2064 - rmse: 0.4543\n",
      "Epoch 21/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2001 - rmse: 0.4473\n",
      "Epoch 22/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1834 - rmse: 0.4283\n",
      "Epoch 23/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1786 - rmse: 0.4226\n",
      "Epoch 24/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1575 - rmse: 0.3968\n",
      "Epoch 25/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1521 - rmse: 0.3900\n",
      "Epoch 26/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1314 - rmse: 0.3624\n",
      "Epoch 27/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1360 - rmse: 0.3688\n",
      "Epoch 28/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1260 - rmse: 0.3550\n",
      "Epoch 29/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1155 - rmse: 0.3398\n",
      "Epoch 30/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1046 - rmse: 0.3234\n",
      "Epoch 31/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1067 - rmse: 0.3267\n",
      "Epoch 32/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1022 - rmse: 0.3196\n",
      "Epoch 33/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1031 - rmse: 0.3211\n",
      "Epoch 34/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0935 - rmse: 0.3058\n",
      "Epoch 35/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0966 - rmse: 0.3107\n",
      "Epoch 36/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0801 - rmse: 0.2830\n",
      "Epoch 37/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0806 - rmse: 0.2839\n",
      "Epoch 38/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0822 - rmse: 0.2867\n",
      "Epoch 39/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0700 - rmse: 0.2645\n",
      "Epoch 40/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0636 - rmse: 0.2522\n",
      "Epoch 41/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0702 - rmse: 0.2650\n",
      "Epoch 42/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0644 - rmse: 0.2538\n",
      "Epoch 43/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0684 - rmse: 0.2615\n",
      "Epoch 44/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0663 - rmse: 0.2575\n",
      "Epoch 45/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0664 - rmse: 0.2576\n",
      "Epoch 46/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0644 - rmse: 0.2537\n",
      "Epoch 47/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0515 - rmse: 0.2268\n",
      "Epoch 48/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0542 - rmse: 0.2328\n",
      "Epoch 49/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0710 - rmse: 0.2664\n",
      "Epoch 50/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0644 - rmse: 0.2538\n",
      "Epoch 51/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0476 - rmse: 0.2181\n",
      "Epoch 52/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0506 - rmse: 0.2251\n",
      "Epoch 53/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0469 - rmse: 0.2167\n",
      "Epoch 54/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0542 - rmse: 0.2328\n",
      "Epoch 55/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0530 - rmse: 0.2303\n",
      "Epoch 56/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0574 - rmse: 0.2396\n",
      "Epoch 57/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0598 - rmse: 0.2445\n",
      "Epoch 58/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0423 - rmse: 0.2056\n",
      "Epoch 59/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0484 - rmse: 0.2200\n",
      "Epoch 60/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0583 - rmse: 0.2415\n",
      "Epoch 61/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0546 - rmse: 0.2338\n",
      "Epoch 62/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0419 - rmse: 0.2047\n",
      "Epoch 63/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0549 - rmse: 0.2343\n",
      "Epoch 64/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0502 - rmse: 0.2241\n",
      "Epoch 65/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0455 - rmse: 0.2134\n",
      "Epoch 66/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0412 - rmse: 0.2030\n",
      "Epoch 67/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0424 - rmse: 0.2059\n",
      "Epoch 68/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0757 - rmse: 0.2752\n",
      "Epoch 69/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0467 - rmse: 0.2162\n",
      "Epoch 70/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0415 - rmse: 0.2037\n",
      "Epoch 71/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0370 - rmse: 0.1923\n",
      "Epoch 72/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0384 - rmse: 0.1959\n",
      "Epoch 73/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0372 - rmse: 0.1929\n",
      "Epoch 74/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0484 - rmse: 0.2200\n",
      "Epoch 75/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0510 - rmse: 0.2258\n",
      "Epoch 76/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0580 - rmse: 0.2409\n",
      "Epoch 77/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0434 - rmse: 0.2084\n",
      "Epoch 78/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0299 - rmse: 0.1729\n",
      "Epoch 79/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0306 - rmse: 0.1750\n",
      "Epoch 80/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0259 - rmse: 0.1611\n",
      "Epoch 81/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0350 - rmse: 0.1872\n",
      "Epoch 82/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0373 - rmse: 0.1932\n",
      "Epoch 83/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0356 - rmse: 0.1887\n",
      "Epoch 84/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0435 - rmse: 0.2086\n",
      "Epoch 85/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0421 - rmse: 0.2052\n",
      "Epoch 86/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0548 - rmse: 0.2342\n",
      "Epoch 87/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0564 - rmse: 0.2376\n",
      "Epoch 88/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0404 - rmse: 0.2009\n",
      "Epoch 89/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0350 - rmse: 0.1871\n",
      "Epoch 90/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0350 - rmse: 0.1872\n",
      "Epoch 91/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0349 - rmse: 0.1867\n",
      "Epoch 92/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0289 - rmse: 0.1700\n",
      "Epoch 93/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0440 - rmse: 0.2098\n",
      "Epoch 94/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0510 - rmse: 0.2257\n",
      "Epoch 95/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0523 - rmse: 0.2286\n",
      "Epoch 96/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0524 - rmse: 0.2289\n",
      "Epoch 97/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0346 - rmse: 0.1861\n",
      "Epoch 98/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0247 - rmse: 0.1572\n",
      "Epoch 99/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0433 - rmse: 0.2080\n",
      "Epoch 100/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0444 - rmse: 0.2106\n",
      "Epoch 101/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0444 - rmse: 0.2108\n",
      "Epoch 102/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0460 - rmse: 0.2144\n",
      "Epoch 103/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0390 - rmse: 0.1976\n",
      "Epoch 104/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0361 - rmse: 0.1899\n",
      "Epoch 105/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0255 - rmse: 0.1598\n",
      "Epoch 106/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0250 - rmse: 0.1580\n",
      "Epoch 107/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0266 - rmse: 0.1631\n",
      "Epoch 108/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0231 - rmse: 0.1520\n",
      "Epoch 109/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0202 - rmse: 0.1421\n",
      "Epoch 110/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0253 - rmse: 0.1589\n",
      "Epoch 111/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0417 - rmse: 0.2042\n",
      "Epoch 112/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0437 - rmse: 0.2091\n",
      "Epoch 113/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0548 - rmse: 0.2341\n",
      "Epoch 114/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0479 - rmse: 0.2188\n",
      "Epoch 115/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0422 - rmse: 0.2055\n",
      "Epoch 116/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0372 - rmse: 0.1930\n",
      "Epoch 117/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0301 - rmse: 0.1736\n",
      "Epoch 118/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0226 - rmse: 0.1505\n",
      "Epoch 119/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0206 - rmse: 0.1435\n",
      "Epoch 120/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0219 - rmse: 0.1479\n",
      "Epoch 121/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0204 - rmse: 0.1429\n",
      "Epoch 122/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0305 - rmse: 0.1747\n",
      "Epoch 123/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0541 - rmse: 0.2327\n",
      "Epoch 124/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0499 - rmse: 0.2234\n",
      "Epoch 125/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0714 - rmse: 0.2673\n",
      "Epoch 126/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0451 - rmse: 0.2124\n",
      "Epoch 127/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0347 - rmse: 0.1864\n",
      "Epoch 128/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0284 - rmse: 0.1685\n",
      "Epoch 129/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0354 - rmse: 0.1881\n",
      "Epoch 130/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0264 - rmse: 0.1624\n",
      "Epoch 131/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0304 - rmse: 0.1745\n",
      "Epoch 132/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0270 - rmse: 0.1642\n",
      "Epoch 133/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0256 - rmse: 0.1600\n",
      "Epoch 134/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0256 - rmse: 0.1598\n",
      "Epoch 135/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0255 - rmse: 0.1597\n",
      "Epoch 136/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0251 - rmse: 0.1585\n",
      "Epoch 137/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0332 - rmse: 0.1823\n",
      "Epoch 138/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0285 - rmse: 0.1687\n",
      "Epoch 139/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0238 - rmse: 0.1542\n",
      "Epoch 140/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0297 - rmse: 0.1722\n",
      "Epoch 141/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0349 - rmse: 0.1869\n",
      "Epoch 142/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0393 - rmse: 0.1982\n",
      "Epoch 143/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0392 - rmse: 0.1981\n",
      "Epoch 144/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0473 - rmse: 0.2176\n",
      "Epoch 145/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0414 - rmse: 0.2035\n",
      "Epoch 146/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0525 - rmse: 0.2291\n",
      "Epoch 147/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0486 - rmse: 0.2205\n",
      "Epoch 148/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0389 - rmse: 0.1973\n",
      "Epoch 149/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0253 - rmse: 0.1590\n",
      "Epoch 150/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0244 - rmse: 0.1562\n",
      "Epoch 151/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0185 - rmse: 0.1360\n",
      "Epoch 152/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0210 - rmse: 0.1448\n",
      "Epoch 153/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0168 - rmse: 0.1297\n",
      "Epoch 154/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0146 - rmse: 0.1207\n",
      "Epoch 155/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0138 - rmse: 0.1176\n",
      "Epoch 156/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0166 - rmse: 0.1290\n",
      "Epoch 157/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0251 - rmse: 0.1584\n",
      "Epoch 158/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0445 - rmse: 0.2109\n",
      "Epoch 159/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0334 - rmse: 0.1827\n",
      "Epoch 160/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0254 - rmse: 0.1593\n",
      "Epoch 161/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0257 - rmse: 0.1604\n",
      "Epoch 162/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0389 - rmse: 0.1973\n",
      "Epoch 163/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0322 - rmse: 0.1793\n",
      "Epoch 164/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0238 - rmse: 0.1542\n",
      "Epoch 165/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0280 - rmse: 0.1673\n",
      "Epoch 166/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0226 - rmse: 0.1504\n",
      "Epoch 167/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0329 - rmse: 0.1813\n",
      "Epoch 168/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0319 - rmse: 0.1786\n",
      "Epoch 169/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0431 - rmse: 0.2077\n",
      "Epoch 170/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0420 - rmse: 0.2049\n",
      "Epoch 171/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0242 - rmse: 0.1556\n",
      "Epoch 172/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0200 - rmse: 0.1414\n",
      "Epoch 173/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0248 - rmse: 0.1576\n",
      "Epoch 174/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0177 - rmse: 0.1332\n",
      "Epoch 175/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0145 - rmse: 0.1202\n",
      "Epoch 176/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0167 - rmse: 0.1291\n",
      "Epoch 177/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0187 - rmse: 0.1367\n",
      "Epoch 178/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0225 - rmse: 0.1500\n",
      "Epoch 179/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0337 - rmse: 0.1836\n",
      "Epoch 180/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0374 - rmse: 0.1935\n",
      "Epoch 181/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0392 - rmse: 0.1981\n",
      "Epoch 182/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0374 - rmse: 0.1933\n",
      "Epoch 183/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0316 - rmse: 0.1777\n",
      "Epoch 184/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0385 - rmse: 0.1962\n",
      "Epoch 185/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0284 - rmse: 0.1685\n",
      "Epoch 186/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0202 - rmse: 0.1421\n",
      "Epoch 187/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0216 - rmse: 0.1469\n",
      "Epoch 188/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0270 - rmse: 0.1642\n",
      "Epoch 189/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0280 - rmse: 0.1672\n",
      "Epoch 190/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0184 - rmse: 0.1356\n",
      "Epoch 191/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0169 - rmse: 0.1299\n",
      "Epoch 192/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0135 - rmse: 0.1160\n",
      "Epoch 193/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0141 - rmse: 0.1188\n",
      "Epoch 194/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0178 - rmse: 0.1336\n",
      "Epoch 195/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0286 - rmse: 0.1691\n",
      "Epoch 196/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0267 - rmse: 0.1635\n",
      "Epoch 197/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0259 - rmse: 0.1610\n",
      "Epoch 198/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0217 - rmse: 0.1473\n",
      "Epoch 199/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0260 - rmse: 0.1612\n",
      "Epoch 200/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0311 - rmse: 0.1764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2477dced8e0>"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model__ = Sequential()\n",
    "#----------------------\n",
    "# 30, 13, 6\n",
    "model__.add(Dense(90, input_dim=43, activation='relu'))\n",
    "model__.add(Dense(21, activation='relu'))\n",
    "model__.add(Dense(1))\n",
    "#-----------------------------------------------------------\n",
    "model__.compile(loss=tf.keras.losses.MeanSquaredError(),optimizer='adam', metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n",
    "#---------------------------------------------------------\n",
    "model__.fit(X_train,y_train, epochs=200, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "df18875b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 1ms/step - loss: 0.5356 - rmse: 0.7319\n",
      "Test Accuracy : [0.5356471538543701, 0.7318791747093201]\n"
     ]
    }
   ],
   "source": [
    "print('Test Accuracy : {}'.format(model__.evaluate(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "d32633f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>238821.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>144618.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>164692.390625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>262058.265625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>127007.515625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2915</td>\n",
       "      <td>109849.789062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2916</td>\n",
       "      <td>299706.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2917</td>\n",
       "      <td>893266.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2918</td>\n",
       "      <td>295952.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2919</td>\n",
       "      <td>282190.875000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id      SalePrice\n",
       "0     1461  238821.812500\n",
       "1     1462  144618.812500\n",
       "2     1463  164692.390625\n",
       "3     1464  262058.265625\n",
       "4     1465  127007.515625\n",
       "...    ...            ...\n",
       "1454  2915  109849.789062\n",
       "1455  2916  299706.500000\n",
       "1456  2917  893266.625000\n",
       "1457  2918  295952.500000\n",
       "1458  2919  282190.875000\n",
       "\n",
       "[1459 rows x 2 columns]"
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_from_model = model__.predict(df_house_test_extracted2b_predicted)\n",
    "df_house_test['SalePrice'] = 0\n",
    "df_house_test['SalePrice'] = np.expm1(result_from_model)\n",
    "df_house_test[['Id','SalePrice']].to_csv('C:/Users/thsong/submission15_3_2.csv',index=False)\n",
    "df_house_test[['Id','SalePrice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "0d3803ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "115/115 [==============================] - 1s 1ms/step - loss: 42.6791 - accuracy: 0.0000e+00\n",
      "Epoch 2/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 2.4625 - accuracy: 0.0000e+00\n",
      "Epoch 3/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 1.5809 - accuracy: 0.0000e+00\n",
      "Epoch 4/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 1.2519 - accuracy: 0.0000e+00\n",
      "Epoch 5/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 1.0367 - accuracy: 0.0000e+00\n",
      "Epoch 6/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.9020 - accuracy: 0.0000e+00\n",
      "Epoch 7/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.7679 - accuracy: 0.0000e+00\n",
      "Epoch 8/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6722 - accuracy: 0.0000e+00\n",
      "Epoch 9/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6094 - accuracy: 0.0000e+00\n",
      "Epoch 10/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.5363 - accuracy: 0.0000e+00\n",
      "Epoch 11/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.4831 - accuracy: 0.0000e+00\n",
      "Epoch 12/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.4403 - accuracy: 0.0000e+00\n",
      "Epoch 13/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.4228 - accuracy: 0.0000e+00\n",
      "Epoch 14/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.3631 - accuracy: 0.0000e+00\n",
      "Epoch 15/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.3416 - accuracy: 0.0000e+00\n",
      "Epoch 16/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.3145 - accuracy: 0.0000e+00\n",
      "Epoch 17/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.2888 - accuracy: 0.0000e+00\n",
      "Epoch 18/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.2691 - accuracy: 0.0000e+00\n",
      "Epoch 19/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.2573 - accuracy: 0.0000e+00\n",
      "Epoch 20/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.2333 - accuracy: 0.0000e+00\n",
      "Epoch 21/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2133 - accuracy: 0.0000e+00\n",
      "Epoch 22/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2075 - accuracy: 0.0000e+00\n",
      "Epoch 23/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1935 - accuracy: 0.0000e+00\n",
      "Epoch 24/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1724 - accuracy: 0.0000e+00\n",
      "Epoch 25/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1711 - accuracy: 0.0000e+00\n",
      "Epoch 26/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1465 - accuracy: 0.0000e+00\n",
      "Epoch 27/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1443 - accuracy: 0.0000e+00\n",
      "Epoch 28/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1420 - accuracy: 0.0000e+00\n",
      "Epoch 29/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1252 - accuracy: 0.0000e+00\n",
      "Epoch 30/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1182 - accuracy: 0.0000e+00\n",
      "Epoch 31/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1136 - accuracy: 0.0000e+00\n",
      "Epoch 32/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1028 - accuracy: 0.0000e+00\n",
      "Epoch 33/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1101 - accuracy: 0.0000e+00\n",
      "Epoch 34/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0960 - accuracy: 0.0000e+00\n",
      "Epoch 35/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0942 - accuracy: 0.0000e+00\n",
      "Epoch 36/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0944 - accuracy: 0.0000e+00\n",
      "Epoch 37/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0899 - accuracy: 0.0000e+00\n",
      "Epoch 38/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0792 - accuracy: 0.0000e+00\n",
      "Epoch 39/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0733 - accuracy: 0.0000e+00\n",
      "Epoch 40/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0760 - accuracy: 0.0000e+00\n",
      "Epoch 41/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0777 - accuracy: 0.0000e+00\n",
      "Epoch 42/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0796 - accuracy: 0.0000e+00\n",
      "Epoch 43/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0756 - accuracy: 0.0000e+00\n",
      "Epoch 44/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0754 - accuracy: 0.0000e+00\n",
      "Epoch 45/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0668 - accuracy: 0.0000e+00\n",
      "Epoch 46/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0626 - accuracy: 0.0000e+00\n",
      "Epoch 47/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0578 - accuracy: 0.0000e+00\n",
      "Epoch 48/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0543 - accuracy: 0.0000e+00\n",
      "Epoch 49/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0474 - accuracy: 0.0000e+00\n",
      "Epoch 50/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0547 - accuracy: 0.0000e+00\n",
      "Epoch 51/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0550 - accuracy: 0.0000e+00\n",
      "Epoch 52/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0620 - accuracy: 0.0000e+00\n",
      "Epoch 53/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0549 - accuracy: 0.0000e+00\n",
      "Epoch 54/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0452 - accuracy: 0.0000e+00\n",
      "Epoch 55/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0466 - accuracy: 0.0000e+00\n",
      "Epoch 56/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0374 - accuracy: 0.0000e+00\n",
      "Epoch 57/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0460 - accuracy: 0.0000e+00\n",
      "Epoch 58/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0482 - accuracy: 0.0000e+00\n",
      "Epoch 59/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0486 - accuracy: 0.0000e+00\n",
      "Epoch 60/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0527 - accuracy: 0.0000e+00\n",
      "Epoch 61/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0486 - accuracy: 0.0000e+00\n",
      "Epoch 62/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0320 - accuracy: 0.0000e+00\n",
      "Epoch 63/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0367 - accuracy: 0.0000e+00\n",
      "Epoch 64/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0500 - accuracy: 0.0000e+00\n",
      "Epoch 65/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0353 - accuracy: 0.0000e+00\n",
      "Epoch 66/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0341 - accuracy: 0.0000e+00\n",
      "Epoch 67/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0342 - accuracy: 0.0000e+00\n",
      "Epoch 68/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0432 - accuracy: 0.0000e+00\n",
      "Epoch 69/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0354 - accuracy: 0.0000e+00\n",
      "Epoch 70/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0517 - accuracy: 0.0000e+00\n",
      "Epoch 71/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0513 - accuracy: 0.0000e+00\n",
      "Epoch 72/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0431 - accuracy: 0.0000e+00\n",
      "Epoch 73/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0427 - accuracy: 0.0000e+00\n",
      "Epoch 74/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0682 - accuracy: 0.0000e+00\n",
      "Epoch 75/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0517 - accuracy: 0.0000e+00\n",
      "Epoch 76/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0440 - accuracy: 0.0000e+00\n",
      "Epoch 77/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0344 - accuracy: 0.0000e+00\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0349 - accuracy: 0.0000e+00\n",
      "Epoch 79/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0356 - accuracy: 0.0000e+00\n",
      "Epoch 80/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0378 - accuracy: 0.0000e+00\n",
      "Epoch 81/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0350 - accuracy: 0.0000e+00\n",
      "Epoch 82/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0293 - accuracy: 0.0000e+00\n",
      "Epoch 83/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0363 - accuracy: 0.0000e+00\n",
      "Epoch 84/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0563 - accuracy: 0.0000e+00\n",
      "Epoch 85/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0545 - accuracy: 0.0000e+00\n",
      "Epoch 86/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0464 - accuracy: 0.0000e+00\n",
      "Epoch 87/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0430 - accuracy: 0.0000e+00\n",
      "Epoch 88/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0359 - accuracy: 0.0000e+00\n",
      "Epoch 89/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0257 - accuracy: 0.0000e+00\n",
      "Epoch 90/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0247 - accuracy: 0.0000e+00\n",
      "Epoch 91/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0445 - accuracy: 0.0000e+00\n",
      "Epoch 92/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0415 - accuracy: 0.0000e+00\n",
      "Epoch 93/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0372 - accuracy: 0.0000e+00\n",
      "Epoch 94/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0392 - accuracy: 0.0000e+00\n",
      "Epoch 95/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0327 - accuracy: 0.0000e+00\n",
      "Epoch 96/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0339 - accuracy: 0.0000e+00\n",
      "Epoch 97/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0383 - accuracy: 0.0000e+00\n",
      "Epoch 98/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0374 - accuracy: 0.0000e+00\n",
      "Epoch 99/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0433 - accuracy: 0.0000e+00\n",
      "Epoch 100/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0440 - accuracy: 0.0000e+00\n",
      "Epoch 101/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0402 - accuracy: 0.0000e+00\n",
      "Epoch 102/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0377 - accuracy: 0.0000e+00\n",
      "Epoch 103/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0398 - accuracy: 0.0000e+00\n",
      "Epoch 104/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0417 - accuracy: 0.0000e+00\n",
      "Epoch 105/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0307 - accuracy: 0.0000e+00\n",
      "Epoch 106/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0279 - accuracy: 0.0000e+00\n",
      "Epoch 107/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0333 - accuracy: 0.0000e+00\n",
      "Epoch 108/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0349 - accuracy: 0.0000e+00\n",
      "Epoch 109/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0480 - accuracy: 0.0000e+00\n",
      "Epoch 110/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0412 - accuracy: 0.0000e+00\n",
      "Epoch 111/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0330 - accuracy: 0.0000e+00\n",
      "Epoch 112/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0280 - accuracy: 0.0000e+00\n",
      "Epoch 113/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0209 - accuracy: 0.0000e+00\n",
      "Epoch 114/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0243 - accuracy: 0.0000e+00\n",
      "Epoch 115/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0194 - accuracy: 0.0000e+00\n",
      "Epoch 116/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0240 - accuracy: 0.0000e+00\n",
      "Epoch 117/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0211 - accuracy: 0.0000e+00\n",
      "Epoch 118/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0223 - accuracy: 0.0000e+00\n",
      "Epoch 119/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0282 - accuracy: 0.0000e+00\n",
      "Epoch 120/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0446 - accuracy: 0.0000e+00\n",
      "Epoch 121/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0332 - accuracy: 0.0000e+00\n",
      "Epoch 122/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0395 - accuracy: 0.0000e+00\n",
      "Epoch 123/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0362 - accuracy: 0.0000e+00\n",
      "Epoch 124/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0391 - accuracy: 0.0000e+00\n",
      "Epoch 125/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0328 - accuracy: 0.0000e+00\n",
      "Epoch 126/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0257 - accuracy: 0.0000e+00\n",
      "Epoch 127/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0208 - accuracy: 0.0000e+00\n",
      "Epoch 128/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0250 - accuracy: 0.0000e+00\n",
      "Epoch 129/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0308 - accuracy: 0.0000e+00\n",
      "Epoch 130/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0389 - accuracy: 0.0000e+00\n",
      "Epoch 131/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0353 - accuracy: 0.0000e+00\n",
      "Epoch 132/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0352 - accuracy: 0.0000e+00\n",
      "Epoch 133/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0243 - accuracy: 0.0000e+00\n",
      "Epoch 134/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0244 - accuracy: 0.0000e+00\n",
      "Epoch 135/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0200 - accuracy: 0.0000e+00\n",
      "Epoch 136/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0312 - accuracy: 0.0000e+00\n",
      "Epoch 137/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0330 - accuracy: 0.0000e+00\n",
      "Epoch 138/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0311 - accuracy: 0.0000e+00\n",
      "Epoch 139/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0196 - accuracy: 0.0000e+00\n",
      "Epoch 140/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0290 - accuracy: 0.0000e+00\n",
      "Epoch 141/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0255 - accuracy: 0.0000e+00\n",
      "Epoch 142/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0351 - accuracy: 0.0000e+00\n",
      "Epoch 143/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0242 - accuracy: 0.0000e+00\n",
      "Epoch 144/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0295 - accuracy: 0.0000e+00\n",
      "Epoch 145/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0297 - accuracy: 0.0000e+00\n",
      "Epoch 146/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0342 - accuracy: 0.0000e+00\n",
      "Epoch 147/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0297 - accuracy: 0.0000e+00\n",
      "Epoch 148/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0260 - accuracy: 0.0000e+00\n",
      "Epoch 149/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0255 - accuracy: 0.0000e+00\n",
      "Epoch 150/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0361 - accuracy: 0.0000e+00\n",
      "Epoch 151/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0317 - accuracy: 0.0000e+00\n",
      "Epoch 152/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0315 - accuracy: 0.0000e+00\n",
      "Epoch 153/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0250 - accuracy: 0.0000e+00\n",
      "Epoch 154/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0170 - accuracy: 0.0000e+00\n",
      "Epoch 155/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0197 - accuracy: 0.0000e+00\n",
      "Epoch 156/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0256 - accuracy: 0.0000e+00\n",
      "Epoch 157/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0304 - accuracy: 0.0000e+00\n",
      "Epoch 158/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0350 - accuracy: 0.0000e+00\n",
      "Epoch 159/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0263 - accuracy: 0.0000e+00\n",
      "Epoch 160/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0309 - accuracy: 0.0000e+00\n",
      "Epoch 161/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0205 - accuracy: 0.0000e+00\n",
      "Epoch 162/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0177 - accuracy: 0.0000e+00\n",
      "Epoch 163/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0177 - accuracy: 0.0000e+00\n",
      "Epoch 164/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0163 - accuracy: 0.0000e+00\n",
      "Epoch 165/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0353 - accuracy: 0.0000e+00\n",
      "Epoch 166/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0261 - accuracy: 0.0000e+00\n",
      "Epoch 167/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0370 - accuracy: 0.0000e+00\n",
      "Epoch 168/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0239 - accuracy: 0.0000e+00\n",
      "Epoch 169/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0253 - accuracy: 0.0000e+00\n",
      "Epoch 170/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0230 - accuracy: 0.0000e+00\n",
      "Epoch 171/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0260 - accuracy: 0.0000e+00\n",
      "Epoch 172/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0190 - accuracy: 0.0000e+00\n",
      "Epoch 173/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0296 - accuracy: 0.0000e+00\n",
      "Epoch 174/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0251 - accuracy: 0.0000e+00\n",
      "Epoch 175/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0245 - accuracy: 0.0000e+00\n",
      "Epoch 176/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0177 - accuracy: 0.0000e+00\n",
      "Epoch 177/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0208 - accuracy: 0.0000e+00\n",
      "Epoch 178/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0279 - accuracy: 0.0000e+00\n",
      "Epoch 179/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0238 - accuracy: 0.0000e+00\n",
      "Epoch 180/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0296 - accuracy: 0.0000e+00\n",
      "Epoch 181/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0337 - accuracy: 0.0000e+00\n",
      "Epoch 182/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0296 - accuracy: 0.0000e+00\n",
      "Epoch 183/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0322 - accuracy: 0.0000e+00\n",
      "Epoch 184/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0202 - accuracy: 0.0000e+00\n",
      "Epoch 185/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0231 - accuracy: 0.0000e+00\n",
      "Epoch 186/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0240 - accuracy: 0.0000e+00\n",
      "Epoch 187/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0174 - accuracy: 0.0000e+00\n",
      "Epoch 188/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0152 - accuracy: 0.0000e+00\n",
      "Epoch 189/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0149 - accuracy: 0.0000e+00\n",
      "Epoch 190/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0000e+00\n",
      "Epoch 191/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0199 - accuracy: 0.0000e+00\n",
      "Epoch 192/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0245 - accuracy: 0.0000e+00\n",
      "Epoch 193/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0193 - accuracy: 0.0000e+00\n",
      "Epoch 194/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0186 - accuracy: 0.0000e+00\n",
      "Epoch 195/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0233 - accuracy: 0.0000e+00\n",
      "Epoch 196/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0315 - accuracy: 0.0000e+00\n",
      "Epoch 197/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0236 - accuracy: 0.0000e+00\n",
      "Epoch 198/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0241 - accuracy: 0.0000e+00\n",
      "Epoch 199/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0219 - accuracy: 0.0000e+00\n",
      "Epoch 200/200\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0227 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24770158610>"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model___ = Sequential()\n",
    "#----------------------\n",
    "# 30, 13, 6\n",
    "model___.add(Dense(90, input_dim=43, activation='relu'))\n",
    "model___.add(Dense(21, activation='relu'))\n",
    "model___.add(Dense(1))\n",
    "#-----------------------------------------------------------\n",
    "model___.compile(loss=tf.keras.losses.MeanSquaredError(),optimizer='adam', metrics=['accuracy'])\n",
    "#---------------------------------------------------------\n",
    "model___.fit(X_train,y_train, epochs=200, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "9378d114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 1ms/step - loss: 0.5755 - accuracy: 0.0000e+00\n",
      "Test Accuracy : [0.5754716396331787, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print('Test Accuracy : {}'.format(model___.evaluate(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "d64d1221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>121481.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>96444.757812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>200834.671875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>327809.968750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>139283.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2915</td>\n",
       "      <td>97251.359375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2916</td>\n",
       "      <td>155031.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2917</td>\n",
       "      <td>124096.132812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2918</td>\n",
       "      <td>142963.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2919</td>\n",
       "      <td>307382.937500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id      SalePrice\n",
       "0     1461  121481.937500\n",
       "1     1462   96444.757812\n",
       "2     1463  200834.671875\n",
       "3     1464  327809.968750\n",
       "4     1465  139283.187500\n",
       "...    ...            ...\n",
       "1454  2915   97251.359375\n",
       "1455  2916  155031.093750\n",
       "1456  2917  124096.132812\n",
       "1457  2918  142963.531250\n",
       "1458  2919  307382.937500\n",
       "\n",
       "[1459 rows x 2 columns]"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_from_model = model___.predict(df_house_test_extracted2b_predicted)\n",
    "df_house_test['SalePrice'] = 0\n",
    "df_house_test['SalePrice'] = np.expm1(result_from_model)\n",
    "df_house_test[['Id','SalePrice']].to_csv('C:/Users/thsong/submission15_3_3.csv',index=False)\n",
    "df_house_test[['Id','SalePrice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667f63f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "2fb71d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "115/115 [==============================] - 2s 3ms/step - loss: 27.4800 - accuracy: 0.0000e+00\n",
      "Epoch 2/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 1.3252 - accuracy: 0.0000e+00\n",
      "Epoch 3/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 1.0053 - accuracy: 0.0000e+00\n",
      "Epoch 4/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.6969 - accuracy: 0.0000e+00\n",
      "Epoch 5/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.5573 - accuracy: 0.0000e+00\n",
      "Epoch 6/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.4119 - accuracy: 0.0000e+00\n",
      "Epoch 7/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.2942 - accuracy: 0.0000e+00\n",
      "Epoch 8/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.2881 - accuracy: 0.0000e+00\n",
      "Epoch 9/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.2489 - accuracy: 0.0000e+00\n",
      "Epoch 10/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.1896 - accuracy: 0.0000e+00\n",
      "Epoch 11/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.1687 - accuracy: 0.0000e+00\n",
      "Epoch 12/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.1689 - accuracy: 0.0000e+00\n",
      "Epoch 13/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.1965 - accuracy: 0.0000e+00\n",
      "Epoch 14/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.1585 - accuracy: 0.0000e+00\n",
      "Epoch 15/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.1229 - accuracy: 0.0000e+00\n",
      "Epoch 16/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0885 - accuracy: 0.0000e+00\n",
      "Epoch 17/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.1018 - accuracy: 0.0000e+00\n",
      "Epoch 18/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.1291 - accuracy: 0.0000e+00\n",
      "Epoch 19/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0853 - accuracy: 0.0000e+00\n",
      "Epoch 20/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0915 - accuracy: 0.0000e+00\n",
      "Epoch 21/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0733 - accuracy: 0.0000e+00\n",
      "Epoch 22/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.1246 - accuracy: 0.0000e+00\n",
      "Epoch 23/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0680 - accuracy: 0.0000e+00\n",
      "Epoch 24/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0698 - accuracy: 0.0000e+00\n",
      "Epoch 25/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0629 - accuracy: 0.0000e+00\n",
      "Epoch 26/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0640 - accuracy: 0.0000e+00\n",
      "Epoch 27/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0584 - accuracy: 0.0000e+00\n",
      "Epoch 28/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0997 - accuracy: 0.0000e+00\n",
      "Epoch 29/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0699 - accuracy: 0.0000e+00\n",
      "Epoch 30/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0767 - accuracy: 0.0000e+00\n",
      "Epoch 31/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0544 - accuracy: 0.0000e+00\n",
      "Epoch 32/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0637 - accuracy: 0.0000e+00\n",
      "Epoch 33/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0858 - accuracy: 0.0000e+00\n",
      "Epoch 34/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0561 - accuracy: 0.0000e+00\n",
      "Epoch 35/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0506 - accuracy: 0.0000e+00\n",
      "Epoch 36/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0685 - accuracy: 0.0000e+00\n",
      "Epoch 37/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0533 - accuracy: 0.0000e+00\n",
      "Epoch 38/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0400 - accuracy: 0.0000e+00\n",
      "Epoch 39/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.1148 - accuracy: 0.0000e+00\n",
      "Epoch 40/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0833 - accuracy: 0.0000e+00\n",
      "Epoch 41/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0685 - accuracy: 0.0000e+00\n",
      "Epoch 42/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0617 - accuracy: 0.0000e+00\n",
      "Epoch 43/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0498 - accuracy: 0.0000e+00\n",
      "Epoch 44/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0426 - accuracy: 0.0000e+00\n",
      "Epoch 45/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0337 - accuracy: 0.0000e+00\n",
      "Epoch 46/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0294 - accuracy: 0.0000e+00\n",
      "Epoch 47/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0220 - accuracy: 0.0000e+00\n",
      "Epoch 48/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0246 - accuracy: 0.0000e+00\n",
      "Epoch 49/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0425 - accuracy: 0.0000e+00\n",
      "Epoch 50/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0390 - accuracy: 0.0000e+00\n",
      "Epoch 51/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0293 - accuracy: 0.0000e+00\n",
      "Epoch 52/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0319 - accuracy: 0.0000e+00\n",
      "Epoch 53/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0489 - accuracy: 0.0000e+00\n",
      "Epoch 54/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0436 - accuracy: 0.0000e+00\n",
      "Epoch 55/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0643 - accuracy: 0.0000e+00\n",
      "Epoch 56/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0396 - accuracy: 0.0000e+00\n",
      "Epoch 57/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0359 - accuracy: 0.0000e+00\n",
      "Epoch 58/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0327 - accuracy: 0.0000e+00\n",
      "Epoch 59/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0361 - accuracy: 0.0000e+00\n",
      "Epoch 60/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0423 - accuracy: 0.0000e+00\n",
      "Epoch 61/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0348 - accuracy: 0.0000e+00\n",
      "Epoch 62/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0284 - accuracy: 0.0000e+00\n",
      "Epoch 63/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0183 - accuracy: 0.0000e+00\n",
      "Epoch 64/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0185 - accuracy: 0.0000e+00\n",
      "Epoch 65/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0276 - accuracy: 0.0000e+00\n",
      "Epoch 66/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0000e+00\n",
      "Epoch 67/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0259 - accuracy: 0.0000e+00\n",
      "Epoch 68/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0797 - accuracy: 0.0000e+00\n",
      "Epoch 69/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0774 - accuracy: 0.0000e+00\n",
      "Epoch 70/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0515 - accuracy: 0.0000e+00\n",
      "Epoch 71/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0344 - accuracy: 0.0000e+00\n",
      "Epoch 72/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0353 - accuracy: 0.0000e+00\n",
      "Epoch 73/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0260 - accuracy: 0.0000e+00\n",
      "Epoch 74/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0376 - accuracy: 0.0000e+00\n",
      "Epoch 75/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0206 - accuracy: 0.0000e+00\n",
      "Epoch 76/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0171 - accuracy: 0.0000e+00\n",
      "Epoch 77/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0131 - accuracy: 0.0000e+00\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0112 - accuracy: 0.0000e+00\n",
      "Epoch 79/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0153 - accuracy: 0.0000e+00\n",
      "Epoch 80/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0182 - accuracy: 0.0000e+00\n",
      "Epoch 81/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0165 - accuracy: 0.0000e+00\n",
      "Epoch 82/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0152 - accuracy: 0.0000e+00\n",
      "Epoch 83/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0220 - accuracy: 0.0000e+00\n",
      "Epoch 84/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0234 - accuracy: 0.0000e+00\n",
      "Epoch 85/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0000e+00\n",
      "Epoch 86/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0237 - accuracy: 0.0000e+00\n",
      "Epoch 87/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0173 - accuracy: 0.0000e+00\n",
      "Epoch 88/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0000e+00\n",
      "Epoch 89/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0294 - accuracy: 0.0000e+00\n",
      "Epoch 90/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0369 - accuracy: 0.0000e+00\n",
      "Epoch 91/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0308 - accuracy: 0.0000e+00\n",
      "Epoch 92/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0228 - accuracy: 0.0000e+00\n",
      "Epoch 93/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0172 - accuracy: 0.0000e+00\n",
      "Epoch 94/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0181 - accuracy: 0.0000e+00\n",
      "Epoch 95/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0213 - accuracy: 0.0000e+00\n",
      "Epoch 96/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0214 - accuracy: 0.0000e+00\n",
      "Epoch 97/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 0.0000e+00\n",
      "Epoch 98/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 0.0000e+00\n",
      "Epoch 99/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0169 - accuracy: 0.0000e+00\n",
      "Epoch 100/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0168 - accuracy: 0.0000e+00\n",
      "Epoch 101/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0158 - accuracy: 0.0000e+00\n",
      "Epoch 102/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0151 - accuracy: 0.0000e+00\n",
      "Epoch 103/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0143 - accuracy: 0.0000e+00\n",
      "Epoch 104/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 0.0000e+00\n",
      "Epoch 105/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0220 - accuracy: 0.0000e+00\n",
      "Epoch 106/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0162 - accuracy: 0.0000e+00\n",
      "Epoch 107/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0181 - accuracy: 0.0000e+00\n",
      "Epoch 108/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0347 - accuracy: 0.0000e+00\n",
      "Epoch 109/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0397 - accuracy: 0.0000e+00\n",
      "Epoch 110/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0247 - accuracy: 0.0000e+00\n",
      "Epoch 111/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0168 - accuracy: 0.0000e+00\n",
      "Epoch 112/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 0.0000e+00\n",
      "Epoch 113/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0116 - accuracy: 0.0000e+00\n",
      "Epoch 114/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0113 - accuracy: 0.0000e+00\n",
      "Epoch 115/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0176 - accuracy: 0.0000e+00\n",
      "Epoch 116/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0148 - accuracy: 0.0000e+00\n",
      "Epoch 117/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0144 - accuracy: 0.0000e+00\n",
      "Epoch 118/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0101 - accuracy: 0.0000e+00\n",
      "Epoch 119/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0096 - accuracy: 0.0000e+00\n",
      "Epoch 120/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0148 - accuracy: 0.0000e+00\n",
      "Epoch 121/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0100 - accuracy: 0.0000e+00\n",
      "Epoch 122/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0275 - accuracy: 0.0000e+00\n",
      "Epoch 123/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0166 - accuracy: 0.0000e+00\n",
      "Epoch 124/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0147 - accuracy: 0.0000e+00\n",
      "Epoch 125/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0115 - accuracy: 0.0000e+00\n",
      "Epoch 126/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0153 - accuracy: 0.0000e+00\n",
      "Epoch 127/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0131 - accuracy: 0.0000e+00\n",
      "Epoch 128/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0146 - accuracy: 0.0000e+00\n",
      "Epoch 129/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0190 - accuracy: 0.0000e+00\n",
      "Epoch 130/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0154 - accuracy: 0.0000e+00\n",
      "Epoch 131/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0197 - accuracy: 0.0000e+00\n",
      "Epoch 132/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0131 - accuracy: 0.0000e+00\n",
      "Epoch 133/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0095 - accuracy: 0.0000e+00\n",
      "Epoch 134/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0086 - accuracy: 0.0000e+00\n",
      "Epoch 135/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0090 - accuracy: 0.0000e+00\n",
      "Epoch 136/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0074 - accuracy: 0.0000e+00\n",
      "Epoch 137/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0077 - accuracy: 0.0000e+00\n",
      "Epoch 138/200\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0064 - accuracy: 0.0000e+00\n",
      "Epoch 139/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0078 - accuracy: 0.0000e+00\n",
      "Epoch 140/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0130 - accuracy: 0.0000e+00\n",
      "Epoch 141/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0158 - accuracy: 0.0000e+00\n",
      "Epoch 142/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0143 - accuracy: 0.0000e+00\n",
      "Epoch 143/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 0.0000e+00\n",
      "Epoch 144/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0101 - accuracy: 0.0000e+00\n",
      "Epoch 145/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0092 - accuracy: 0.0000e+00\n",
      "Epoch 146/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0110 - accuracy: 0.0000e+00\n",
      "Epoch 147/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0090 - accuracy: 0.0000e+00\n",
      "Epoch 148/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0087 - accuracy: 0.0000e+00\n",
      "Epoch 149/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0091 - accuracy: 0.0000e+00\n",
      "Epoch 150/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0094 - accuracy: 0.0000e+00\n",
      "Epoch 151/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0137 - accuracy: 0.0000e+00\n",
      "Epoch 152/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0166 - accuracy: 0.0000e+00\n",
      "Epoch 153/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0319 - accuracy: 0.0000e+00\n",
      "Epoch 154/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0330 - accuracy: 0.0000e+00\n",
      "Epoch 155/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.0000e+00\n",
      "Epoch 156/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0127 - accuracy: 0.0000e+00\n",
      "Epoch 157/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0102 - accuracy: 0.0000e+00\n",
      "Epoch 158/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0088 - accuracy: 0.0000e+00\n",
      "Epoch 159/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0086 - accuracy: 0.0000e+00\n",
      "Epoch 160/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0055 - accuracy: 0.0000e+00\n",
      "Epoch 161/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0056 - accuracy: 0.0000e+00\n",
      "Epoch 162/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0046 - accuracy: 0.0000e+00\n",
      "Epoch 163/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0042 - accuracy: 0.0000e+00\n",
      "Epoch 164/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0066 - accuracy: 0.0000e+00\n",
      "Epoch 165/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0138 - accuracy: 0.0000e+00\n",
      "Epoch 166/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0095 - accuracy: 0.0000e+00\n",
      "Epoch 167/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0120 - accuracy: 0.0000e+00\n",
      "Epoch 168/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0093 - accuracy: 0.0000e+00\n",
      "Epoch 169/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0080 - accuracy: 0.0000e+00\n",
      "Epoch 170/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0054 - accuracy: 0.0000e+00\n",
      "Epoch 171/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0088 - accuracy: 0.0000e+00\n",
      "Epoch 172/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0068 - accuracy: 0.0000e+00\n",
      "Epoch 173/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0070 - accuracy: 0.0000e+00\n",
      "Epoch 174/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0084 - accuracy: 0.0000e+00\n",
      "Epoch 175/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0070 - accuracy: 0.0000e+00\n",
      "Epoch 176/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0064 - accuracy: 0.0000e+00\n",
      "Epoch 177/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0072 - accuracy: 0.0000e+00\n",
      "Epoch 178/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0080 - accuracy: 0.0000e+00\n",
      "Epoch 179/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0100 - accuracy: 0.0000e+00\n",
      "Epoch 180/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0227 - accuracy: 0.0000e+00\n",
      "Epoch 181/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0159 - accuracy: 0.0000e+00\n",
      "Epoch 182/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0119 - accuracy: 0.0000e+00\n",
      "Epoch 183/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0115 - accuracy: 0.0000e+00\n",
      "Epoch 184/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0225 - accuracy: 0.0000e+00\n",
      "Epoch 185/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0000e+00\n",
      "Epoch 186/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0145 - accuracy: 0.0000e+00\n",
      "Epoch 187/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0095 - accuracy: 0.0000e+00\n",
      "Epoch 188/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0066 - accuracy: 0.0000e+00\n",
      "Epoch 189/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0058 - accuracy: 0.0000e+00\n",
      "Epoch 190/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0072 - accuracy: 0.0000e+00\n",
      "Epoch 191/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0089 - accuracy: 0.0000e+00\n",
      "Epoch 192/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0076 - accuracy: 0.0000e+00\n",
      "Epoch 193/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0057 - accuracy: 0.0000e+00\n",
      "Epoch 194/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0075 - accuracy: 0.0000e+00\n",
      "Epoch 195/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0067 - accuracy: 0.0000e+00\n",
      "Epoch 196/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0059 - accuracy: 0.0000e+00\n",
      "Epoch 197/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0079 - accuracy: 0.0000e+00\n",
      "Epoch 198/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0068 - accuracy: 0.0000e+00\n",
      "Epoch 199/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0084 - accuracy: 0.0000e+00\n",
      "Epoch 200/200\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0112 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x247023c3700>"
      ]
     },
     "execution_count": 600,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4 = Sequential()\n",
    "model_4.add(Dense(100, input_dim=43, activation='relu'))\n",
    "#------------------------------------------------------------\n",
    "### 2/3 size of input layer , 28.67\n",
    "#---------------------------------------------------------\n",
    "model_4.add(Dense(96, activation='relu'))\n",
    "model_4.add(Dense(92, activation='relu'))\n",
    "model_4.add(Dense(88, activation='relu'))\n",
    "model_4.add(Dense(84, activation='relu'))\n",
    "model_4.add(Dense(80, activation='relu'))\n",
    "model_4.add(Dense(76, activation='relu'))\n",
    "model_4.add(Dense(72, activation='relu'))\n",
    "model_4.add(Dense(68, activation='relu'))\n",
    "model_4.add(Dense(64, activation='relu'))\n",
    "model_4.add(Dense(60, activation='relu'))\n",
    "model_4.add(Dense(56, activation='relu'))\n",
    "model_4.add(Dense(52, activation='relu'))\n",
    "model_4.add(Dense(48, activation='relu'))\n",
    "model_4.add(Dense(44, activation='relu'))\n",
    "model_4.add(Dense(40, activation='relu'))\n",
    "model_4.add(Dense(36, activation='relu'))\n",
    "model_4.add(Dense(32, activation='relu'))\n",
    "model_4.add(Dense(28, activation='relu'))\n",
    "model_4.add(Dense(24, activation='relu'))\n",
    "model_4.add(Dense(20, activation='relu'))\n",
    "model_4.add(Dense(16, activation='relu'))\n",
    "model_4.add(Dense(12, activation='relu'))\n",
    "model_4.add(Dense(8, activation='relu'))\n",
    "model_4.add(Dense(4, activation='relu'))\n",
    "#---------------------------------------------------\n",
    "model_4.add(Dense(1))\n",
    "model_4.compile(loss=tf.keras.losses.MeanSquaredError(),optimizer='adam', metrics=['accuracy']) # \n",
    "model_4.fit(X_train,y_train, epochs=200, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "8c92eddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 2ms/step - loss: 0.2192 - accuracy: 0.0000e+00\n",
      "Test Accuracy : [0.21915064752101898, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print('Test Accuracy : {}'.format(model_4.evaluate(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "21a67976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>133205.515625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>202684.328125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>170856.296875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>121755.429688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>222578.328125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2915</td>\n",
       "      <td>131072.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2916</td>\n",
       "      <td>189902.281250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2917</td>\n",
       "      <td>152768.359375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2918</td>\n",
       "      <td>137575.296875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2919</td>\n",
       "      <td>149628.203125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id      SalePrice\n",
       "0     1461  133205.515625\n",
       "1     1462  202684.328125\n",
       "2     1463  170856.296875\n",
       "3     1464  121755.429688\n",
       "4     1465  222578.328125\n",
       "...    ...            ...\n",
       "1454  2915  131072.562500\n",
       "1455  2916  189902.281250\n",
       "1456  2917  152768.359375\n",
       "1457  2918  137575.296875\n",
       "1458  2919  149628.203125\n",
       "\n",
       "[1459 rows x 2 columns]"
      ]
     },
     "execution_count": 602,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_from_model = model_4.predict(df_house_test_extracted2b_predicted)\n",
    "df_house_test['SalePrice'] = 0\n",
    "df_house_test['SalePrice'] = np.expm1(result_from_model)\n",
    "df_house_test[['Id','SalePrice']].to_csv('C:/Users/thsong/submission15_4.csv',index=False)\n",
    "df_house_test[['Id','SalePrice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caaa132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "d13ab578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "115/115 [==============================] - 3s 4ms/step - loss: 134.4598 - accuracy: 0.0000e+00\n",
      "Epoch 2/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 9.3992 - accuracy: 0.0000e+00\n",
      "Epoch 3/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 1.0178 - accuracy: 0.0000e+00\n",
      "Epoch 4/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.5557 - accuracy: 0.0000e+00\n",
      "Epoch 5/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.4473 - accuracy: 0.0000e+00\n",
      "Epoch 6/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.3520 - accuracy: 0.0000e+00\n",
      "Epoch 7/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.2238 - accuracy: 0.0000e+00\n",
      "Epoch 8/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.2029 - accuracy: 0.0000e+00\n",
      "Epoch 9/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.1840 - accuracy: 0.0000e+00\n",
      "Epoch 10/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.1678 - accuracy: 0.0000e+00\n",
      "Epoch 11/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.1475 - accuracy: 0.0000e+00\n",
      "Epoch 12/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.1307 - accuracy: 0.0000e+00\n",
      "Epoch 13/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.1311 - accuracy: 0.0000e+00\n",
      "Epoch 14/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.1209 - accuracy: 0.0000e+00\n",
      "Epoch 15/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.1052 - accuracy: 0.0000e+00\n",
      "Epoch 16/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0978 - accuracy: 0.0000e+00\n",
      "Epoch 17/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.1290 - accuracy: 0.0000e+00\n",
      "Epoch 18/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0921 - accuracy: 0.0000e+00\n",
      "Epoch 19/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.1058 - accuracy: 0.0000e+00\n",
      "Epoch 20/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0777 - accuracy: 0.0000e+00\n",
      "Epoch 21/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0895 - accuracy: 0.0000e+00\n",
      "Epoch 22/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0735 - accuracy: 0.0000e+00\n",
      "Epoch 23/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0561 - accuracy: 0.0000e+00\n",
      "Epoch 24/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0571 - accuracy: 0.0000e+00\n",
      "Epoch 25/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0504 - accuracy: 0.0000e+00\n",
      "Epoch 26/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0527 - accuracy: 0.0000e+00\n",
      "Epoch 27/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0471 - accuracy: 0.0000e+00\n",
      "Epoch 28/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0428 - accuracy: 0.0000e+00\n",
      "Epoch 29/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0572 - accuracy: 0.0000e+00\n",
      "Epoch 30/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0480 - accuracy: 0.0000e+00\n",
      "Epoch 31/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0456 - accuracy: 0.0000e+00\n",
      "Epoch 32/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0509 - accuracy: 0.0000e+00\n",
      "Epoch 33/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0434 - accuracy: 0.0000e+00\n",
      "Epoch 34/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0664 - accuracy: 0.0000e+00\n",
      "Epoch 35/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0719 - accuracy: 0.0000e+00\n",
      "Epoch 36/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0556 - accuracy: 0.0000e+00\n",
      "Epoch 37/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0523 - accuracy: 0.0000e+00\n",
      "Epoch 38/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0683 - accuracy: 0.0000e+00\n",
      "Epoch 39/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0596 - accuracy: 0.0000e+00\n",
      "Epoch 40/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0511 - accuracy: 0.0000e+00\n",
      "Epoch 41/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0661 - accuracy: 0.0000e+00\n",
      "Epoch 42/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0857 - accuracy: 0.0000e+00\n",
      "Epoch 43/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0411 - accuracy: 0.0000e+00\n",
      "Epoch 44/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0334 - accuracy: 0.0000e+00\n",
      "Epoch 45/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0462 - accuracy: 0.0000e+00\n",
      "Epoch 46/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0567 - accuracy: 0.0000e+00\n",
      "Epoch 47/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0511 - accuracy: 0.0000e+00\n",
      "Epoch 48/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0825 - accuracy: 0.0000e+00\n",
      "Epoch 49/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0540 - accuracy: 0.0000e+00\n",
      "Epoch 50/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0580 - accuracy: 0.0000e+00\n",
      "Epoch 51/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0459 - accuracy: 0.0000e+00\n",
      "Epoch 52/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0712 - accuracy: 0.0000e+00\n",
      "Epoch 53/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0509 - accuracy: 0.0000e+00\n",
      "Epoch 54/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0454 - accuracy: 0.0000e+00\n",
      "Epoch 55/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0293 - accuracy: 0.0000e+00\n",
      "Epoch 56/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0217 - accuracy: 0.0000e+00\n",
      "Epoch 57/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0217 - accuracy: 0.0000e+00\n",
      "Epoch 58/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0577 - accuracy: 0.0000e+00\n",
      "Epoch 59/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0704 - accuracy: 0.0000e+00\n",
      "Epoch 60/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0550 - accuracy: 0.0000e+00\n",
      "Epoch 61/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0371 - accuracy: 0.0000e+00\n",
      "Epoch 62/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0654 - accuracy: 0.0000e+00\n",
      "Epoch 63/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0651 - accuracy: 0.0000e+00\n",
      "Epoch 64/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0485 - accuracy: 0.0000e+00\n",
      "Epoch 65/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0511 - accuracy: 0.0000e+00\n",
      "Epoch 66/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0441 - accuracy: 0.0000e+00\n",
      "Epoch 67/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0526 - accuracy: 0.0000e+00\n",
      "Epoch 68/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0245 - accuracy: 0.0000e+00\n",
      "Epoch 69/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0393 - accuracy: 0.0000e+00\n",
      "Epoch 70/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0364 - accuracy: 0.0000e+00\n",
      "Epoch 71/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0340 - accuracy: 0.0000e+00\n",
      "Epoch 72/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0463 - accuracy: 0.0000e+00\n",
      "Epoch 73/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0472 - accuracy: 0.0000e+00\n",
      "Epoch 74/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0422 - accuracy: 0.0000e+00\n",
      "Epoch 75/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0313 - accuracy: 0.0000e+00\n",
      "Epoch 76/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0484 - accuracy: 0.0000e+00\n",
      "Epoch 77/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0574 - accuracy: 0.0000e+00\n",
      "Epoch 78/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0429 - accuracy: 0.0000e+00\n",
      "Epoch 79/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0437 - accuracy: 0.0000e+00\n",
      "Epoch 80/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0282 - accuracy: 0.0000e+00\n",
      "Epoch 81/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0448 - accuracy: 0.0000e+00\n",
      "Epoch 82/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0793 - accuracy: 0.0000e+00\n",
      "Epoch 83/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0531 - accuracy: 0.0000e+00\n",
      "Epoch 84/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0254 - accuracy: 0.0000e+00\n",
      "Epoch 85/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0346 - accuracy: 0.0000e+00\n",
      "Epoch 86/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0252 - accuracy: 0.0000e+00\n",
      "Epoch 87/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0186 - accuracy: 0.0000e+00\n",
      "Epoch 88/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0147 - accuracy: 0.0000e+00\n",
      "Epoch 89/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0102 - accuracy: 0.0000e+00\n",
      "Epoch 90/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0382 - accuracy: 0.0000e+00\n",
      "Epoch 91/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0598 - accuracy: 0.0000e+00\n",
      "Epoch 92/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0334 - accuracy: 0.0000e+00\n",
      "Epoch 93/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0304 - accuracy: 0.0000e+00\n",
      "Epoch 94/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0323 - accuracy: 0.0000e+00\n",
      "Epoch 95/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0502 - accuracy: 0.0000e+00\n",
      "Epoch 96/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0699 - accuracy: 0.0000e+00\n",
      "Epoch 97/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0325 - accuracy: 0.0000e+00\n",
      "Epoch 98/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0308 - accuracy: 0.0000e+00\n",
      "Epoch 99/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0474 - accuracy: 0.0000e+00\n",
      "Epoch 100/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0404 - accuracy: 0.0000e+00\n",
      "Epoch 101/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0317 - accuracy: 0.0000e+00\n",
      "Epoch 102/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0227 - accuracy: 0.0000e+00\n",
      "Epoch 103/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0257 - accuracy: 0.0000e+00\n",
      "Epoch 104/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0215 - accuracy: 0.0000e+00\n",
      "Epoch 105/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0263 - accuracy: 0.0000e+00\n",
      "Epoch 106/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0177 - accuracy: 0.0000e+00\n",
      "Epoch 107/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0128 - accuracy: 0.0000e+00\n",
      "Epoch 108/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0254 - accuracy: 0.0000e+00\n",
      "Epoch 109/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0263 - accuracy: 0.0000e+00\n",
      "Epoch 110/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0193 - accuracy: 0.0000e+00\n",
      "Epoch 111/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0265 - accuracy: 0.0000e+00\n",
      "Epoch 112/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0419 - accuracy: 0.0000e+00\n",
      "Epoch 113/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0342 - accuracy: 0.0000e+00\n",
      "Epoch 114/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0294 - accuracy: 0.0000e+00\n",
      "Epoch 115/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0223 - accuracy: 0.0000e+00\n",
      "Epoch 116/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0342 - accuracy: 0.0000e+00\n",
      "Epoch 117/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0227 - accuracy: 0.0000e+00\n",
      "Epoch 118/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0219 - accuracy: 0.0000e+00\n",
      "Epoch 119/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0226 - accuracy: 0.0000e+00\n",
      "Epoch 120/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0328 - accuracy: 0.0000e+00\n",
      "Epoch 121/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0322 - accuracy: 0.0000e+00\n",
      "Epoch 122/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0374 - accuracy: 0.0000e+00\n",
      "Epoch 123/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0249 - accuracy: 0.0000e+00\n",
      "Epoch 124/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0175 - accuracy: 0.0000e+00\n",
      "Epoch 125/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0214 - accuracy: 0.0000e+00\n",
      "Epoch 126/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0203 - accuracy: 0.0000e+00\n",
      "Epoch 127/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0157 - accuracy: 0.0000e+00\n",
      "Epoch 128/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0236 - accuracy: 0.0000e+00\n",
      "Epoch 129/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0174 - accuracy: 0.0000e+00\n",
      "Epoch 130/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0163 - accuracy: 0.0000e+00\n",
      "Epoch 131/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0301 - accuracy: 0.0000e+00\n",
      "Epoch 132/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0296 - accuracy: 0.0000e+00\n",
      "Epoch 133/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0222 - accuracy: 0.0000e+00\n",
      "Epoch 134/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0230 - accuracy: 0.0000e+00\n",
      "Epoch 135/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0202 - accuracy: 0.0000e+00\n",
      "Epoch 136/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0212 - accuracy: 0.0000e+00\n",
      "Epoch 137/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0178 - accuracy: 0.0000e+00\n",
      "Epoch 138/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0209 - accuracy: 0.0000e+00\n",
      "Epoch 139/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0230 - accuracy: 0.0000e+00\n",
      "Epoch 140/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0212 - accuracy: 0.0000e+00\n",
      "Epoch 141/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0156 - accuracy: 0.0000e+00\n",
      "Epoch 142/1000\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.0177 - accuracy: 0.0000e+00\n",
      "Epoch 143/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0241 - accuracy: 0.0000e+00\n",
      "Epoch 144/1000\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.0225 - accuracy: 0.0000e+00\n",
      "Epoch 145/1000\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.0325 - accuracy: 0.0000e+00\n",
      "Epoch 146/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0224 - accuracy: 0.0000e+00\n",
      "Epoch 147/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0183 - accuracy: 0.0000e+00\n",
      "Epoch 148/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0120 - accuracy: 0.0000e+00\n",
      "Epoch 149/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0126 - accuracy: 0.0000e+00\n",
      "Epoch 150/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0123 - accuracy: 0.0000e+00\n",
      "Epoch 151/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0176 - accuracy: 0.0000e+00\n",
      "Epoch 152/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0199 - accuracy: 0.0000e+00\n",
      "Epoch 153/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0162 - accuracy: 0.0000e+00\n",
      "Epoch 154/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0162 - accuracy: 0.0000e+00\n",
      "Epoch 155/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0234 - accuracy: 0.0000e+00\n",
      "Epoch 156/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0146 - accuracy: 0.0000e+00\n",
      "Epoch 157/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0214 - accuracy: 0.0000e+00\n",
      "Epoch 158/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0316 - accuracy: 0.0000e+00\n",
      "Epoch 159/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0322 - accuracy: 0.0000e+00\n",
      "Epoch 160/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0286 - accuracy: 0.0000e+00\n",
      "Epoch 161/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0233 - accuracy: 0.0000e+00\n",
      "Epoch 162/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0200 - accuracy: 0.0000e+00\n",
      "Epoch 163/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0135 - accuracy: 0.0000e+00\n",
      "Epoch 164/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0125 - accuracy: 0.0000e+00\n",
      "Epoch 165/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0183 - accuracy: 0.0000e+00\n",
      "Epoch 166/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0161 - accuracy: 0.0000e+00\n",
      "Epoch 167/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0174 - accuracy: 0.0000e+00\n",
      "Epoch 168/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0130 - accuracy: 0.0000e+00\n",
      "Epoch 169/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0128 - accuracy: 0.0000e+00\n",
      "Epoch 170/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0187 - accuracy: 0.0000e+00\n",
      "Epoch 171/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0152 - accuracy: 0.0000e+00\n",
      "Epoch 172/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0089 - accuracy: 0.0000e+00\n",
      "Epoch 173/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0117 - accuracy: 0.0000e+00\n",
      "Epoch 174/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0186 - accuracy: 0.0000e+00\n",
      "Epoch 175/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0131 - accuracy: 0.0000e+00\n",
      "Epoch 176/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0117 - accuracy: 0.0000e+00\n",
      "Epoch 177/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0153 - accuracy: 0.0000e+00\n",
      "Epoch 178/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0135 - accuracy: 0.0000e+00\n",
      "Epoch 179/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0146 - accuracy: 0.0000e+00\n",
      "Epoch 180/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0120 - accuracy: 0.0000e+00\n",
      "Epoch 181/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0244 - accuracy: 0.0000e+00\n",
      "Epoch 182/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0192 - accuracy: 0.0000e+00\n",
      "Epoch 183/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0228 - accuracy: 0.0000e+00\n",
      "Epoch 184/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0177 - accuracy: 0.0000e+00\n",
      "Epoch 185/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0173 - accuracy: 0.0000e+00\n",
      "Epoch 186/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0129 - accuracy: 0.0000e+00\n",
      "Epoch 187/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0098 - accuracy: 0.0000e+00\n",
      "Epoch 188/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0100 - accuracy: 0.0000e+00\n",
      "Epoch 189/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0084 - accuracy: 0.0000e+00\n",
      "Epoch 190/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0133 - accuracy: 0.0000e+00\n",
      "Epoch 191/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0139 - accuracy: 0.0000e+00\n",
      "Epoch 192/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0273 - accuracy: 0.0000e+00\n",
      "Epoch 193/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0318 - accuracy: 0.0000e+00\n",
      "Epoch 194/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0336 - accuracy: 0.0000e+00\n",
      "Epoch 195/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0178 - accuracy: 0.0000e+00\n",
      "Epoch 196/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0128 - accuracy: 0.0000e+00\n",
      "Epoch 197/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0120 - accuracy: 0.0000e+00\n",
      "Epoch 198/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0095 - accuracy: 0.0000e+00\n",
      "Epoch 199/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0128 - accuracy: 0.0000e+00\n",
      "Epoch 200/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0139 - accuracy: 0.0000e+00\n",
      "Epoch 201/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0089 - accuracy: 0.0000e+00\n",
      "Epoch 202/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0079 - accuracy: 0.0000e+00\n",
      "Epoch 203/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 0.0000e+00\n",
      "Epoch 204/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0076 - accuracy: 0.0000e+00\n",
      "Epoch 205/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0067 - accuracy: 0.0000e+00\n",
      "Epoch 206/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0106 - accuracy: 0.0000e+00\n",
      "Epoch 207/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0084 - accuracy: 0.0000e+00\n",
      "Epoch 208/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0065 - accuracy: 0.0000e+00\n",
      "Epoch 209/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0151 - accuracy: 0.0000e+00\n",
      "Epoch 210/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0170 - accuracy: 0.0000e+00\n",
      "Epoch 211/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0185 - accuracy: 0.0000e+00\n",
      "Epoch 212/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0139 - accuracy: 0.0000e+00\n",
      "Epoch 213/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0201 - accuracy: 0.0000e+00\n",
      "Epoch 214/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0198 - accuracy: 0.0000e+00\n",
      "Epoch 215/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0164 - accuracy: 0.0000e+00\n",
      "Epoch 216/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0147 - accuracy: 0.0000e+00\n",
      "Epoch 217/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0104 - accuracy: 0.0000e+00\n",
      "Epoch 218/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0073 - accuracy: 0.0000e+00\n",
      "Epoch 219/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0082 - accuracy: 0.0000e+00\n",
      "Epoch 220/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0147 - accuracy: 0.0000e+00\n",
      "Epoch 221/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0112 - accuracy: 0.0000e+00\n",
      "Epoch 222/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0149 - accuracy: 0.0000e+00\n",
      "Epoch 223/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0105 - accuracy: 0.0000e+00\n",
      "Epoch 224/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0087 - accuracy: 0.0000e+00\n",
      "Epoch 225/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0080 - accuracy: 0.0000e+00\n",
      "Epoch 226/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0106 - accuracy: 0.0000e+00\n",
      "Epoch 227/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0149 - accuracy: 0.0000e+00\n",
      "Epoch 228/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0130 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0078 - accuracy: 0.0000e+00\n",
      "Epoch 230/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0090 - accuracy: 0.0000e+00\n",
      "Epoch 231/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0118 - accuracy: 0.0000e+00\n",
      "Epoch 232/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0103 - accuracy: 0.0000e+00\n",
      "Epoch 233/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0132 - accuracy: 0.0000e+00\n",
      "Epoch 234/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0099 - accuracy: 0.0000e+00\n",
      "Epoch 235/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 0.0000e+00\n",
      "Epoch 236/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0146 - accuracy: 0.0000e+00\n",
      "Epoch 237/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0148 - accuracy: 0.0000e+00\n",
      "Epoch 238/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0123 - accuracy: 0.0000e+00\n",
      "Epoch 239/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0128 - accuracy: 0.0000e+00\n",
      "Epoch 240/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0096 - accuracy: 0.0000e+00\n",
      "Epoch 241/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0092 - accuracy: 0.0000e+00\n",
      "Epoch 242/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0098 - accuracy: 0.0000e+00\n",
      "Epoch 243/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 0.0000e+00\n",
      "Epoch 244/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0163 - accuracy: 0.0000e+00\n",
      "Epoch 245/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0153 - accuracy: 0.0000e+00\n",
      "Epoch 246/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0126 - accuracy: 0.0000e+00\n",
      "Epoch 247/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0153 - accuracy: 0.0000e+00\n",
      "Epoch 248/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0126 - accuracy: 0.0000e+00\n",
      "Epoch 249/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0142 - accuracy: 0.0000e+00\n",
      "Epoch 250/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0111 - accuracy: 0.0000e+00\n",
      "Epoch 251/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0135 - accuracy: 0.0000e+00\n",
      "Epoch 252/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0077 - accuracy: 0.0000e+00\n",
      "Epoch 253/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0083 - accuracy: 0.0000e+00\n",
      "Epoch 254/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0074 - accuracy: 0.0000e+00\n",
      "Epoch 255/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0067 - accuracy: 0.0000e+00\n",
      "Epoch 256/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0061 - accuracy: 0.0000e+00\n",
      "Epoch 257/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0044 - accuracy: 0.0000e+00\n",
      "Epoch 258/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0064 - accuracy: 0.0000e+00\n",
      "Epoch 259/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0092 - accuracy: 0.0000e+00\n",
      "Epoch 260/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0111 - accuracy: 0.0000e+00\n",
      "Epoch 261/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0100 - accuracy: 0.0000e+00\n",
      "Epoch 262/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0105 - accuracy: 0.0000e+00\n",
      "Epoch 263/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0078 - accuracy: 0.0000e+00\n",
      "Epoch 264/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0085 - accuracy: 0.0000e+00\n",
      "Epoch 265/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0066 - accuracy: 0.0000e+00\n",
      "Epoch 266/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0082 - accuracy: 0.0000e+00\n",
      "Epoch 267/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0072 - accuracy: 0.0000e+00\n",
      "Epoch 268/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0079 - accuracy: 0.0000e+00\n",
      "Epoch 269/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0088 - accuracy: 0.0000e+00\n",
      "Epoch 270/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0081 - accuracy: 0.0000e+00\n",
      "Epoch 271/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 0.0000e+00\n",
      "Epoch 272/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0092 - accuracy: 0.0000e+00\n",
      "Epoch 273/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0061 - accuracy: 0.0000e+00\n",
      "Epoch 274/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0102 - accuracy: 0.0000e+00\n",
      "Epoch 275/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0169 - accuracy: 0.0000e+00\n",
      "Epoch 276/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0131 - accuracy: 0.0000e+00\n",
      "Epoch 277/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0211 - accuracy: 0.0000e+00\n",
      "Epoch 278/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0183 - accuracy: 0.0000e+00\n",
      "Epoch 279/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0091 - accuracy: 0.0000e+00\n",
      "Epoch 280/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0070 - accuracy: 0.0000e+00\n",
      "Epoch 281/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 0.0000e+00\n",
      "Epoch 282/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0078 - accuracy: 0.0000e+00\n",
      "Epoch 283/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0076 - accuracy: 0.0000e+00\n",
      "Epoch 284/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0098 - accuracy: 0.0000e+00\n",
      "Epoch 285/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0064 - accuracy: 0.0000e+00\n",
      "Epoch 286/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0060 - accuracy: 0.0000e+00\n",
      "Epoch 287/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0074 - accuracy: 0.0000e+00\n",
      "Epoch 288/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0051 - accuracy: 0.0000e+00\n",
      "Epoch 289/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0069 - accuracy: 0.0000e+00\n",
      "Epoch 290/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 0.0000e+00\n",
      "Epoch 291/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0087 - accuracy: 0.0000e+00\n",
      "Epoch 292/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0104 - accuracy: 0.0000e+00\n",
      "Epoch 293/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0113 - accuracy: 0.0000e+00\n",
      "Epoch 294/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0057 - accuracy: 0.0000e+00\n",
      "Epoch 295/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0087 - accuracy: 0.0000e+00\n",
      "Epoch 296/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0089 - accuracy: 0.0000e+00\n",
      "Epoch 297/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0088 - accuracy: 0.0000e+00\n",
      "Epoch 298/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0101 - accuracy: 0.0000e+00\n",
      "Epoch 299/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0132 - accuracy: 0.0000e+00\n",
      "Epoch 300/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0117 - accuracy: 0.0000e+00\n",
      "Epoch 301/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0071 - accuracy: 0.0000e+00\n",
      "Epoch 302/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0066 - accuracy: 0.0000e+00\n",
      "Epoch 303/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0054 - accuracy: 0.0000e+00\n",
      "Epoch 304/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0043 - accuracy: 0.0000e+00\n",
      "Epoch 305/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0076 - accuracy: 0.0000e+00\n",
      "Epoch 306/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0063 - accuracy: 0.0000e+00\n",
      "Epoch 307/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0061 - accuracy: 0.0000e+00\n",
      "Epoch 308/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0042 - accuracy: 0.0000e+00\n",
      "Epoch 309/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0067 - accuracy: 0.0000e+00\n",
      "Epoch 310/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0064 - accuracy: 0.0000e+00\n",
      "Epoch 311/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0070 - accuracy: 0.0000e+00\n",
      "Epoch 312/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0053 - accuracy: 0.0000e+00\n",
      "Epoch 313/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0047 - accuracy: 0.0000e+00\n",
      "Epoch 314/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0067 - accuracy: 0.0000e+00\n",
      "Epoch 315/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0075 - accuracy: 0.0000e+00\n",
      "Epoch 316/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0077 - accuracy: 0.0000e+00\n",
      "Epoch 317/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0068 - accuracy: 0.0000e+00\n",
      "Epoch 318/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0081 - accuracy: 0.0000e+00\n",
      "Epoch 319/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0143 - accuracy: 0.0000e+00\n",
      "Epoch 320/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0120 - accuracy: 0.0000e+00\n",
      "Epoch 321/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0125 - accuracy: 0.0000e+00\n",
      "Epoch 322/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0154 - accuracy: 0.0000e+00\n",
      "Epoch 323/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0133 - accuracy: 0.0000e+00\n",
      "Epoch 324/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0092 - accuracy: 0.0000e+00\n",
      "Epoch 325/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0056 - accuracy: 0.0000e+00\n",
      "Epoch 326/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0153 - accuracy: 0.0000e+00\n",
      "Epoch 327/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0108 - accuracy: 0.0000e+00\n",
      "Epoch 328/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0143 - accuracy: 0.0000e+00\n",
      "Epoch 329/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0089 - accuracy: 0.0000e+00\n",
      "Epoch 330/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0117 - accuracy: 0.0000e+00\n",
      "Epoch 331/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0093 - accuracy: 0.0000e+00\n",
      "Epoch 332/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0065 - accuracy: 0.0000e+00\n",
      "Epoch 333/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 0.0000e+00\n",
      "Epoch 334/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0070 - accuracy: 0.0000e+00\n",
      "Epoch 335/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0050 - accuracy: 0.0000e+00\n",
      "Epoch 336/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0059 - accuracy: 0.0000e+00\n",
      "Epoch 337/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0069 - accuracy: 0.0000e+00\n",
      "Epoch 338/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0071 - accuracy: 0.0000e+00\n",
      "Epoch 339/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0055 - accuracy: 0.0000e+00\n",
      "Epoch 340/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0050 - accuracy: 0.0000e+00\n",
      "Epoch 341/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0099 - accuracy: 0.0000e+00\n",
      "Epoch 342/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0105 - accuracy: 0.0000e+00\n",
      "Epoch 343/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0074 - accuracy: 0.0000e+00\n",
      "Epoch 344/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0087 - accuracy: 0.0000e+00\n",
      "Epoch 345/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0073 - accuracy: 0.0000e+00\n",
      "Epoch 346/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0101 - accuracy: 0.0000e+00\n",
      "Epoch 347/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0068 - accuracy: 0.0000e+00\n",
      "Epoch 348/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0047 - accuracy: 0.0000e+00\n",
      "Epoch 349/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 350/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0041 - accuracy: 0.0000e+00\n",
      "Epoch 351/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0077 - accuracy: 0.0000e+00\n",
      "Epoch 352/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0062 - accuracy: 0.0000e+00\n",
      "Epoch 353/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 0.0000e+00\n",
      "Epoch 354/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0075 - accuracy: 0.0000e+00\n",
      "Epoch 355/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0046 - accuracy: 0.0000e+00\n",
      "Epoch 356/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0044 - accuracy: 0.0000e+00\n",
      "Epoch 357/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0046 - accuracy: 0.0000e+00\n",
      "Epoch 358/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0065 - accuracy: 0.0000e+00\n",
      "Epoch 359/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0046 - accuracy: 0.0000e+00\n",
      "Epoch 360/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 361/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0044 - accuracy: 0.0000e+00\n",
      "Epoch 362/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0063 - accuracy: 0.0000e+00\n",
      "Epoch 363/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0078 - accuracy: 0.0000e+00\n",
      "Epoch 364/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 0.0000e+00\n",
      "Epoch 365/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0062 - accuracy: 0.0000e+00\n",
      "Epoch 366/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0136 - accuracy: 0.0000e+00\n",
      "Epoch 367/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0142 - accuracy: 0.0000e+00\n",
      "Epoch 368/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0092 - accuracy: 0.0000e+00\n",
      "Epoch 369/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0089 - accuracy: 0.0000e+00\n",
      "Epoch 370/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0081 - accuracy: 0.0000e+00\n",
      "Epoch 371/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0086 - accuracy: 0.0000e+00\n",
      "Epoch 372/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0047 - accuracy: 0.0000e+00\n",
      "Epoch 373/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0044 - accuracy: 0.0000e+00\n",
      "Epoch 374/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0054 - accuracy: 0.0000e+00\n",
      "Epoch 375/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0051 - accuracy: 0.0000e+00\n",
      "Epoch 376/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0079 - accuracy: 0.0000e+00\n",
      "Epoch 377/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0074 - accuracy: 0.0000e+00\n",
      "Epoch 378/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0046 - accuracy: 0.0000e+00\n",
      "Epoch 379/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 380/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 381/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0104 - accuracy: 0.0000e+00\n",
      "Epoch 382/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0072 - accuracy: 0.0000e+00\n",
      "Epoch 383/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0047 - accuracy: 0.0000e+00\n",
      "Epoch 384/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0044 - accuracy: 0.0000e+00\n",
      "Epoch 385/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0047 - accuracy: 0.0000e+00\n",
      "Epoch 386/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0071 - accuracy: 0.0000e+00\n",
      "Epoch 387/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0049 - accuracy: 0.0000e+00\n",
      "Epoch 388/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0064 - accuracy: 0.0000e+00\n",
      "Epoch 389/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0049 - accuracy: 0.0000e+00\n",
      "Epoch 390/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0048 - accuracy: 0.0000e+00\n",
      "Epoch 391/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0059 - accuracy: 0.0000e+00\n",
      "Epoch 392/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0058 - accuracy: 0.0000e+00\n",
      "Epoch 393/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0069 - accuracy: 0.0000e+00\n",
      "Epoch 394/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0054 - accuracy: 0.0000e+00\n",
      "Epoch 395/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0045 - accuracy: 0.0000e+00\n",
      "Epoch 396/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0046 - accuracy: 0.0000e+00\n",
      "Epoch 397/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 398/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0028 - accuracy: 0.0000e+00\n",
      "Epoch 399/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 400/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 401/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0061 - accuracy: 0.0000e+00\n",
      "Epoch 402/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0113 - accuracy: 0.0000e+00\n",
      "Epoch 403/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0369 - accuracy: 0.0000e+00\n",
      "Epoch 404/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0354 - accuracy: 0.0000e+00\n",
      "Epoch 405/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0111 - accuracy: 0.0000e+00\n",
      "Epoch 406/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0068 - accuracy: 0.0000e+00\n",
      "Epoch 407/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0040 - accuracy: 0.0000e+00\n",
      "Epoch 408/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0028 - accuracy: 0.0000e+00\n",
      "Epoch 409/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0024 - accuracy: 0.0000e+00\n",
      "Epoch 410/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0028 - accuracy: 0.0000e+00\n",
      "Epoch 411/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 412/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0030 - accuracy: 0.0000e+00\n",
      "Epoch 413/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0047 - accuracy: 0.0000e+00\n",
      "Epoch 414/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 415/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0028 - accuracy: 0.0000e+00\n",
      "Epoch 416/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 417/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0056 - accuracy: 0.0000e+00\n",
      "Epoch 418/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0042 - accuracy: 0.0000e+00\n",
      "Epoch 419/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 420/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 421/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 422/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 423/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0051 - accuracy: 0.0000e+00\n",
      "Epoch 424/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0050 - accuracy: 0.0000e+00\n",
      "Epoch 425/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0063 - accuracy: 0.0000e+00\n",
      "Epoch 426/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0056 - accuracy: 0.0000e+00\n",
      "Epoch 427/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0088 - accuracy: 0.0000e+00\n",
      "Epoch 428/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0085 - accuracy: 0.0000e+00\n",
      "Epoch 429/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0072 - accuracy: 0.0000e+00\n",
      "Epoch 430/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0053 - accuracy: 0.0000e+00\n",
      "Epoch 431/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0041 - accuracy: 0.0000e+00\n",
      "Epoch 432/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 433/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0043 - accuracy: 0.0000e+00\n",
      "Epoch 434/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0027 - accuracy: 0.0000e+00\n",
      "Epoch 435/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 436/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0031 - accuracy: 0.0000e+00\n",
      "Epoch 437/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0039 - accuracy: 0.0000e+00\n",
      "Epoch 438/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0047 - accuracy: 0.0000e+00\n",
      "Epoch 439/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0058 - accuracy: 0.0000e+00\n",
      "Epoch 440/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0045 - accuracy: 0.0000e+00\n",
      "Epoch 441/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0089 - accuracy: 0.0000e+00\n",
      "Epoch 442/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0154 - accuracy: 0.0000e+00\n",
      "Epoch 443/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0110 - accuracy: 0.0000e+00\n",
      "Epoch 444/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0101 - accuracy: 0.0000e+00\n",
      "Epoch 445/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0084 - accuracy: 0.0000e+00\n",
      "Epoch 446/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0050 - accuracy: 0.0000e+00\n",
      "Epoch 447/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 448/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0046 - accuracy: 0.0000e+00\n",
      "Epoch 449/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0030 - accuracy: 0.0000e+00\n",
      "Epoch 450/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0029 - accuracy: 0.0000e+00\n",
      "Epoch 451/1000\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.0020 - accuracy: 0.0000e+00\n",
      "Epoch 452/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0025 - accuracy: 0.0000e+00\n",
      "Epoch 453/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0029 - accuracy: 0.0000e+00\n",
      "Epoch 454/1000\n",
      "115/115 [==============================] - 1s 7ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 455/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 1s 6ms/step - loss: 0.0046 - accuracy: 0.0000e+00\n",
      "Epoch 456/1000\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 457/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0042 - accuracy: 0.0000e+00\n",
      "Epoch 458/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0106 - accuracy: 0.0000e+00\n",
      "Epoch 459/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0105 - accuracy: 0.0000e+00\n",
      "Epoch 460/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0107 - accuracy: 0.0000e+00\n",
      "Epoch 461/1000\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.0076 - accuracy: 0.0000e+00\n",
      "Epoch 462/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0056 - accuracy: 0.0000e+00\n",
      "Epoch 463/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0048 - accuracy: 0.0000e+00\n",
      "Epoch 464/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0044 - accuracy: 0.0000e+00\n",
      "Epoch 465/1000\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 466/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0038 - accuracy: 0.0000e+00\n",
      "Epoch 467/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 468/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0044 - accuracy: 0.0000e+00\n",
      "Epoch 469/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0041 - accuracy: 0.0000e+00\n",
      "Epoch 470/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 471/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0023 - accuracy: 0.0000e+00\n",
      "Epoch 472/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 473/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0048 - accuracy: 0.0000e+00\n",
      "Epoch 474/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0075 - accuracy: 0.0000e+00\n",
      "Epoch 475/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0076 - accuracy: 0.0000e+00\n",
      "Epoch 476/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0091 - accuracy: 0.0000e+00\n",
      "Epoch 477/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0120 - accuracy: 0.0000e+00\n",
      "Epoch 478/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0112 - accuracy: 0.0000e+00\n",
      "Epoch 479/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0076 - accuracy: 0.0000e+00\n",
      "Epoch 480/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0081 - accuracy: 0.0000e+00\n",
      "Epoch 481/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0051 - accuracy: 0.0000e+00\n",
      "Epoch 482/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0051 - accuracy: 0.0000e+00\n",
      "Epoch 483/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0039 - accuracy: 0.0000e+00\n",
      "Epoch 484/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0041 - accuracy: 0.0000e+00\n",
      "Epoch 485/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0038 - accuracy: 0.0000e+00\n",
      "Epoch 486/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 487/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0054 - accuracy: 0.0000e+00\n",
      "Epoch 488/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 489/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 490/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0027 - accuracy: 0.0000e+00\n",
      "Epoch 491/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 492/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0063 - accuracy: 0.0000e+00\n",
      "Epoch 493/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0065 - accuracy: 0.0000e+00\n",
      "Epoch 494/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0129 - accuracy: 0.0000e+00\n",
      "Epoch 495/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0187 - accuracy: 0.0000e+00\n",
      "Epoch 496/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0161 - accuracy: 0.0000e+00\n",
      "Epoch 497/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0209 - accuracy: 0.0000e+00\n",
      "Epoch 498/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0150 - accuracy: 0.0000e+00\n",
      "Epoch 499/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0065 - accuracy: 0.0000e+00\n",
      "Epoch 500/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0031 - accuracy: 0.0000e+00\n",
      "Epoch 501/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0030 - accuracy: 0.0000e+00\n",
      "Epoch 502/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0027 - accuracy: 0.0000e+00\n",
      "Epoch 503/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0020 - accuracy: 0.0000e+00\n",
      "Epoch 504/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0024 - accuracy: 0.0000e+00\n",
      "Epoch 505/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0024 - accuracy: 0.0000e+00\n",
      "Epoch 506/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0021 - accuracy: 0.0000e+00\n",
      "Epoch 507/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0031 - accuracy: 0.0000e+00\n",
      "Epoch 508/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0018 - accuracy: 0.0000e+00\n",
      "Epoch 509/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0023 - accuracy: 0.0000e+00\n",
      "Epoch 510/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0029 - accuracy: 0.0000e+00\n",
      "Epoch 511/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0027 - accuracy: 0.0000e+00\n",
      "Epoch 512/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0024 - accuracy: 0.0000e+00\n",
      "Epoch 513/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0028 - accuracy: 0.0000e+00\n",
      "Epoch 514/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0051 - accuracy: 0.0000e+00\n",
      "Epoch 515/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0029 - accuracy: 0.0000e+00\n",
      "Epoch 516/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0038 - accuracy: 0.0000e+00\n",
      "Epoch 517/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0051 - accuracy: 0.0000e+00\n",
      "Epoch 518/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0095 - accuracy: 0.0000e+00\n",
      "Epoch 519/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0103 - accuracy: 0.0000e+00\n",
      "Epoch 520/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0101 - accuracy: 0.0000e+00\n",
      "Epoch 521/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0067 - accuracy: 0.0000e+00\n",
      "Epoch 522/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0081 - accuracy: 0.0000e+00\n",
      "Epoch 523/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 524/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 525/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0028 - accuracy: 0.0000e+00\n",
      "Epoch 526/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0030 - accuracy: 0.0000e+00\n",
      "Epoch 527/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 528/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0039 - accuracy: 0.0000e+00\n",
      "Epoch 529/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0023 - accuracy: 0.0000e+00\n",
      "Epoch 530/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0043 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 531/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0045 - accuracy: 0.0000e+00\n",
      "Epoch 532/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0054 - accuracy: 0.0000e+00\n",
      "Epoch 533/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 534/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0038 - accuracy: 0.0000e+00\n",
      "Epoch 535/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0040 - accuracy: 0.0000e+00\n",
      "Epoch 536/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0098 - accuracy: 0.0000e+00\n",
      "Epoch 537/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0075 - accuracy: 0.0000e+00\n",
      "Epoch 538/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0085 - accuracy: 0.0000e+00\n",
      "Epoch 539/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0109 - accuracy: 0.0000e+00\n",
      "Epoch 540/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0128 - accuracy: 0.0000e+00\n",
      "Epoch 541/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0093 - accuracy: 0.0000e+00\n",
      "Epoch 542/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0129 - accuracy: 0.0000e+00\n",
      "Epoch 543/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0074 - accuracy: 0.0000e+00\n",
      "Epoch 544/1000\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.0039 - accuracy: 0.0000e+00\n",
      "Epoch 545/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0029 - accuracy: 0.0000e+00\n",
      "Epoch 546/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 547/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 548/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0038 - accuracy: 0.0000e+00\n",
      "Epoch 549/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0039 - accuracy: 0.0000e+00\n",
      "Epoch 550/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0031 - accuracy: 0.0000e+00\n",
      "Epoch 551/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0029 - accuracy: 0.0000e+00\n",
      "Epoch 552/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0024 - accuracy: 0.0000e+00\n",
      "Epoch 553/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0028 - accuracy: 0.0000e+00\n",
      "Epoch 554/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0021 - accuracy: 0.0000e+00\n",
      "Epoch 555/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 556/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0051 - accuracy: 0.0000e+00\n",
      "Epoch 557/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0039 - accuracy: 0.0000e+00\n",
      "Epoch 558/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0042 - accuracy: 0.0000e+00\n",
      "Epoch 559/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0052 - accuracy: 0.0000e+00\n",
      "Epoch 560/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 561/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0031 - accuracy: 0.0000e+00\n",
      "Epoch 562/1000\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.0045 - accuracy: 0.0000e+00\n",
      "Epoch 563/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0048 - accuracy: 0.0000e+00\n",
      "Epoch 564/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0055 - accuracy: 0.0000e+00\n",
      "Epoch 565/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0104 - accuracy: 0.0000e+00\n",
      "Epoch 566/1000\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.0067 - accuracy: 0.0000e+00\n",
      "Epoch 567/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 568/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0041 - accuracy: 0.0000e+00\n",
      "Epoch 569/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0031 - accuracy: 0.0000e+00\n",
      "Epoch 570/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0042 - accuracy: 0.0000e+00\n",
      "Epoch 571/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0027 - accuracy: 0.0000e+00\n",
      "Epoch 572/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 573/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 574/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 575/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0024 - accuracy: 0.0000e+00\n",
      "Epoch 576/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 577/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0045 - accuracy: 0.0000e+00\n",
      "Epoch 578/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0038 - accuracy: 0.0000e+00\n",
      "Epoch 579/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0048 - accuracy: 0.0000e+00\n",
      "Epoch 580/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0059 - accuracy: 0.0000e+00\n",
      "Epoch 581/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0045 - accuracy: 0.0000e+00\n",
      "Epoch 582/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0051 - accuracy: 0.0000e+00\n",
      "Epoch 583/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 584/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0040 - accuracy: 0.0000e+00\n",
      "Epoch 585/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0026 - accuracy: 0.0000e+00\n",
      "Epoch 586/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0041 - accuracy: 0.0000e+00\n",
      "Epoch 587/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0045 - accuracy: 0.0000e+00\n",
      "Epoch 588/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0047 - accuracy: 0.0000e+00\n",
      "Epoch 589/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0030 - accuracy: 0.0000e+00\n",
      "Epoch 590/1000\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.0044 - accuracy: 0.0000e+00\n",
      "Epoch 591/1000\n",
      "115/115 [==============================] - 1s 7ms/step - loss: 0.0031 - accuracy: 0.0000e+00\n",
      "Epoch 592/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 593/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0026 - accuracy: 0.0000e+00\n",
      "Epoch 594/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0025 - accuracy: 0.0000e+00\n",
      "Epoch 595/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0039 - accuracy: 0.0000e+00\n",
      "Epoch 596/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0031 - accuracy: 0.0000e+00\n",
      "Epoch 597/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0030 - accuracy: 0.0000e+00\n",
      "Epoch 598/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0062 - accuracy: 0.0000e+00\n",
      "Epoch 599/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0065 - accuracy: 0.0000e+00\n",
      "Epoch 600/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0072 - accuracy: 0.0000e+00\n",
      "Epoch 601/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0059 - accuracy: 0.0000e+00\n",
      "Epoch 602/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0043 - accuracy: 0.0000e+00\n",
      "Epoch 603/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0047 - accuracy: 0.0000e+00\n",
      "Epoch 604/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 605/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0026 - accuracy: 0.0000e+00\n",
      "Epoch 606/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0049 - accuracy: 0.0000e+00\n",
      "Epoch 607/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0060 - accuracy: 0.0000e+00\n",
      "Epoch 608/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 609/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0031 - accuracy: 0.0000e+00\n",
      "Epoch 610/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0022 - accuracy: 0.0000e+00\n",
      "Epoch 611/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0025 - accuracy: 0.0000e+00\n",
      "Epoch 612/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0026 - accuracy: 0.0000e+00\n",
      "Epoch 613/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0030 - accuracy: 0.0000e+00\n",
      "Epoch 614/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0053 - accuracy: 0.0000e+00\n",
      "Epoch 615/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0053 - accuracy: 0.0000e+00\n",
      "Epoch 616/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0047 - accuracy: 0.0000e+00\n",
      "Epoch 617/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0049 - accuracy: 0.0000e+00\n",
      "Epoch 618/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0081 - accuracy: 0.0000e+00\n",
      "Epoch 619/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0071 - accuracy: 0.0000e+00\n",
      "Epoch 620/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0083 - accuracy: 0.0000e+00\n",
      "Epoch 621/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0059 - accuracy: 0.0000e+00\n",
      "Epoch 622/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0062 - accuracy: 0.0000e+00\n",
      "Epoch 623/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0062 - accuracy: 0.0000e+00\n",
      "Epoch 624/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0039 - accuracy: 0.0000e+00\n",
      "Epoch 625/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0028 - accuracy: 0.0000e+00\n",
      "Epoch 626/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0021 - accuracy: 0.0000e+00\n",
      "Epoch 627/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0030 - accuracy: 0.0000e+00\n",
      "Epoch 628/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0027 - accuracy: 0.0000e+00\n",
      "Epoch 629/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0023 - accuracy: 0.0000e+00\n",
      "Epoch 630/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0018 - accuracy: 0.0000e+00\n",
      "Epoch 631/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0022 - accuracy: 0.0000e+00\n",
      "Epoch 632/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0028 - accuracy: 0.0000e+00\n",
      "Epoch 633/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0027 - accuracy: 0.0000e+00\n",
      "Epoch 634/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0071 - accuracy: 0.0000e+00\n",
      "Epoch 635/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0046 - accuracy: 0.0000e+00\n",
      "Epoch 636/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 637/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 638/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0053 - accuracy: 0.0000e+00\n",
      "Epoch 639/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 640/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0044 - accuracy: 0.0000e+00\n",
      "Epoch 641/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 642/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 643/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0040 - accuracy: 0.0000e+00\n",
      "Epoch 644/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 645/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 646/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 647/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 648/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 649/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0047 - accuracy: 0.0000e+00\n",
      "Epoch 650/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0040 - accuracy: 0.0000e+00\n",
      "Epoch 651/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 652/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0028 - accuracy: 0.0000e+00\n",
      "Epoch 653/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0027 - accuracy: 0.0000e+00\n",
      "Epoch 654/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0030 - accuracy: 0.0000e+00\n",
      "Epoch 655/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 656/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0040 - accuracy: 0.0000e+00\n",
      "Epoch 657/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0050 - accuracy: 0.0000e+00\n",
      "Epoch 658/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0025 - accuracy: 0.0000e+00\n",
      "Epoch 659/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 660/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 661/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0051 - accuracy: 0.0000e+00\n",
      "Epoch 662/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0102 - accuracy: 0.0000e+00\n",
      "Epoch 663/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0086 - accuracy: 0.0000e+00\n",
      "Epoch 664/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0083 - accuracy: 0.0000e+00\n",
      "Epoch 665/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0045 - accuracy: 0.0000e+00\n",
      "Epoch 666/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0031 - accuracy: 0.0000e+00\n",
      "Epoch 667/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0018 - accuracy: 0.0000e+00\n",
      "Epoch 668/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 669/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0044 - accuracy: 0.0000e+00\n",
      "Epoch 670/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0023 - accuracy: 0.0000e+00\n",
      "Epoch 671/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0031 - accuracy: 0.0000e+00\n",
      "Epoch 672/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0026 - accuracy: 0.0000e+00\n",
      "Epoch 673/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 674/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0027 - accuracy: 0.0000e+00\n",
      "Epoch 675/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0025 - accuracy: 0.0000e+00\n",
      "Epoch 676/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0024 - accuracy: 0.0000e+00\n",
      "Epoch 677/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0026 - accuracy: 0.0000e+00\n",
      "Epoch 678/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0023 - accuracy: 0.0000e+00\n",
      "Epoch 679/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0030 - accuracy: 0.0000e+00\n",
      "Epoch 680/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0029 - accuracy: 0.0000e+00\n",
      "Epoch 681/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0024 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 682/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0030 - accuracy: 0.0000e+00\n",
      "Epoch 683/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0018 - accuracy: 0.0000e+00\n",
      "Epoch 684/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0066 - accuracy: 0.0000e+00\n",
      "Epoch 685/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0056 - accuracy: 0.0000e+00\n",
      "Epoch 686/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0039 - accuracy: 0.0000e+00\n",
      "Epoch 687/1000\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.0024 - accuracy: 0.0000e+00\n",
      "Epoch 688/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0027 - accuracy: 0.0000e+00\n",
      "Epoch 689/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0022 - accuracy: 0.0000e+00\n",
      "Epoch 690/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 691/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0047 - accuracy: 0.0000e+00\n",
      "Epoch 692/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0048 - accuracy: 0.0000e+00\n",
      "Epoch 693/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0051 - accuracy: 0.0000e+00\n",
      "Epoch 694/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0045 - accuracy: 0.0000e+00\n",
      "Epoch 695/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 696/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0040 - accuracy: 0.0000e+00\n",
      "Epoch 697/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 698/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0049 - accuracy: 0.0000e+00\n",
      "Epoch 699/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0042 - accuracy: 0.0000e+00\n",
      "Epoch 700/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0029 - accuracy: 0.0000e+00\n",
      "Epoch 701/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 702/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0043 - accuracy: 0.0000e+00\n",
      "Epoch 703/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0029 - accuracy: 0.0000e+00\n",
      "Epoch 704/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0031 - accuracy: 0.0000e+00\n",
      "Epoch 705/1000\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.0026 - accuracy: 0.0000e+00\n",
      "Epoch 706/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0045 - accuracy: 0.0000e+00\n",
      "Epoch 707/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0054 - accuracy: 0.0000e+00\n",
      "Epoch 708/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0028 - accuracy: 0.0000e+00\n",
      "Epoch 709/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0021 - accuracy: 0.0000e+00\n",
      "Epoch 710/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0016 - accuracy: 0.0000e+00\n",
      "Epoch 711/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0015 - accuracy: 0.0000e+00\n",
      "Epoch 712/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0027 - accuracy: 0.0000e+00\n",
      "Epoch 713/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0031 - accuracy: 0.0000e+00\n",
      "Epoch 714/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0027 - accuracy: 0.0000e+00\n",
      "Epoch 715/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0024 - accuracy: 0.0000e+00\n",
      "Epoch 716/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 0.0000e+00\n",
      "Epoch 717/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0039 - accuracy: 0.0000e+00\n",
      "Epoch 718/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0067 - accuracy: 0.0000e+00\n",
      "Epoch 719/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0079 - accuracy: 0.0000e+00\n",
      "Epoch 720/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0124 - accuracy: 0.0000e+00\n",
      "Epoch 721/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0151 - accuracy: 0.0000e+00\n",
      "Epoch 722/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0090 - accuracy: 0.0000e+00\n",
      "Epoch 723/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0054 - accuracy: 0.0000e+00\n",
      "Epoch 724/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0038 - accuracy: 0.0000e+00\n",
      "Epoch 725/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0039 - accuracy: 0.0000e+00\n",
      "Epoch 726/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 727/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0027 - accuracy: 0.0000e+00\n",
      "Epoch 728/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 729/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0025 - accuracy: 0.0000e+00\n",
      "Epoch 730/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0028 - accuracy: 0.0000e+00\n",
      "Epoch 731/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0025 - accuracy: 0.0000e+00\n",
      "Epoch 732/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0026 - accuracy: 0.0000e+00\n",
      "Epoch 733/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0025 - accuracy: 0.0000e+00\n",
      "Epoch 734/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0018 - accuracy: 0.0000e+00\n",
      "Epoch 735/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0016 - accuracy: 0.0000e+00\n",
      "Epoch 736/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0018 - accuracy: 0.0000e+00\n",
      "Epoch 737/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0018 - accuracy: 0.0000e+00\n",
      "Epoch 738/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0017 - accuracy: 0.0000e+00\n",
      "Epoch 739/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0028 - accuracy: 0.0000e+00\n",
      "Epoch 740/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0031 - accuracy: 0.0000e+00\n",
      "Epoch 741/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0028 - accuracy: 0.0000e+00\n",
      "Epoch 742/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0021 - accuracy: 0.0000e+00\n",
      "Epoch 743/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0031 - accuracy: 0.0000e+00\n",
      "Epoch 744/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0047 - accuracy: 0.0000e+00\n",
      "Epoch 745/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0025 - accuracy: 0.0000e+00\n",
      "Epoch 746/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 747/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0027 - accuracy: 0.0000e+00\n",
      "Epoch 748/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0039 - accuracy: 0.0000e+00\n",
      "Epoch 749/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 750/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 751/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0061 - accuracy: 0.0000e+00\n",
      "Epoch 752/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0078 - accuracy: 0.0000e+00\n",
      "Epoch 753/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0078 - accuracy: 0.0000e+00\n",
      "Epoch 754/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0083 - accuracy: 0.0000e+00\n",
      "Epoch 755/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0048 - accuracy: 0.0000e+00\n",
      "Epoch 756/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0063 - accuracy: 0.0000e+00\n",
      "Epoch 757/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 758/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0026 - accuracy: 0.0000e+00\n",
      "Epoch 759/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0030 - accuracy: 0.0000e+00\n",
      "Epoch 760/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0025 - accuracy: 0.0000e+00\n",
      "Epoch 761/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0024 - accuracy: 0.0000e+00\n",
      "Epoch 762/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0029 - accuracy: 0.0000e+00\n",
      "Epoch 763/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0022 - accuracy: 0.0000e+00\n",
      "Epoch 764/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 765/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0024 - accuracy: 0.0000e+00\n",
      "Epoch 766/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0022 - accuracy: 0.0000e+00\n",
      "Epoch 767/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0018 - accuracy: 0.0000e+00\n",
      "Epoch 768/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0014 - accuracy: 0.0000e+00\n",
      "Epoch 769/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0023 - accuracy: 0.0000e+00\n",
      "Epoch 770/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0026 - accuracy: 0.0000e+00\n",
      "Epoch 771/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0027 - accuracy: 0.0000e+00\n",
      "Epoch 772/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0029 - accuracy: 0.0000e+00\n",
      "Epoch 773/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0044 - accuracy: 0.0000e+00\n",
      "Epoch 774/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 775/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 776/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0039 - accuracy: 0.0000e+00\n",
      "Epoch 777/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0051 - accuracy: 0.0000e+00\n",
      "Epoch 778/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0063 - accuracy: 0.0000e+00\n",
      "Epoch 779/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0118 - accuracy: 0.0000e+00\n",
      "Epoch 780/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 0.0000e+00\n",
      "Epoch 781/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0053 - accuracy: 0.0000e+00\n",
      "Epoch 782/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0029 - accuracy: 0.0000e+00\n",
      "Epoch 783/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 784/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0022 - accuracy: 0.0000e+00\n",
      "Epoch 785/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0026 - accuracy: 0.0000e+00\n",
      "Epoch 786/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0027 - accuracy: 0.0000e+00\n",
      "Epoch 787/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0017 - accuracy: 0.0000e+00\n",
      "Epoch 788/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0018 - accuracy: 0.0000e+00\n",
      "Epoch 789/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0014 - accuracy: 0.0000e+00\n",
      "Epoch 790/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0014 - accuracy: 0.0000e+00\n",
      "Epoch 791/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0021 - accuracy: 0.0000e+00\n",
      "Epoch 792/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 0.0000e+00\n",
      "Epoch 793/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0023 - accuracy: 0.0000e+00\n",
      "Epoch 794/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0029 - accuracy: 0.0000e+00\n",
      "Epoch 795/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0022 - accuracy: 0.0000e+00\n",
      "Epoch 796/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0024 - accuracy: 0.0000e+00\n",
      "Epoch 797/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0026 - accuracy: 0.0000e+00\n",
      "Epoch 798/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0022 - accuracy: 0.0000e+00\n",
      "Epoch 799/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0079 - accuracy: 0.0000e+00\n",
      "Epoch 800/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0057 - accuracy: 0.0000e+00\n",
      "Epoch 801/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0093 - accuracy: 0.0000e+00\n",
      "Epoch 802/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0086 - accuracy: 0.0000e+00\n",
      "Epoch 803/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0063 - accuracy: 0.0000e+00\n",
      "Epoch 804/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 805/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0028 - accuracy: 0.0000e+00\n",
      "Epoch 806/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0024 - accuracy: 0.0000e+00\n",
      "Epoch 807/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0018 - accuracy: 0.0000e+00\n",
      "Epoch 808/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0020 - accuracy: 0.0000e+00\n",
      "Epoch 809/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0020 - accuracy: 0.0000e+00\n",
      "Epoch 810/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0019 - accuracy: 0.0000e+00\n",
      "Epoch 811/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0018 - accuracy: 0.0000e+00\n",
      "Epoch 812/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0028 - accuracy: 0.0000e+00\n",
      "Epoch 813/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0020 - accuracy: 0.0000e+00\n",
      "Epoch 814/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0018 - accuracy: 0.0000e+00\n",
      "Epoch 815/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0023 - accuracy: 0.0000e+00\n",
      "Epoch 816/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 817/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0028 - accuracy: 0.0000e+00\n",
      "Epoch 818/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0038 - accuracy: 0.0000e+00\n",
      "Epoch 819/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 820/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0028 - accuracy: 0.0000e+00\n",
      "Epoch 821/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 822/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 823/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0045 - accuracy: 0.0000e+00\n",
      "Epoch 824/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0021 - accuracy: 0.0000e+00\n",
      "Epoch 825/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0021 - accuracy: 0.0000e+00\n",
      "Epoch 826/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 0.0000e+00\n",
      "Epoch 827/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0023 - accuracy: 0.0000e+00\n",
      "Epoch 828/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0021 - accuracy: 0.0000e+00\n",
      "Epoch 829/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0016 - accuracy: 0.0000e+00\n",
      "Epoch 830/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0015 - accuracy: 0.0000e+00\n",
      "Epoch 831/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0015 - accuracy: 0.0000e+00\n",
      "Epoch 832/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 833/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0020 - accuracy: 0.0000e+00\n",
      "Epoch 834/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0024 - accuracy: 0.0000e+00\n",
      "Epoch 835/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0029 - accuracy: 0.0000e+00\n",
      "Epoch 836/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0027 - accuracy: 0.0000e+00\n",
      "Epoch 837/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0031 - accuracy: 0.0000e+00\n",
      "Epoch 838/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0023 - accuracy: 0.0000e+00\n",
      "Epoch 839/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 840/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0046 - accuracy: 0.0000e+00\n",
      "Epoch 841/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 842/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 843/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0030 - accuracy: 0.0000e+00\n",
      "Epoch 844/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0022 - accuracy: 0.0000e+00\n",
      "Epoch 845/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0024 - accuracy: 0.0000e+00\n",
      "Epoch 846/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0028 - accuracy: 0.0000e+00\n",
      "Epoch 847/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0049 - accuracy: 0.0000e+00\n",
      "Epoch 848/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0051 - accuracy: 0.0000e+00\n",
      "Epoch 849/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0071 - accuracy: 0.0000e+00\n",
      "Epoch 850/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0050 - accuracy: 0.0000e+00\n",
      "Epoch 851/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0029 - accuracy: 0.0000e+00\n",
      "Epoch 852/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 853/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 854/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0029 - accuracy: 0.0000e+00\n",
      "Epoch 855/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 856/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0021 - accuracy: 0.0000e+00\n",
      "Epoch 857/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 858/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0018 - accuracy: 0.0000e+00\n",
      "Epoch 859/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 0.0000e+00\n",
      "Epoch 860/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0015 - accuracy: 0.0000e+00\n",
      "Epoch 861/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0017 - accuracy: 0.0000e+00\n",
      "Epoch 862/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0027 - accuracy: 0.0000e+00\n",
      "Epoch 863/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0031 - accuracy: 0.0000e+00\n",
      "Epoch 864/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0027 - accuracy: 0.0000e+00\n",
      "Epoch 865/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0025 - accuracy: 0.0000e+00\n",
      "Epoch 866/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0026 - accuracy: 0.0000e+00\n",
      "Epoch 867/1000\n",
      "115/115 [==============================] - 1s 6ms/step - loss: 0.0025 - accuracy: 0.0000e+00\n",
      "Epoch 868/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0021 - accuracy: 0.0000e+00\n",
      "Epoch 869/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0021 - accuracy: 0.0000e+00\n",
      "Epoch 870/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0027 - accuracy: 0.0000e+00\n",
      "Epoch 871/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0028 - accuracy: 0.0000e+00\n",
      "Epoch 872/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0028 - accuracy: 0.0000e+00\n",
      "Epoch 873/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0018 - accuracy: 0.0000e+00\n",
      "Epoch 874/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0026 - accuracy: 0.0000e+00\n",
      "Epoch 875/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0020 - accuracy: 0.0000e+00\n",
      "Epoch 876/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0022 - accuracy: 0.0000e+00\n",
      "Epoch 877/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0021 - accuracy: 0.0000e+00\n",
      "Epoch 878/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0024 - accuracy: 0.0000e+00\n",
      "Epoch 879/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0030 - accuracy: 0.0000e+00\n",
      "Epoch 880/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0044 - accuracy: 0.0000e+00\n",
      "Epoch 881/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0061 - accuracy: 0.0000e+00\n",
      "Epoch 882/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 883/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 884/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0038 - accuracy: 0.0000e+00\n",
      "Epoch 885/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0021 - accuracy: 0.0000e+00\n",
      "Epoch 886/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0025 - accuracy: 0.0000e+00\n",
      "Epoch 887/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0017 - accuracy: 0.0000e+00\n",
      "Epoch 888/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0020 - accuracy: 0.0000e+00\n",
      "Epoch 889/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0026 - accuracy: 0.0000e+00\n",
      "Epoch 890/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0021 - accuracy: 0.0000e+00\n",
      "Epoch 891/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0024 - accuracy: 0.0000e+00\n",
      "Epoch 892/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 893/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0020 - accuracy: 0.0000e+00\n",
      "Epoch 894/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0051 - accuracy: 0.0000e+00\n",
      "Epoch 895/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0072 - accuracy: 0.0000e+00\n",
      "Epoch 896/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0050 - accuracy: 0.0000e+00\n",
      "Epoch 897/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0054 - accuracy: 0.0000e+00\n",
      "Epoch 898/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0051 - accuracy: 0.0000e+00\n",
      "Epoch 899/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0068 - accuracy: 0.0000e+00\n",
      "Epoch 900/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0061 - accuracy: 0.0000e+00\n",
      "Epoch 901/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0040 - accuracy: 0.0000e+00\n",
      "Epoch 902/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0062 - accuracy: 0.0000e+00\n",
      "Epoch 903/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0041 - accuracy: 0.0000e+00\n",
      "Epoch 904/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 905/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0044 - accuracy: 0.0000e+00\n",
      "Epoch 906/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 907/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0050 - accuracy: 0.0000e+00\n",
      "Epoch 908/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0021 - accuracy: 0.0000e+00\n",
      "Epoch 909/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0069 - accuracy: 0.0000e+00\n",
      "Epoch 910/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 911/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0044 - accuracy: 0.0000e+00\n",
      "Epoch 912/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0031 - accuracy: 0.0000e+00\n",
      "Epoch 913/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0021 - accuracy: 0.0000e+00\n",
      "Epoch 914/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0024 - accuracy: 0.0000e+00\n",
      "Epoch 915/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0023 - accuracy: 0.0000e+00\n",
      "Epoch 916/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0027 - accuracy: 0.0000e+00\n",
      "Epoch 917/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 0.0000e+00\n",
      "Epoch 918/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 919/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0023 - accuracy: 0.0000e+00\n",
      "Epoch 920/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0023 - accuracy: 0.0000e+00\n",
      "Epoch 921/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0024 - accuracy: 0.0000e+00\n",
      "Epoch 922/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0016 - accuracy: 0.0000e+00\n",
      "Epoch 923/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0017 - accuracy: 0.0000e+00\n",
      "Epoch 924/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0016 - accuracy: 0.0000e+00\n",
      "Epoch 925/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 926/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 0.0000e+00\n",
      "Epoch 927/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0031 - accuracy: 0.0000e+00\n",
      "Epoch 928/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0026 - accuracy: 0.0000e+00\n",
      "Epoch 929/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0020 - accuracy: 0.0000e+00\n",
      "Epoch 930/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0029 - accuracy: 0.0000e+00\n",
      "Epoch 931/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0027 - accuracy: 0.0000e+00\n",
      "Epoch 932/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0023 - accuracy: 0.0000e+00\n",
      "Epoch 933/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0024 - accuracy: 0.0000e+00\n",
      "Epoch 934/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0025 - accuracy: 0.0000e+00\n",
      "Epoch 935/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0055 - accuracy: 0.0000e+00\n",
      "Epoch 936/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0094 - accuracy: 0.0000e+00\n",
      "Epoch 937/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0140 - accuracy: 0.0000e+00\n",
      "Epoch 938/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0070 - accuracy: 0.0000e+00\n",
      "Epoch 939/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0039 - accuracy: 0.0000e+00\n",
      "Epoch 940/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0027 - accuracy: 0.0000e+00\n",
      "Epoch 941/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0021 - accuracy: 0.0000e+00\n",
      "Epoch 942/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0023 - accuracy: 0.0000e+00\n",
      "Epoch 943/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0030 - accuracy: 0.0000e+00\n",
      "Epoch 944/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 0.0000e+00\n",
      "Epoch 945/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0017 - accuracy: 0.0000e+00\n",
      "Epoch 946/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0018 - accuracy: 0.0000e+00\n",
      "Epoch 947/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0014 - accuracy: 0.0000e+00\n",
      "Epoch 948/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0031 - accuracy: 0.0000e+00\n",
      "Epoch 949/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0020 - accuracy: 0.0000e+00\n",
      "Epoch 950/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0020 - accuracy: 0.0000e+00\n",
      "Epoch 951/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0023 - accuracy: 0.0000e+00\n",
      "Epoch 952/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0018 - accuracy: 0.0000e+00\n",
      "Epoch 953/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0020 - accuracy: 0.0000e+00\n",
      "Epoch 954/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0016 - accuracy: 0.0000e+00\n",
      "Epoch 955/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0012 - accuracy: 0.0000e+00\n",
      "Epoch 956/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0017 - accuracy: 0.0000e+00\n",
      "Epoch 957/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0025 - accuracy: 0.0000e+00\n",
      "Epoch 958/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0018 - accuracy: 0.0000e+00\n",
      "Epoch 959/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0018 - accuracy: 0.0000e+00\n",
      "Epoch 960/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0023 - accuracy: 0.0000e+00\n",
      "Epoch 961/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0018 - accuracy: 0.0000e+00\n",
      "Epoch 962/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0016 - accuracy: 0.0000e+00\n",
      "Epoch 963/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 964/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0020 - accuracy: 0.0000e+00\n",
      "Epoch 965/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0029 - accuracy: 0.0000e+00\n",
      "Epoch 966/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0018 - accuracy: 0.0000e+00\n",
      "Epoch 967/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0026 - accuracy: 0.0000e+00\n",
      "Epoch 968/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0054 - accuracy: 0.0000e+00\n",
      "Epoch 969/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0040 - accuracy: 0.0000e+00\n",
      "Epoch 970/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 971/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0072 - accuracy: 0.0000e+00\n",
      "Epoch 972/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0081 - accuracy: 0.0000e+00\n",
      "Epoch 973/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0059 - accuracy: 0.0000e+00\n",
      "Epoch 974/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0040 - accuracy: 0.0000e+00\n",
      "Epoch 975/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0025 - accuracy: 0.0000e+00\n",
      "Epoch 976/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 0.0000e+00\n",
      "Epoch 977/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0019 - accuracy: 0.0000e+00\n",
      "Epoch 978/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0018 - accuracy: 0.0000e+00\n",
      "Epoch 979/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0016 - accuracy: 0.0000e+00\n",
      "Epoch 980/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0017 - accuracy: 0.0000e+00\n",
      "Epoch 981/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0020 - accuracy: 0.0000e+00\n",
      "Epoch 982/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0028 - accuracy: 0.0000e+00\n",
      "Epoch 983/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0024 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 984/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0028 - accuracy: 0.0000e+00\n",
      "Epoch 985/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 986/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0022 - accuracy: 0.0000e+00\n",
      "Epoch 987/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0017 - accuracy: 0.0000e+00\n",
      "Epoch 988/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0041 - accuracy: 0.0000e+00\n",
      "Epoch 989/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 990/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0023 - accuracy: 0.0000e+00\n",
      "Epoch 991/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0022 - accuracy: 0.0000e+00\n",
      "Epoch 992/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 0.0000e+00\n",
      "Epoch 993/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 0.0000e+00\n",
      "Epoch 994/1000\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.0017 - accuracy: 0.0000e+00\n",
      "Epoch 995/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 996/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 997/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 998/1000\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.0031 - accuracy: 0.0000e+00\n",
      "Epoch 999/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 1000/1000\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0027 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2477e1548b0>"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5 = Sequential()\n",
    "model_5.add(Dense(100, input_dim=43, activation='relu'))\n",
    "#------------------------------------------------------------\n",
    "### 2/3 size of input layer , 28.67\n",
    "#---------------------------------------------------------\n",
    "model_5.add(Dense(98, activation='relu'))\n",
    "model_5.add(Dense(96, activation='relu'))\n",
    "model_5.add(Dense(94, activation='relu'))\n",
    "model_5.add(Dense(92, activation='relu'))\n",
    "model_5.add(Dense(90, activation='relu'))\n",
    "model_5.add(Dense(88, activation='relu'))\n",
    "model_5.add(Dense(86, activation='relu'))\n",
    "model_5.add(Dense(84, activation='relu'))\n",
    "model_5.add(Dense(82, activation='relu'))\n",
    "model_5.add(Dense(80, activation='relu'))\n",
    "model_5.add(Dense(78, activation='relu'))\n",
    "model_5.add(Dense(76, activation='relu'))\n",
    "model_5.add(Dense(74, activation='relu'))\n",
    "model_5.add(Dense(72, activation='relu'))\n",
    "model_5.add(Dense(70, activation='relu'))\n",
    "model_5.add(Dense(68, activation='relu'))\n",
    "model_5.add(Dense(66, activation='relu'))\n",
    "model_5.add(Dense(64, activation='relu'))\n",
    "model_5.add(Dense(62, activation='relu'))\n",
    "model_5.add(Dense(60, activation='relu'))\n",
    "model_5.add(Dense(58, activation='relu'))\n",
    "model_5.add(Dense(56, activation='relu'))\n",
    "model_5.add(Dense(54, activation='relu'))\n",
    "model_5.add(Dense(52, activation='relu'))\n",
    "model_5.add(Dense(50, activation='relu'))\n",
    "model_5.add(Dense(48, activation='relu'))\n",
    "model_5.add(Dense(46, activation='relu'))\n",
    "model_5.add(Dense(44, activation='relu'))\n",
    "model_5.add(Dense(42, activation='relu'))\n",
    "model_5.add(Dense(40, activation='relu'))\n",
    "model_5.add(Dense(38, activation='relu'))\n",
    "model_5.add(Dense(36, activation='relu'))\n",
    "model_5.add(Dense(34, activation='relu'))\n",
    "model_5.add(Dense(32, activation='relu'))\n",
    "model_5.add(Dense(30, activation='relu'))\n",
    "model_5.add(Dense(28, activation='relu'))\n",
    "model_5.add(Dense(26, activation='relu'))\n",
    "model_5.add(Dense(24, activation='relu'))\n",
    "model_5.add(Dense(22, activation='relu'))\n",
    "model_5.add(Dense(20, activation='relu'))\n",
    "model_5.add(Dense(18, activation='relu'))\n",
    "model_5.add(Dense(16, activation='relu'))\n",
    "model_5.add(Dense(14, activation='relu'))\n",
    "model_5.add(Dense(12, activation='relu'))\n",
    "model_5.add(Dense(10, activation='relu'))\n",
    "model_5.add(Dense(8, activation='relu'))\n",
    "model_5.add(Dense(6, activation='relu'))\n",
    "model_5.add(Dense(4, activation='relu'))\n",
    "model_5.add(Dense(2, activation='relu'))\n",
    "#---------------------------------------------------\n",
    "model_5.add(Dense(1))\n",
    "model_5.compile(loss=tf.keras.losses.MeanSquaredError(),optimizer='adam', metrics=['accuracy']) # \n",
    "model_5.fit(X_train,y_train, epochs=1000, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "0fdc3dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 3ms/step - loss: 0.2666 - accuracy: 0.0000e+00\n",
      "Test Accuracy : [0.2665681838989258, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print('Test Accuracy : {}'.format(model_5.evaluate(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "f5457d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>160987.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>194111.140625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>102971.109375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>176381.468750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>157024.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2915</td>\n",
       "      <td>118825.421875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2916</td>\n",
       "      <td>161750.328125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2917</td>\n",
       "      <td>164342.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2918</td>\n",
       "      <td>125617.156250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2919</td>\n",
       "      <td>156988.156250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id      SalePrice\n",
       "0     1461  160987.781250\n",
       "1     1462  194111.140625\n",
       "2     1463  102971.109375\n",
       "3     1464  176381.468750\n",
       "4     1465  157024.250000\n",
       "...    ...            ...\n",
       "1454  2915  118825.421875\n",
       "1455  2916  161750.328125\n",
       "1456  2917  164342.984375\n",
       "1457  2918  125617.156250\n",
       "1458  2919  156988.156250\n",
       "\n",
       "[1459 rows x 2 columns]"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_from_model = model_5.predict(df_house_test_extracted2b_predicted)\n",
    "df_house_test['SalePrice'] = 0\n",
    "df_house_test['SalePrice'] = np.expm1(result_from_model)\n",
    "df_house_test[['Id','SalePrice']].to_csv('C:/Users/thsong/submission15_5.csv',index=False)\n",
    "df_house_test[['Id','SalePrice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "1d8159a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1432, 43)"
      ]
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a926ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "bf406db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "115/115 [==============================] - 1s 2ms/step - loss: 84.7856 - accuracy: 0.0000e+00\n",
      "Epoch 2/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 3.0629 - accuracy: 0.0000e+00\n",
      "Epoch 3/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 1.8515 - accuracy: 0.0000e+00\n",
      "Epoch 4/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 1.3804 - accuracy: 0.0000e+00\n",
      "Epoch 5/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 1.1157 - accuracy: 0.0000e+00\n",
      "Epoch 6/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.9776 - accuracy: 0.0000e+00\n",
      "Epoch 7/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.7785 - accuracy: 0.0000e+00\n",
      "Epoch 8/300\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.6844 - accuracy: 0.0000e+00\n",
      "Epoch 9/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6799 - accuracy: 0.0000e+00\n",
      "Epoch 10/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.5380 - accuracy: 0.0000e+00\n",
      "Epoch 11/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.4688 - accuracy: 0.0000e+00\n",
      "Epoch 12/300\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.4244 - accuracy: 0.0000e+00\n",
      "Epoch 13/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.3937 - accuracy: 0.0000e+00\n",
      "Epoch 14/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.3882 - accuracy: 0.0000e+00\n",
      "Epoch 15/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.3215 - accuracy: 0.0000e+00\n",
      "Epoch 16/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.3198 - accuracy: 0.0000e+00\n",
      "Epoch 17/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2871 - accuracy: 0.0000e+00\n",
      "Epoch 18/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2801 - accuracy: 0.0000e+00\n",
      "Epoch 19/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2695 - accuracy: 0.0000e+00\n",
      "Epoch 20/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2447 - accuracy: 0.0000e+00\n",
      "Epoch 21/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2091 - accuracy: 0.0000e+00\n",
      "Epoch 22/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1983 - accuracy: 0.0000e+00\n",
      "Epoch 23/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1953 - accuracy: 0.0000e+00\n",
      "Epoch 24/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2984 - accuracy: 0.0000e+00\n",
      "Epoch 25/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1955 - accuracy: 0.0000e+00\n",
      "Epoch 26/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1663 - accuracy: 0.0000e+00\n",
      "Epoch 27/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1484 - accuracy: 0.0000e+00\n",
      "Epoch 28/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1673 - accuracy: 0.0000e+00\n",
      "Epoch 29/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1347 - accuracy: 0.0000e+00\n",
      "Epoch 30/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1255 - accuracy: 0.0000e+00\n",
      "Epoch 31/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1270 - accuracy: 0.0000e+00\n",
      "Epoch 32/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1311 - accuracy: 0.0000e+00\n",
      "Epoch 33/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1444 - accuracy: 0.0000e+00\n",
      "Epoch 34/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1408 - accuracy: 0.0000e+00\n",
      "Epoch 35/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1300 - accuracy: 0.0000e+00\n",
      "Epoch 36/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1445 - accuracy: 0.0000e+00\n",
      "Epoch 37/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1213 - accuracy: 0.0000e+00\n",
      "Epoch 38/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0897 - accuracy: 0.0000e+00\n",
      "Epoch 39/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0862 - accuracy: 0.0000e+00\n",
      "Epoch 40/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1056 - accuracy: 0.0000e+00\n",
      "Epoch 41/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0889 - accuracy: 0.0000e+00\n",
      "Epoch 42/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0782 - accuracy: 0.0000e+00\n",
      "Epoch 43/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0853 - accuracy: 0.0000e+00\n",
      "Epoch 44/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0742 - accuracy: 0.0000e+00\n",
      "Epoch 45/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0794 - accuracy: 0.0000e+00\n",
      "Epoch 46/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0727 - accuracy: 0.0000e+00\n",
      "Epoch 47/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0750 - accuracy: 0.0000e+00\n",
      "Epoch 48/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0607 - accuracy: 0.0000e+00\n",
      "Epoch 49/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0979 - accuracy: 0.0000e+00\n",
      "Epoch 50/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0968 - accuracy: 0.0000e+00\n",
      "Epoch 51/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0685 - accuracy: 0.0000e+00\n",
      "Epoch 52/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0686 - accuracy: 0.0000e+00\n",
      "Epoch 53/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0708 - accuracy: 0.0000e+00\n",
      "Epoch 54/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0588 - accuracy: 0.0000e+00\n",
      "Epoch 55/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0673 - accuracy: 0.0000e+00\n",
      "Epoch 56/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1083 - accuracy: 0.0000e+00\n",
      "Epoch 57/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0804 - accuracy: 0.0000e+00\n",
      "Epoch 58/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0645 - accuracy: 0.0000e+00\n",
      "Epoch 59/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0795 - accuracy: 0.0000e+00\n",
      "Epoch 60/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0687 - accuracy: 0.0000e+00\n",
      "Epoch 61/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0582 - accuracy: 0.0000e+00\n",
      "Epoch 62/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0480 - accuracy: 0.0000e+00\n",
      "Epoch 63/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0453 - accuracy: 0.0000e+00\n",
      "Epoch 64/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0727 - accuracy: 0.0000e+00\n",
      "Epoch 65/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0549 - accuracy: 0.0000e+00\n",
      "Epoch 66/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0460 - accuracy: 0.0000e+00\n",
      "Epoch 67/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0570 - accuracy: 0.0000e+00\n",
      "Epoch 68/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0565 - accuracy: 0.0000e+00\n",
      "Epoch 69/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0530 - accuracy: 0.0000e+00\n",
      "Epoch 70/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0582 - accuracy: 0.0000e+00\n",
      "Epoch 71/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0556 - accuracy: 0.0000e+00\n",
      "Epoch 72/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0380 - accuracy: 0.0000e+00\n",
      "Epoch 73/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0470 - accuracy: 0.0000e+00\n",
      "Epoch 74/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0425 - accuracy: 0.0000e+00\n",
      "Epoch 75/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0438 - accuracy: 0.0000e+00\n",
      "Epoch 76/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0602 - accuracy: 0.0000e+00\n",
      "Epoch 77/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0766 - accuracy: 0.0000e+00\n",
      "Epoch 78/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0736 - accuracy: 0.0000e+00\n",
      "Epoch 79/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0716 - accuracy: 0.0000e+00\n",
      "Epoch 80/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0637 - accuracy: 0.0000e+00\n",
      "Epoch 81/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0585 - accuracy: 0.0000e+00\n",
      "Epoch 82/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0471 - accuracy: 0.0000e+00\n",
      "Epoch 83/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0427 - accuracy: 0.0000e+00\n",
      "Epoch 84/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0360 - accuracy: 0.0000e+00\n",
      "Epoch 85/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0371 - accuracy: 0.0000e+00\n",
      "Epoch 86/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0387 - accuracy: 0.0000e+00\n",
      "Epoch 87/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0362 - accuracy: 0.0000e+00\n",
      "Epoch 88/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0468 - accuracy: 0.0000e+00\n",
      "Epoch 89/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1243 - accuracy: 0.0000e+00\n",
      "Epoch 90/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0714 - accuracy: 0.0000e+00\n",
      "Epoch 91/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0432 - accuracy: 0.0000e+00\n",
      "Epoch 92/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0448 - accuracy: 0.0000e+00\n",
      "Epoch 93/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0325 - accuracy: 0.0000e+00\n",
      "Epoch 94/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0287 - accuracy: 0.0000e+00\n",
      "Epoch 95/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0413 - accuracy: 0.0000e+00\n",
      "Epoch 96/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0440 - accuracy: 0.0000e+00\n",
      "Epoch 97/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0416 - accuracy: 0.0000e+00\n",
      "Epoch 98/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0581 - accuracy: 0.0000e+00\n",
      "Epoch 99/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0935 - accuracy: 0.0000e+00\n",
      "Epoch 100/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0441 - accuracy: 0.0000e+00\n",
      "Epoch 101/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0293 - accuracy: 0.0000e+00\n",
      "Epoch 102/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0479 - accuracy: 0.0000e+00\n",
      "Epoch 103/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0555 - accuracy: 0.0000e+00\n",
      "Epoch 104/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0464 - accuracy: 0.0000e+00\n",
      "Epoch 105/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0381 - accuracy: 0.0000e+00\n",
      "Epoch 106/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0478 - accuracy: 0.0000e+00\n",
      "Epoch 107/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0393 - accuracy: 0.0000e+00\n",
      "Epoch 108/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0316 - accuracy: 0.0000e+00\n",
      "Epoch 109/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0305 - accuracy: 0.0000e+00\n",
      "Epoch 110/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0416 - accuracy: 0.0000e+00\n",
      "Epoch 111/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0463 - accuracy: 0.0000e+00\n",
      "Epoch 112/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0483 - accuracy: 0.0000e+00\n",
      "Epoch 113/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0000e+00\n",
      "Epoch 114/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0281 - accuracy: 0.0000e+00\n",
      "Epoch 115/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0407 - accuracy: 0.0000e+00\n",
      "Epoch 116/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0424 - accuracy: 0.0000e+00\n",
      "Epoch 117/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0323 - accuracy: 0.0000e+00\n",
      "Epoch 118/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0349 - accuracy: 0.0000e+00\n",
      "Epoch 119/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0347 - accuracy: 0.0000e+00\n",
      "Epoch 120/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0324 - accuracy: 0.0000e+00\n",
      "Epoch 121/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0354 - accuracy: 0.0000e+00\n",
      "Epoch 122/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0597 - accuracy: 0.0000e+00\n",
      "Epoch 123/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0476 - accuracy: 0.0000e+00\n",
      "Epoch 124/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0428 - accuracy: 0.0000e+00\n",
      "Epoch 125/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0287 - accuracy: 0.0000e+00\n",
      "Epoch 126/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0000e+00\n",
      "Epoch 127/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0209 - accuracy: 0.0000e+00\n",
      "Epoch 128/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0177 - accuracy: 0.0000e+00\n",
      "Epoch 129/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0331 - accuracy: 0.0000e+00\n",
      "Epoch 130/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0741 - accuracy: 0.0000e+00\n",
      "Epoch 131/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0393 - accuracy: 0.0000e+00\n",
      "Epoch 132/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0563 - accuracy: 0.0000e+00\n",
      "Epoch 133/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0284 - accuracy: 0.0000e+00\n",
      "Epoch 134/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0375 - accuracy: 0.0000e+00\n",
      "Epoch 135/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0768 - accuracy: 0.0000e+00\n",
      "Epoch 136/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0670 - accuracy: 0.0000e+00\n",
      "Epoch 137/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0337 - accuracy: 0.0000e+00\n",
      "Epoch 138/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0212 - accuracy: 0.0000e+00\n",
      "Epoch 139/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0290 - accuracy: 0.0000e+00\n",
      "Epoch 140/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0194 - accuracy: 0.0000e+00\n",
      "Epoch 141/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0225 - accuracy: 0.0000e+00\n",
      "Epoch 142/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0185 - accuracy: 0.0000e+00\n",
      "Epoch 143/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0291 - accuracy: 0.0000e+00\n",
      "Epoch 144/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0527 - accuracy: 0.0000e+00\n",
      "Epoch 145/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0499 - accuracy: 0.0000e+00\n",
      "Epoch 146/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0287 - accuracy: 0.0000e+00\n",
      "Epoch 147/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 0.0000e+00\n",
      "Epoch 148/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0320 - accuracy: 0.0000e+00\n",
      "Epoch 149/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0258 - accuracy: 0.0000e+00\n",
      "Epoch 150/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0206 - accuracy: 0.0000e+00\n",
      "Epoch 151/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0172 - accuracy: 0.0000e+00\n",
      "Epoch 152/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0221 - accuracy: 0.0000e+00\n",
      "Epoch 153/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0474 - accuracy: 0.0000e+00\n",
      "Epoch 154/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0390 - accuracy: 0.0000e+00\n",
      "Epoch 155/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0320 - accuracy: 0.0000e+00\n",
      "Epoch 156/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0303 - accuracy: 0.0000e+00\n",
      "Epoch 157/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0379 - accuracy: 0.0000e+00\n",
      "Epoch 158/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0254 - accuracy: 0.0000e+00\n",
      "Epoch 159/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0426 - accuracy: 0.0000e+00\n",
      "Epoch 160/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0339 - accuracy: 0.0000e+00\n",
      "Epoch 161/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0199 - accuracy: 0.0000e+00\n",
      "Epoch 162/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0220 - accuracy: 0.0000e+00\n",
      "Epoch 163/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0286 - accuracy: 0.0000e+00\n",
      "Epoch 164/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0000e+00\n",
      "Epoch 165/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0455 - accuracy: 0.0000e+00\n",
      "Epoch 166/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0666 - accuracy: 0.0000e+00\n",
      "Epoch 167/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0445 - accuracy: 0.0000e+00\n",
      "Epoch 168/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0458 - accuracy: 0.0000e+00\n",
      "Epoch 169/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0000e+00\n",
      "Epoch 170/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0469 - accuracy: 0.0000e+00\n",
      "Epoch 171/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0293 - accuracy: 0.0000e+00\n",
      "Epoch 172/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0298 - accuracy: 0.0000e+00\n",
      "Epoch 173/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0298 - accuracy: 0.0000e+00\n",
      "Epoch 174/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0238 - accuracy: 0.0000e+00\n",
      "Epoch 175/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0210 - accuracy: 0.0000e+00\n",
      "Epoch 176/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0200 - accuracy: 0.0000e+00\n",
      "Epoch 177/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0154 - accuracy: 0.0000e+00\n",
      "Epoch 178/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0189 - accuracy: 0.0000e+00\n",
      "Epoch 179/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0214 - accuracy: 0.0000e+00\n",
      "Epoch 180/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0314 - accuracy: 0.0000e+00\n",
      "Epoch 181/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.0000e+00\n",
      "Epoch 182/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0316 - accuracy: 0.0000e+00\n",
      "Epoch 183/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0363 - accuracy: 0.0000e+00\n",
      "Epoch 184/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0360 - accuracy: 0.0000e+00\n",
      "Epoch 185/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0281 - accuracy: 0.0000e+00\n",
      "Epoch 186/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0183 - accuracy: 0.0000e+00\n",
      "Epoch 187/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0201 - accuracy: 0.0000e+00\n",
      "Epoch 188/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0170 - accuracy: 0.0000e+00\n",
      "Epoch 189/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0201 - accuracy: 0.0000e+00\n",
      "Epoch 190/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0273 - accuracy: 0.0000e+00\n",
      "Epoch 191/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0315 - accuracy: 0.0000e+00\n",
      "Epoch 192/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0367 - accuracy: 0.0000e+00\n",
      "Epoch 193/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0445 - accuracy: 0.0000e+00\n",
      "Epoch 194/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0882 - accuracy: 0.0000e+00\n",
      "Epoch 195/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0569 - accuracy: 0.0000e+00\n",
      "Epoch 196/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0314 - accuracy: 0.0000e+00\n",
      "Epoch 197/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0391 - accuracy: 0.0000e+00\n",
      "Epoch 198/300\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.0000e+00\n",
      "Epoch 199/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0189 - accuracy: 0.0000e+00\n",
      "Epoch 200/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0171 - accuracy: 0.0000e+00\n",
      "Epoch 201/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0206 - accuracy: 0.0000e+00\n",
      "Epoch 202/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0537 - accuracy: 0.0000e+00\n",
      "Epoch 203/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0365 - accuracy: 0.0000e+00\n",
      "Epoch 204/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0243 - accuracy: 0.0000e+00\n",
      "Epoch 205/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0177 - accuracy: 0.0000e+00\n",
      "Epoch 206/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0279 - accuracy: 0.0000e+00\n",
      "Epoch 207/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0296 - accuracy: 0.0000e+00\n",
      "Epoch 208/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0220 - accuracy: 0.0000e+00\n",
      "Epoch 209/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0189 - accuracy: 0.0000e+00\n",
      "Epoch 210/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0252 - accuracy: 0.0000e+00\n",
      "Epoch 211/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0221 - accuracy: 0.0000e+00\n",
      "Epoch 212/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0000e+00\n",
      "Epoch 213/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0235 - accuracy: 0.0000e+00\n",
      "Epoch 214/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0184 - accuracy: 0.0000e+00\n",
      "Epoch 215/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0228 - accuracy: 0.0000e+00\n",
      "Epoch 216/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0217 - accuracy: 0.0000e+00\n",
      "Epoch 217/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0430 - accuracy: 0.0000e+00\n",
      "Epoch 218/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0346 - accuracy: 0.0000e+00\n",
      "Epoch 219/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0437 - accuracy: 0.0000e+00\n",
      "Epoch 220/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0642 - accuracy: 0.0000e+00\n",
      "Epoch 221/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0231 - accuracy: 0.0000e+00\n",
      "Epoch 222/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0198 - accuracy: 0.0000e+00\n",
      "Epoch 223/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0248 - accuracy: 0.0000e+00\n",
      "Epoch 224/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0242 - accuracy: 0.0000e+00\n",
      "Epoch 225/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0160 - accuracy: 0.0000e+00\n",
      "Epoch 226/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0111 - accuracy: 0.0000e+00\n",
      "Epoch 227/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0113 - accuracy: 0.0000e+00\n",
      "Epoch 228/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0148 - accuracy: 0.0000e+00\n",
      "Epoch 229/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0179 - accuracy: 0.0000e+00\n",
      "Epoch 230/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0212 - accuracy: 0.0000e+00\n",
      "Epoch 231/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0193 - accuracy: 0.0000e+00\n",
      "Epoch 232/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0328 - accuracy: 0.0000e+00\n",
      "Epoch 233/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0246 - accuracy: 0.0000e+00\n",
      "Epoch 234/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0552 - accuracy: 0.0000e+00\n",
      "Epoch 235/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0385 - accuracy: 0.0000e+00\n",
      "Epoch 236/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0250 - accuracy: 0.0000e+00\n",
      "Epoch 237/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0183 - accuracy: 0.0000e+00\n",
      "Epoch 238/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0205 - accuracy: 0.0000e+00\n",
      "Epoch 239/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0200 - accuracy: 0.0000e+00\n",
      "Epoch 240/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0260 - accuracy: 0.0000e+00\n",
      "Epoch 241/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0214 - accuracy: 0.0000e+00\n",
      "Epoch 242/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0185 - accuracy: 0.0000e+00\n",
      "Epoch 243/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0165 - accuracy: 0.0000e+00\n",
      "Epoch 244/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0245 - accuracy: 0.0000e+00\n",
      "Epoch 245/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0460 - accuracy: 0.0000e+00\n",
      "Epoch 246/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0273 - accuracy: 0.0000e+00\n",
      "Epoch 247/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0296 - accuracy: 0.0000e+00\n",
      "Epoch 248/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0000e+00\n",
      "Epoch 249/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0238 - accuracy: 0.0000e+00\n",
      "Epoch 250/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0205 - accuracy: 0.0000e+00\n",
      "Epoch 251/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0197 - accuracy: 0.0000e+00\n",
      "Epoch 252/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0176 - accuracy: 0.0000e+00\n",
      "Epoch 253/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0192 - accuracy: 0.0000e+00\n",
      "Epoch 254/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0147 - accuracy: 0.0000e+00\n",
      "Epoch 255/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0123 - accuracy: 0.0000e+00\n",
      "Epoch 256/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0210 - accuracy: 0.0000e+00\n",
      "Epoch 257/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0178 - accuracy: 0.0000e+00\n",
      "Epoch 258/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0166 - accuracy: 0.0000e+00\n",
      "Epoch 259/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0178 - accuracy: 0.0000e+00\n",
      "Epoch 260/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0000e+00\n",
      "Epoch 261/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0249 - accuracy: 0.0000e+00\n",
      "Epoch 262/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0204 - accuracy: 0.0000e+00\n",
      "Epoch 263/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0379 - accuracy: 0.0000e+00\n",
      "Epoch 264/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0247 - accuracy: 0.0000e+00\n",
      "Epoch 265/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0246 - accuracy: 0.0000e+00\n",
      "Epoch 266/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0223 - accuracy: 0.0000e+00\n",
      "Epoch 267/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0228 - accuracy: 0.0000e+00\n",
      "Epoch 268/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0591 - accuracy: 0.0000e+00\n",
      "Epoch 269/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0328 - accuracy: 0.0000e+00\n",
      "Epoch 270/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0187 - accuracy: 0.0000e+00\n",
      "Epoch 271/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0198 - accuracy: 0.0000e+00\n",
      "Epoch 272/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0203 - accuracy: 0.0000e+00\n",
      "Epoch 273/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0180 - accuracy: 0.0000e+00\n",
      "Epoch 274/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0135 - accuracy: 0.0000e+00\n",
      "Epoch 275/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0109 - accuracy: 0.0000e+00\n",
      "Epoch 276/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0149 - accuracy: 0.0000e+00\n",
      "Epoch 277/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0168 - accuracy: 0.0000e+00\n",
      "Epoch 278/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0223 - accuracy: 0.0000e+00\n",
      "Epoch 279/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0203 - accuracy: 0.0000e+00\n",
      "Epoch 280/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0238 - accuracy: 0.0000e+00\n",
      "Epoch 281/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0227 - accuracy: 0.0000e+00\n",
      "Epoch 282/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0175 - accuracy: 0.0000e+00\n",
      "Epoch 283/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0125 - accuracy: 0.0000e+00\n",
      "Epoch 284/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0132 - accuracy: 0.0000e+00\n",
      "Epoch 285/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0172 - accuracy: 0.0000e+00\n",
      "Epoch 286/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0159 - accuracy: 0.0000e+00\n",
      "Epoch 287/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0246 - accuracy: 0.0000e+00\n",
      "Epoch 288/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0380 - accuracy: 0.0000e+00\n",
      "Epoch 289/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0331 - accuracy: 0.0000e+00\n",
      "Epoch 290/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0211 - accuracy: 0.0000e+00\n",
      "Epoch 291/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0179 - accuracy: 0.0000e+00\n",
      "Epoch 292/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0135 - accuracy: 0.0000e+00\n",
      "Epoch 293/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0153 - accuracy: 0.0000e+00\n",
      "Epoch 294/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0189 - accuracy: 0.0000e+00\n",
      "Epoch 295/300\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0134 - accuracy: 0.0000e+00\n",
      "Epoch 296/300\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0183 - accuracy: 0.0000e+00\n",
      "Epoch 297/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0300 - accuracy: 0.0000e+00\n",
      "Epoch 298/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0168 - accuracy: 0.0000e+00\n",
      "Epoch 299/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0164 - accuracy: 0.0000e+00\n",
      "Epoch 300/300\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0182 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2470c41d280>"
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_6 = Sequential()\n",
    "model_6.add(Dense(43, input_dim=43, activation='relu'))\n",
    "#------------------------------------------------------------\n",
    "### 2/3 size of input layer , 28.67\n",
    "#---------------------------------------------------------\n",
    "model_6.add(Dense(40, activation='relu'))\n",
    "model_6.add(Dense(37, activation='relu'))\n",
    "model_6.add(Dense(34, activation='relu'))\n",
    "model_6.add(Dense(31, activation='relu'))\n",
    "model_6.add(Dense(28, activation='relu'))\n",
    "model_6.add(Dense(25, activation='relu'))\n",
    "model_6.add(Dense(22, activation='relu'))\n",
    "model_6.add(Dense(19, activation='relu'))\n",
    "model_6.add(Dense(16, activation='relu'))\n",
    "model_6.add(Dense(13, activation='relu'))\n",
    "model_6.add(Dense(10, activation='relu'))\n",
    "model_6.add(Dense(7, activation='relu'))\n",
    "model_6.add(Dense(4, activation='relu'))\n",
    "#---------------------------------------------------\n",
    "model_6.add(Dense(1))\n",
    "model_6.compile(loss=tf.keras.losses.MeanSquaredError(),optimizer='adam', metrics=['accuracy']) # \n",
    "model_6.fit(X_train,y_train, epochs=300, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "8b797990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>139976.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>193380.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>132996.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>169006.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>188757.890625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2915</td>\n",
       "      <td>111106.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2916</td>\n",
       "      <td>255799.765625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2917</td>\n",
       "      <td>331956.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2918</td>\n",
       "      <td>194944.484375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2919</td>\n",
       "      <td>368408.656250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id      SalePrice\n",
       "0     1461  139976.562500\n",
       "1     1462  193380.187500\n",
       "2     1463  132996.062500\n",
       "3     1464  169006.562500\n",
       "4     1465  188757.890625\n",
       "...    ...            ...\n",
       "1454  2915  111106.312500\n",
       "1455  2916  255799.765625\n",
       "1456  2917  331956.687500\n",
       "1457  2918  194944.484375\n",
       "1458  2919  368408.656250\n",
       "\n",
       "[1459 rows x 2 columns]"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_from_model = model_6.predict(df_house_test_extracted2b_predicted)\n",
    "df_house_test['SalePrice'] = 0\n",
    "df_house_test['SalePrice'] = np.expm1(result_from_model)\n",
    "df_house_test[['Id','SalePrice']].to_csv('C:/Users/thsong/submission15_6.csv',index=False)\n",
    "df_house_test[['Id','SalePrice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92185bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### do further NN experiment after you study how to design neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aad4aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d80dcf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ba9323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60edddb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7478b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6cbcc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ee3ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b44357",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
